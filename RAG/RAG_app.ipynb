{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application - Document Q&A\n",
    "\n",
    "In this notebook, we are going to see in a step-by-step manner how to build a document Q&A application using a simple RAG pipeline. \n",
    "\n",
    "To that end, **Gemini AI models** will be used for embedding and generating answers and **ChromaDB** as the vector database. The RAG module will be constructed manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "* Install the python SDK to use the `Gemini API`\n",
    "* Install langchain_community (this package contains third-party integrations -> e.g. pyPDF loaders`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-genai\n",
    "%pip install -qU langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install YPython\n",
    "# %pip install dotenv\n",
    "# %pip install langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anagutierrez/Documents/Python Projects/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "from dotenv import load_dotenv  # to load environment variables (for API key variable)\n",
    "from pathlib import Path  \n",
    "\n",
    "from IPython.display import Markdown  # to get output in Markdown style\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # to load PDFs from a folder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # langChain text splitter\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # langChain access to google GenAI embedding models\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Google API key\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/api-key \n",
    "\n",
    "* Secure your API key in a environment variable file (.env) and load it using `load_dotenv()`\n",
    "* Ignore the .env file in gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path('./env')\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A System - Step by step\n",
    "\n",
    "### 1 - Load documents\n",
    "The first step is to load PDF documents into the system. We use `PyPDFDirectoryLoader` from the `langchain_community` library to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 0, 'page_label': '1'}, page_content='Prompt  \\nEngineering\\nAuthor: Lee Boonstra'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 1, 'page_label': '2'}, page_content='Prompt Engineering\\nSeptember 2024\\n2\\nAcknowledgements\\nReviewers and Contributors\\nMichael Sherman\\nYuan Cao\\nErick Armbrust\\nAnant Nawalgaria\\nAntonio Gulli\\nSimone Cammel\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nPrompt engineering 7\\nLLM output configuration 8\\n Output length 8\\n Sampling controls 9\\n  Temperature 9\\n  Top-K and top-P 10\\n  Putting it all together 11\\nPrompting techniques 12\\n General prompting / zero shot 13\\n One-shot & few-shot 14\\n System, contextual and role prompting 17\\n  System prompting 18\\n  Role prompting 21\\n  Contextual prompting 23\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 3, 'page_label': '4'}, page_content='Step-back prompting 25\\n Chain of Thought (CoT) 29\\n Self-consistency 32\\n Tree of Thoughts (ToT) 36\\n ReAct (reason & act) 37\\n Automatic Prompt Engineering 40\\n Code prompting 42\\n  Prompts for writing code 42\\n  Prompts for explaining code 44\\n  Prompts for translating code 46\\n  Prompts for debugging and reviewing code 48\\n  What about multimodal prompting? 54\\nBest Practices 54\\n Provide examples 54\\n Design with simplicity 55\\n Be specific about the output 56\\n Use Instructions over Constraints 56\\n Control the max token length 58\\n Use variables in prompts 58\\n Experiment with input formats and writing styles 59\\n For few-shot prompting with classification tasks, mix up the classes 59\\n Adapt to model updates 60\\n Experiment with output formats 60'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 4, 'page_label': '5'}, page_content='Experiment together with other prompt engineers 61\\n CoT Best practices 61\\n Document the various prompt attempts 62\\nSummary 63\\nEndnotes 65'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 5, 'page_label': '6'}, page_content='Prompt Engineering\\nSeptember 2024\\n6\\nIntroduction\\nWhen thinking about a large language model input and output, a text prompt (sometimes \\naccompanied by other modalities such as image prompts) is the input the model uses \\nto predict a specific output. You don’t need to be a data scientist or a machine learning \\nengineer – everyone can write a prompt. However, crafting the most effective prompt can be \\ncomplicated. Many aspects of your prompt affect its efficacy: the model you use, the model’s \\ntraining data, the model configurations, your word-choice, style and tone, structure, and \\ncontext all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts \\ncan lead to ambiguous, inaccurate responses, and can hinder the model’s ability to provide \\nmeaningful output.\\nYou don’t need to be a data \\nscientist or a machine learning \\nengineer – everyone can write \\na prompt.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 6, 'page_label': '7'}, page_content='Prompt Engineering\\nSeptember 2024\\n7\\nWhen you chat with the Gemini chatbot,1 you basically write prompts, however this \\nwhitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  \\nthe API, because by prompting the model directly you will have access to the configuration \\nsuch as temperature etc.\\nThis whitepaper discusses prompt engineering in detail. We will look into the various \\nprompting techniques to help you getting started and share tips and best practices to \\nbecome a prompting expert. We will also discuss some of the challenges you can face  \\nwhile crafting prompts.\\nPrompt engineering\\nRemember how an LLM works; it’s a prediction engine. The model takes sequential text as \\nan input and then predicts what the following token should be, based on the data it was \\ntrained on. The LLM is operationalized to do this over and over again, adding the previously \\npredicted token to the end of the sequential text for predicting the following token. The next \\ntoken prediction is based on the relationship between what’s in the previous tokens and what \\nthe LLM has seen during its training. \\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence \\nof tokens. Prompt engineering is the process of designing high-quality prompts that guide \\nLLMs to produce accurate outputs. This process involves tinkering to find the best prompt, \\noptimizing prompt length, and evaluating a prompt’s writing style and structure in relation \\nto the task. In the context of natural language processing and LLMs, a prompt is an input \\nprovided to the model to generate a response or prediction.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 7, 'page_label': '8'}, page_content='Prompt Engineering\\nSeptember 2024\\n8\\nThese prompts can be used to achieve various kinds of understanding and generation \\ntasks such as text summarization, information extraction, question and answering, text \\nclassification, language or code translation, code generation, and code documentation \\nor reasoning.\\nPlease feel free to refer to Google’s prompting guides2,3 with simple and effective \\nprompting examples.\\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be \\noptimized for your specific model, regardless of whether you use Gemini language models in \\nVertex AI, GPT, Claude, or an open source model like Gemma or LLaMA.\\nBesides the prompt, you will also need to tinker with the various configurations of a LLM.\\nLLM output configuration\\nOnce you choose your model you will need to figure out the model configuration. Most LLMs \\ncome with various configuration options that control the LLM’s output. Effective prompt \\nengineering requires setting these configurations optimally for your task.\\nOutput length\\nAn important configuration setting is the number of tokens to generate in a response. \\nGenerating more tokens requires more computation from the LLM, leading to higher energy \\nconsumption, potentially slower response times, and higher costs.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 8, 'page_label': '9'}, page_content='Prompt Engineering\\nSeptember 2024\\n9\\nReducing the output length of the LLM doesn’t cause the LLM to become more stylistically \\nor textually succinct in the output it creates, it just causes the LLM to stop predicting more \\ntokens once the limit is reached. If your needs require a short output length, you’ll also \\npossibly need to engineer your prompt to accommodate.\\nOutput length restriction is especially important for some LLM prompting techniques, like \\nReAct, where the LLM will keep emitting useless tokens after the response you want.\\nSampling controls\\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the \\nnext token could be, with each token in the LLM’s vocabulary getting a probability. Those \\ntoken probabilities are then sampled to determine what the next produced token will be. \\nTemperature, top-K, and top-P are the most common configuration settings that determine \\nhow predicted token probabilities are processed to choose a single output token.\\nTemperature\\nTemperature controls the degree of randomness in token selection. Lower temperatures \\nare good for prompts that expect a more deterministic response, while higher temperatures \\ncan lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is \\ndeterministic: the highest probability token is always selected (though note that if two tokens \\nhave the same highest predicted probability, depending on how tiebreaking is implemented \\nyou may not always get the same output with temperature 0).\\nTemperatures close to the max tend to create more random output. And as temperature gets \\nhigher and higher, all tokens become equally likely to be the next predicted token.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 9, 'page_label': '10'}, page_content='Prompt Engineering\\nSeptember 2024\\n10\\nThe Gemini temperature control can be understood in a similar way to the softmax function \\nused in machine learning. A low temperature setting mirrors a low softmax temperature (T), \\nemphasizing a single, preferred temperature with high certainty. A higher Gemini temperature \\nsetting is like a high softmax temperature, making a wider range of temperatures around \\nthe selected setting more acceptable. This increased uncertainty accommodates scenarios \\nwhere a rigid, precise temperature may not be essential like for example when experimenting \\nwith creative outputs.\\nTop-K and top-P\\nTop-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs \\nto restrict the predicted next token to come from tokens with the top predicted probabilities.  \\nLike temperature, these sampling settings control the randomness and diversity of \\ngenerated text.\\n• Top-K sampling selects the top K most likely tokens from the model’s predicted \\ndistribution. The higher top-K, the more creative and varied the model’s output; the \\nlower top-K, the more restive and factual the model’s output. A top-K of 1 is equivalent to \\ngreedy decoding.\\n• Top-P sampling selects the top tokens whose cumulative probability does not exceed \\na certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the \\nLLM’s vocabulary).\\nThe best way to choose between top-K and top-P is to experiment with both methods (or \\nboth together) and see which one produces the results you are looking for.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 10, 'page_label': '11'}, page_content='Prompt Engineering\\nSeptember 2024\\n11\\nAnother important configuration setting is the number of tokens to generate in a response. \\nBe aware, generating more tokens requires more computation from the LLM, leading \\nto higher energy consumption and potentially slower response times, which leads to \\nhigher costs.\\nPutting it all together\\nChoosing between top-K, top-P, temperature, and the number of tokens to generate, \\ndepends on the specific application and desired outcome, and the settings all impact one \\nanother. It’s also important to make sure you understand how your chosen model combines \\nthe different sampling settings together.\\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet \\nboth the top-K and top-P criteria are candidates for the next predicted token, and then \\ntemperature is applied to sample from the tokens that passed the top-K and top-P criteria. If \\nonly top-K or top-P is available, the behavior is the same but only the one top-K or P setting \\nis used. \\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then \\nrandomly selected from to produce a single next predicted token.\\nAt extreme settings of one sampling configuration value, that one sampling setting either \\ncancels out other configuration settings or becomes irrelevant.\\n• If you set temperature to 0, top-K and top-P become irrelevant–the most probable \\ntoken becomes the next token predicted. If you set temperature extremely high (above \\n1–generally into the 10s), temperature becomes irrelevant and whatever tokens make \\nit through the top-K and/or top-P criteria are then randomly sampled to choose a next \\npredicted token.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 11, 'page_label': '12'}, page_content='Prompt Engineering\\nSeptember 2024\\n12\\n• If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the \\ntop-K criteria, and that token is the next predicted token. If you set top-K extremely high, \\nlike to the size of the LLM’s vocabulary, any token with a nonzero probability of being the \\nnext token will meet the top-K criteria and none are selected out.\\n• If you set top-P to 0 (or a very small value), most LLM sampling implementations will then \\nonly consider the most probable token to meet the top-P criteria, making temperature and \\ntop-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the \\nnext token will meet the top-P criteria, and none are selected out.\\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you \\nrelatively coherent results that can be creative but not excessively so. If you want especially \\ncreative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you \\nwant less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. \\nFinally, if your task always has a single correct answer (e.g., answering a math problem), start \\nwith a temperature of 0.\\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM \\nmight generate text that is less relevant.\\nPrompting techniques\\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can \\nunderstand a prompt and generate an answer. But LLMs aren’t perfect; the clearer your \\nprompt text, the better it is for the LLM to predict the next likely text. Additionally, specific \\ntechniques that take advantage of how LLMs are trained and how LLMs work will help you get \\nthe relevant results from LLMs'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 12, 'page_label': '13'}, page_content='Prompt Engineering\\nSeptember 2024\\n13\\nNow that we understand what prompt engineering is and what it takes, let’s dive into some \\nexamples of the most important prompting techniques.\\nGeneral prompting / zero shot\\nA zero-shot5 prompt is the simplest type of prompt. It only provides a description of a task \\nand some text for the LLM to get started with. This input could be anything: a question, a \\nstart of a story, or instructions. The name zero-shot stands for ’no examples’.\\nLet’s use Vertex AI Studio (for Language) in Vertex AI,6 which provides a playground to test \\nprompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews. \\nThe table format as used below is a great way of documenting prompts. Your prompts will \\nlikely go through many iterations before they end up in a codebase, so it’s important to keep \\ntrack of your prompt engineering work in a disciplined, structured way. More on this table \\nformat, the importance of tracking prompt engineering work, and the prompt development \\nprocess is in the Best Practices section later in this chapter (“Document the various prompt \\nattempts”).  \\nThe model temperature should be set to a low number, since no creativity is needed, and we \\nuse the gemini-pro default top-K and top-P values, which effectively disable both settings \\n(see ‘LLM Output Configuration’ above). Pay attention to the generated output. The words \\ndisturbing and masterpiece should make the prediction a little more complicated, as both \\nwords are used in the same sentence.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 13, 'page_label': '14'}, page_content='Prompt Engineering\\nSeptember 2024\\n14\\nName 1_1_movie_classification\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 5\\nTop-K N/A Top-P 1\\nPrompt Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. \\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. I wish there were more movies like this masterpiece. \\nSentiment:\\nOutput POSITIVE\\nTable 1. An example of zero-shot prompting\\nWhen zero-shot doesn’t work, you can provide demonstrations or examples in the prompt, \\nwhich leads to “one-shot” and “few-shot” prompting. General prompting / zero shot\\nOne-shot & few-shot\\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can \\nhelp the model understand what you are asking for. Examples are especially useful when you \\nwant to steer the model to a certain output structure or pattern. \\nA one-shot prompt, provides a single example,  hence the name one-shot. The idea is the \\nmodel has an example it can imitate to best complete the task.\\nA few-shot prompt 7 provides multiple examples to the model. This approach shows the  \\nmodel a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples \\nof the desired pattern increases the chance the model follows the pattern.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 14, 'page_label': '15'}, page_content='Prompt Engineering\\nSeptember 2024\\n15\\nThe number of examples you need for few-shot prompting depends on a few factors, \\nincluding the complexity of the task, the quality of the examples, and the capabilities of the \\ngenerative AI (gen AI) model you are using. As a general rule of thumb, you should use at \\nleast three to five examples for few-shot prompting. However, you may need to use more \\nexamples for more complex tasks, or you may need to use fewer due to the input length \\nlimitation of your model.\\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model \\nconfiguration settings as before, other than increasing the token limit to accommodate the \\nneed for a longer response.\\nGoal Parse pizza orders to JSON\\nModel gemini-pro\\nTemperature 0.1 Token Limit 250\\nTop-K N/A Top-P 1\\nPrompt Parse a customer\\'s pizza order into valid JSON:\\nEXAMPLE:\\nI want a small pizza with cheese, tomato sauce, and pepperoni.\\nJSON Response:\\n```\\n{\\n\"size\": \"small\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\\n}\\n```\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 15, 'page_label': '16'}, page_content='Prompt Engineering\\nSeptember 2024\\n16\\nPrompt EXAMPLE:\\nCan I get a large pizza with tomato sauce, basil and mozzarella \\n{\\n\"size\": \"large\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\\n}\\nNow, I would like a large pizza, with the first half cheese and \\nmozzarella. And the other tomato sauce, ham and pineapple.\\nJSON Response:\\nOutput ```\\n{\\n\"size\": \"large\",\\n\"type\": \"half-half\",\\n\"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\", \\n\"ham\", \"pineapple\"]]\\n}\\n```\\nTable 2. An example of few-shot prompting\\nWhen you choose examples for your prompt, use examples that are relevant to the task you \\nwant to perform. The examples should be diverse, of high quality, and well written. One small \\nmistake can confuse the model and will result in undesired output.\\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to \\ninclude edge cases in your examples. Edge cases are inputs that are unusual or unexpected, \\nbut that the model should still be able to handle.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 16, 'page_label': '17'}, page_content='Prompt Engineering\\nSeptember 2024\\n17\\nSystem, contextual and role prompting\\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate \\ntext, but they focus on different aspects:\\n• System prompting sets the overall context and purpose for the language model. It \\ndefines the ‘big picture’ of what the model should be doing, like translating a language, \\nclassifying a review etc.\\n• Contextual prompting provides specific details or background information relevant to \\nthe current conversation or task. It helps the model to understand the nuances of what’s \\nbeing asked and tailor the response accordingly.\\n• Role prompting assigns a specific character or identity for the language model to adopt. \\nThis helps the model generate responses that are consistent with the assigned role and its \\nassociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a \\nprompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt serves a slightly different primary purpose:\\n• System prompt: Defines the model’s fundamental capabilities and overarching purpose.\\n• Contextual prompt: Provides immediate, task-specific information to guide the response. \\nIt’s highly specific to the current task or input, which is dynamic.\\n• Role prompt: Frames the model’s output style and voice. It adds a layer of specificity \\nand personality.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 17, 'page_label': '18'}, page_content='Prompt Engineering\\nSeptember 2024\\n18\\nDistinguishing between system, contextual, and role prompts provides a framework for \\ndesigning prompts with clear intent, allowing for flexible combinations and making it easier to \\nanalyze how each prompt type influences the language model’s output.\\nLet’s dive into these three different kinds of prompts.\\nSystem prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the \\noutput. I increased the temperature to get a higher creativity level, and I specified a higher \\ntoken limit. However, because of my clear instruction on how to return the output the model \\ndidn’t return extra text.\\nGoal Classify movie reviews as positive, neutral or negative.\\nModel gemini-pro\\nTemperature 1 Token Limit 5\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Only \\nreturn the label in uppercase.\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSentiment:\\nOutput NEGATIVE\\nTable 3. An example of system prompting'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 18, 'page_label': '19'}, page_content='Prompt Engineering\\nSeptember 2024\\n19\\nSystem prompts can be useful for generating output that meets specific requirements. The \\nname ‘system prompt’ actually stands for ‘providing an additional task to the system’. For \\nexample, you could use a system prompt to generate a code snippet that is compatible with \\na specific programming language, or you could use a system prompt to return a certain \\nstructure. Have a look into Table 4, where I return the output in JSON format.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 19, 'page_label': '20'}, page_content='Prompt Engineering\\nSeptember 2024\\n20\\nGoal Classify movie reviews as positive, neutral or negative, return JSON.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Classify movie reviews as positive, neutral or negative. Return \\nvalid JSON:\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n{\\n\"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\",\\n\"name\": String\\n}\\nMOVIE REVIEWS:\\n{\\n\"movie_reviews\": [MOVIE]\\n}\\n```\\nJSON Response:\\nOutput ```\\n{\\n  \"movie_reviews\": [\\n    {\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    }\\n  ]\\n}\\n```\\nTable 4. An example of system prompting with JSON format'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 20, 'page_label': '21'}, page_content='Prompt Engineering\\nSeptember 2024\\n21\\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply \\nadd an additional line to your prompt like: ‘You should be respectful in your answer.’.\\nRole prompting\\nRole prompting is a technique in prompt engineering that involves assigning a specific role to \\nthe gen AI model. This can help the model to generate more relevant and informative output, \\nas the model can craft its responses to the specific role that it has been assigned.\\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten \\nteacher, or a motivational speaker. Once the model has been assigned a role, you can then \\ngive it prompts that are specific to that role. For example, you could prompt a teacher to \\ncreate a lesson plan that they can then review. Have a look into Table 5, where the model \\ntakes the role of a travel guide.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 21, 'page_label': '22'}, page_content='Prompt Engineering\\nSeptember 2024\\n22\\nGoal Act as travel guide and provide 3 travel suggestions\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt I want you to act as a travel guide. I will write to you \\nabout my location and you will suggest 3 places to visit near \\nme. In some cases, I will also give you the type of places I \\nwill visit.\\nMy suggestion: \"I am in Amsterdam and I want to visit \\nonly museums.\"\\nTravel Suggestions:\\nOutput 1. Rijksmuseum: World-renowned museum, known for Dutch masterpieces \\nlike Rembrandt’s “The Night Watch.” Don’t miss the incredible collection of \\npaintings and artifacts showcasing Dutch art and history.\\n2. Van Gogh Museum: Immerse yourself in the captivating works of Vincent \\nvan Gogh. The museum houses the largest collection of his paintings and \\ndrawings, including “The Starry Night” and “Sunflowers.”\\n3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \\nfrom around the world. The museum’s collection includes works by Picasso, \\nKandinsky, and Rietveld, housed in a striking modern building.\\nTable 5. An example of role prompting\\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused \\nexpertise you’re looking for to improve the quality, relevance, and effectiveness of your \\noutput. \\nHere are some styles you can choose from which I find effective:\\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, \\nInspirational, Persuasive'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 22, 'page_label': '23'}, page_content='Prompt Engineering\\nSeptember 2024\\n23\\nLet’s change our prompt in Table 6 to include a humorous and inspirational style.\\nPrompt I want you to act as a travel guide. I will write to you about \\nmy location and you will suggest 3 places to visit near me in \\na humorous style.\\nMy suggestion: \"I am in Manhattan.\"\\nTravel Suggestions:\\nOutput 1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire \\nState Building and bask in the glory of Manhattan\\'s skyline. Prepare to feel like \\nKing Kong atop the Big Apple, minus the giant ape-sized banana.\\n2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum \\nof Modern Art (MoMA). Gaze upon masterpieces that will boggle your \\nmind and make you question whether your stick-figure drawings have any \\nartistic merit.\\n3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \\niconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or \\nactually shop) at designer boutiques that will make your wallet cry. But hey, \\nyou’re in Manhattan, so you might as well embrace the fabulousness!\\nTable 6. An example of role prompting with a humorous tone and style\\nContextual prompting\\nBy providing contextual prompts, you can help ensure that your AI interactions are as \\nseamless and efficient as possible. The model will be able to more quickly understand your \\nrequest and be able to generate more accurate and relevant responses, as you can see in the \\nexample of Table 7.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 23, 'page_label': '24'}, page_content=\"Prompt Engineering\\nSeptember 2024\\n24\\nGoal Suggest articles for a blog about retro games\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Context: You are writing for a blog about retro 80's arcade \\nvideo games.\\nSuggest 3 topics to write an article about with a few lines of \\ndescription of what this article should contain.\\nOutput 1. **The Evolution of Arcade Cabinet Design:** \\nThis article would explore the evolution of arcade cabinet designs, from the \\nearly wood and metal cabinets of the 1970s to the sleek, neon-lit designs of \\nthe 1980s and beyond.\\n2. **Blast From The Past: Iconic Arcade Games of The 80’s:** \\n- This article would feature a glimpse into the history, impact, and legacy \\nof some of the most iconic and influential arcade games of the 1980s, \\nhighlighting their innovations, popular mechanics, and enduring charm.\\n3. **The Rise and Retro Revival of Pixel Art:** \\n- This article would delve into the evolution of pixel art as a defining visual \\nstyle of early arcade games, tracing its roots, exploring its techniques, and \\ndiscussing the resurgence of pixel art in modern games and digital art.\\nTable 7. An example of contextual prompting\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 24, 'page_label': '25'}, page_content='Prompt Engineering\\nSeptember 2024\\n25\\nStep-back prompting\\nStep-back8 prompting is a technique for improving the performance by prompting the LLM \\nto first consider a general question related to the specific task at hand, and then feeding the \\nanswer to that general question into a subsequent prompt for the specific task. This ‘step \\nback’ allows the LLM to activate relevant background knowledge and reasoning processes \\nbefore attempting to solve the specific problem.\\nBy considering the broader and underlying principles, LLMs can generate more accurate and \\ninsightful responses. Step-back prompting encourages LLMs to think critically and apply their \\nknowledge in new and creative ways. It changes the final prompt doing the task by utilizing \\nmore knowledge in the LLM’s parameters than would otherwise come into play when the LLM \\nis prompted directly.\\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of \\nspecific details, step-back prompting.\\nLet’s have a look into these examples to understand how step-back prompting can improve \\nthe results. Let’s first review a traditional prompt (Table 8) before we compare it to a step \\nback prompt (Table 9):'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 25, 'page_label': '26'}, page_content='Prompt Engineering\\nSeptember 2024\\n26\\nGoal Write a storyline for a level of a first-person shooter video game.\\nModel gemini-pro\\nTemperature 1 Token Limit 1024\\nTop-K 40 Top-P 0.8\\nPrompt Write a one paragraph storyline for a new level of a first-\\nperson shooter video game that is challenging and engaging.\\nOutput The level begins with the player’s squad being ambushed in a dense urban area \\nby a relentless enemy faction. Players must fight their way through a maze of \\nback alleys and dilapidated buildings, utilizing stealth and precision to eliminate \\nenemy forces and avoid detection. The player must use their environment to their \\nadvantage, setting traps, utilizing cover, and making calculated moves to survive. \\nAlong the way, they uncover intel suggesting the enemy is planning a devastating \\nattack, and the player must race against time to gather evidence and relay it back \\nto their headquarters before the enemy can carry out their plans.\\nTable 8. A traditional prompt before we compare it with a step back prompt\\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a \\nstoryline, but it’s also quite random and generic. So let’s take a step back:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 26, 'page_label': '27'}, page_content='Prompt Engineering\\nSeptember 2024\\n27\\nPrompt Based on popular first-person shooter action games, what are \\n5 fictional key settings that contribute to a challenging and \\nengaging level storyline in a first-person shooter video game?\\nOutput 1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex \\ncrawling with mutated soldiers and rogue robots, ideal for challenging \\nfirearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering \\nskyscrapers and dense alleyways, featuring cybernetically enhanced enemies \\nand hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie \\ncorridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive \\nzombies, featuring intense close-quarters combat and puzzle-solving to find \\nsafe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, \\nfilled with mutated aquatic creatures, and requiring stealth and underwater \\nexploration skills to survive.\\nTable 9. An example of prompting for self consistency\\nYeah those topics seem like a good fit for a first-person video game. Let’s go back to the \\noriginal prompt, but this time we include the answer of the step back as context and see what \\nit will return.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 27, 'page_label': '28'}, page_content='Prompt Engineering\\nSeptember 2024\\n28\\nPrompt Context: 5 engaging themes for a first person shooter video game:\\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic \\nmilitary complex crawling with mutated soldiers and rogue \\nrobots, ideal for challenging firearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment \\nwith towering skyscrapers and dense alleyways, featuring \\ncybernetically enhanced enemies and hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on \\nEarth, with eerie corridors, zero-gravity sections, and \\nextraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of \\naggressive zombies, featuring intense close-quarters combat and \\npuzzle-solving to find safe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded \\nwith water, filled with mutated aquatic creatures, and requiring \\nstealth and underwater exploration skills to survive.\\nTake one of the themes and write a one paragraph storyline \\nfor a new level of a first-person shooter video game that is \\nchallenging and engaging.\\nOutput In the heart of a murky abyss, lies a dilapidated underwater research facility, \\nstanding as a testament to human ambition and its disastrous consequences. \\nShrouded in darkness, pulsating with the hum of malfunctioning machinery, and \\nhaunted by the echoes of drowned souls, the facility is now a breeding ground for \\na nightmare borne from the deep sea. The player, an elite marine equipped with \\nadvanced diving gear and experimental weaponry, is tasked with venturing into \\nthis aquatic realm of terror. Armed with resilience and resourcefulness, they must \\nnavigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea \\nmonstrosities that lurk in the depths. From grappling with the crushing pressure \\nof the deep sea to outsmarting cunning aquatic predators, every moment in this \\nuncharted underworld tests the limits of human endurance and courage.\\nTable 10. An example of prompting for self consistency\\nThat looks like an interesting video game! By using step back prompting techniques you can \\nincrease the accuracy of your prompts.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 28, 'page_label': '29'}, page_content='Prompt Engineering\\nSeptember 2024\\n29\\nChain of Thought (CoT)\\nChain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \\nof LLMs by generating intermediate reasoning steps. This helps the LLM generate more \\naccurate answers. You can combine it with few-shot prompting to get better results on more \\ncomplex tasks that require reasoning before responding as it’s a challenge with a zero-shot \\nchain of thought.\\nCoT has a lot of advantages. First of all, it’s low-effort while being very effective and works \\nwell with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT \\nprompting, as you can learn from the LLM’s responses and see the reasoning steps that were \\nfollowed. If there’s a malfunction, you will be able to identify it. Chain of thought appears \\nto improve robustness when moving between different LLM versions. Which means the \\nperformance of your prompt should drift less between different LLMs than if your prompt \\ndoes not use reasoning chains. Of course there are also disadvantages, but they are \\nsomewhat intuitive.\\nThe LLM response includes the chain of thought reasoning, which means more output \\ntokens, which means predictions cost more money and take longer.\\nTo explain the following example in Table 11, let’s first try to create a prompt that is not using \\nCoT prompting to showcase the flaws of a large language model.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, I \\nam 20 years old. How old is my partner?\\nOutput 63 years old\\nTable 11. An example of a prompt which is trying to solve a mathematical problem'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 29, 'page_label': '30'}, page_content=\"Prompt Engineering\\nSeptember 2024\\n30\\nYikes. That’s obviously the wrong answer. As a matter of fact, LLMs often struggle with \\nmathematical tasks and can provide incorrect answers – even for a task as simple as \\nmultiplying two numbers. This is because they are trained on large volumes of text and math \\nmay require a different approach. So let’s see if intermediate reasoning steps will improve \\nthe output.\\nPrompt When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nOutput 1. Let’s assume my current age is “x” years.\\n2. When I was 3 years old, my age was 3 years.\\n3. My partner’s age at that time was 3 times my age, which means my partner’s \\nage was 3 * 3 = 9 years.\\n4. Now, I am 20 years old, which means my age has increased by 20 - 3 = 17 \\nyears since I was 3 years old.\\n5. Since my partner’s age was 9 years when I was 3 years old, and my age has \\nincreased by 17 years, my partner’s age must have also increased by 17 years.\\n6. Therefore, my partner’s current age is 9 + 17 = 26 years.\\nSo, my partner is 26 years old.\\nTable 12. An example of Chain of Thought prompting\\nAlright, now the final answer is correct. That’s because we clearly instructed the LLM to \\nexplain each step instead of just returning an answer. It’s interesting to see that the model \\nadds up the 17 increased years. In my head I would take the difference of years between my \\npartner and me and add those up. (20+(9-3)). Let’s help the model to think a little bit more \\nlike me.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 30, 'page_label': '31'}, page_content=\"Prompt Engineering\\nSeptember 2024\\n31\\nTable 12 is an example of ‘zero-shot’ Chain of thought. Chain of thought prompting can be \\nvery powerful when combined with a single-shot or few-shot, as what you can see in Table 13:\\nPrompt Q: When my brother was 2 years old, I was double his age. Now \\nI am 40 years old. How old is my brother? Let's think step \\nby step.\\nA: When my brother was 2 years, I was 2 * 2 = 4 years old. \\nThat's an age difference of 2 years and I am older. Now I am 40 \\nyears old, so my brother is 40 - 2  = 38 years old. The answer \\nis 38.\\nQ: When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nA:\\nOutput When I was 3 years old, my partner was 3 * 3 = 9 years old. That’s an age \\ndifference of 6 years and my partner is older. Now I am 20 years old, so my \\npartner is 20 + 6 = 26 years old. The answer is 26.\\nTable 13. An example of chain of thought prompting with a single-shot\\nChain of thought can be useful for various use-cases. Think of code generation, for breaking \\ndown the request into a few steps, and mapping those to specific lines of code. Or for \\ncreating synthetic data when you have some kind of seed like “The product is called XYZ, \\nwrite a description guiding the model through the assumptions you would make based on \\nthe product given title.” Generally, any task that can be solved by ‘talking through is a good \\ncandidate for a chain of thought. If you can explain the steps to solve the problem, try chain \\nof thought.\\nPlease refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which \\nwill go into further detail on CoT prompting:\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 31, 'page_label': '32'}, page_content='Prompt Engineering\\nSeptember 2024\\n32\\nIn the best practices section of this chapter, we will learn some best practices specific to \\nChain of thought prompting.\\nSelf-consistency\\nWhile large language models have shown impressive success in various NLP tasks, their \\nability to reason is often seen as a limitation that cannot be overcome solely by increasing \\nmodel size. As we learned in the previous Chain of Thought prompting section, the model can \\nbe prompted to generate reasoning steps like a human solving a problem. However CoT uses \\na simple ‘greedy decoding’ strategy, limiting its effectiveness. Self-consistency11 combines \\nsampling and majority voting to generate diverse reasoning paths and select the most \\nconsistent answer. It improves the accuracy and coherence of responses generated by LLMs.\\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but \\nobviously has high costs.\\nIt follows the following steps:\\n1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple \\ntimes. A high temperature setting encourages the model to generate different reasoning \\npaths and perspectives on the problem.\\n2. Extract the answer from each generated response.\\n3. Choose the most common answer.\\nLet’s look into an example of an email classification system, which classifies an email as \\nIMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM \\nmultiple times, to see if the responses differ after each submit. Notice the friendly tone, the \\nword choice and the sarcasm that’s been used in the email. All this could trick the LLM.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 32, 'page_label': '33'}, page_content=\"Prompt Engineering\\nSeptember 2024\\n33\\nPrompt EMAIL:\\n```\\nHi,\\nI have seen you use Wordpress for your website. A great open \\nsource content management system. I have used it in the past \\ntoo. It comes with lots of great user plugins. And it's pretty \\neasy to set up.\\nI did notice a bug in the contact form, which happens when \\nyou select the name field. See the attached screenshot of me \\nentering text in the name field. Notice the JavaScript alert \\nbox that I inv0k3d.\\nBut for the rest it's a great website. I enjoy reading it. Feel \\nfree to leave the bug in the website, because it gives me more \\ninteresting things to read.\\nCheers,\\nHarry the Hacker.\\n```\\nClassify the above email as IMPORTANT or NOT IMPORTANT. Let's \\nthink step by step and explain why.\\nContinues next page...\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 33, 'page_label': '34'}, page_content='Prompt Engineering\\nSeptember 2024\\n34\\nOutput \\nAttempt 1\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form \\non their website. The sender also mentions that they enjoy reading the website \\nand that they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender \\nis a malicious actor who is trying to trick the recipient into clicking on a malicious \\nlink or downloading a malicious file.\\n**Conclusion: IMPORTANT**\\nBased on the potential impact of the bug and the credibility of the sender, \\nthe email should be classified as **IMPORTANT**. The recipient should take \\nimmediate action to fix the bug and protect their website from attack.\\nOutput \\nAttempt 2\\n**Explanation:**\\n1. **Lack of urgency:** The email does not convey any sense of urgency or \\ntime-sensitivity. It starts with a casual greeting and presents the information \\nin a matter-of-fact tone.\\n2. **Non-critical bug report:** The bug reported in the contact form is \\ndescribed as interesting but not critical. The email sender even suggests that \\nthe website owner can leave the bug in place.\\n3. **Lack of personal impact:** The email does not indicate any direct or \\npersonal impact on the website owner. It simply mentions that the sender \\nenjoys reading the website.\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 34, 'page_label': '35'}, page_content='Prompt Engineering\\nSeptember 2024\\n35\\nOutput \\nAttempt 2\\n4. **Absence of action request:** The email does not explicitly request any \\naction from the website owner. It does not ask for the bug to be fixed or \\nprovide any specific suggestions for improvement.\\n5. **Sender’s intent:** The sender’s intention seems to be more focused on \\nsharing an observation and expressing interest in the website rather than \\nraising a serious concern or requesting assistance.\\n**Conclusion: NOT IMPORTANT**\\nOutput \\nAttempt 3\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form on \\ntheir website. The sender also mentions that they enjoy reading the website and \\nthat they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the sender’s identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender is \\na malicious actor who is trying to exploit the bug for their own gain.\\n**Conclusion:**\\nBased on the above factors, the email should be classified as **IMPORTANT**. \\nThe bug in the contact form is a serious security risk, and the sender’s credibility \\nis unknown. The recipient should take immediate action to fix the bug and \\ninvestigate the sender’s claims.\\nTable 14. An example of prompting for self consistency\\nYou can use the above prompt and try to see if it returns a consistent classification. \\nDepending on the model that you are using and the temperature configuration, it might \\nreturn “IMPORTANT” or “NOT IMPORTANT”.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 35, 'page_label': '36'}, page_content='Prompt Engineering\\nSeptember 2024\\n36\\nBy generating many Chains of Thoughts, and taking the most commonly occurring answer \\n(“IMPORTANT”), we can get a more consistently correct answer from the LLM.\\nThis example shows how self-consistency prompting can be used to improve the accuracy \\nof an LLM’s response by considering multiple perspectives and selecting the most \\nconsistent answer.\\nTree of Thoughts (ToT)\\nNow that we are familiar with chain of thought and self-consistency prompting, let’s review \\nTree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs \\nto explore multiple different reasoning paths simultaneously, rather than just following a \\nsingle linear chain of thought. This is depicted in Figure 1.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 36, 'page_label': '37'}, page_content='Prompt Engineering\\nSeptember 2024\\n37\\nFigure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on \\nthe right\\nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It \\nworks by maintaining a tree of thoughts, where each thought represents a coherent language \\nsequence that serves as an intermediate step toward solving a problem. The model can then \\nexplore different reasoning paths by branching out from different nodes in the tree. \\nThere’s a great notebook, which goes into a bit more detail showing The Tree of Thought \\n(ToT) which is based on the paper ‘Large Language Model Guided Tree-of-Thought’.9\\nReAct (reason & act)\\nReason and act (ReAct) [10]13 prompting is a paradigm for enabling LLMs to solve complex \\ntasks using natural language reasoning combined with external tools (search, code \\ninterpreter etc.) allowing the LLM to perform certain actions, such as interacting with external \\nAPIs to retrieve information which is a first step towards agent modeling.\\nReAct mimics how humans operate in the real world, as we reason verbally and can \\ntake actions to gain information. ReAct performs well against other prompt engineering \\napproaches in a variety of domains.\\nReAct prompting works by combining reasoning and acting into a thought-action loop. The \\nLLM first reasons about the problem and generates a plan of action. It then performs the \\nactions in the plan and observes the results. The LLM then uses the observations to update \\nits reasoning and generate a new plan of action. This process continues until the LLM \\nreaches a solution to the problem.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 37, 'page_label': '38'}, page_content='Prompt Engineering\\nSeptember 2024\\n38\\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain \\nframework for Python, together with VertexAI (google-cloud-aiplatform) and the \\ngoogle-search-results pip packages.\\nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-\\napi-key and set an environment variable SERPAPI_API_KEY.\\nNext let’s write some Python code, with the task for the LLM to figure out: How many children \\nhave a famous dad that performs in the band Metallica.\\nPython\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import VertexAI\\nprompt = \"How many kids do the band members of Metallica have?\"\\nllm = VertexAI(temperature=0.1)\\ntools = load_tools([\"serpapi\"], llm=llm)\\nagent = initialize_agent(tools, llm,  \\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(prompt)\\nSnippet 1. Creating a ReAct Agent with LangChain and VertexAI\\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact, \\nthe LLM is scraping Google search results to figure out the band names. Then, it lists the \\nresults as observations and chains the thought for the next search.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 38, 'page_label': '39'}, page_content='Prompt Engineering\\nSeptember 2024\\n39\\nUnset\\n> Entering new AgentExecutor chain...\\nMetallica has 4 members. \\nAction: Search\\nAction Input: How many kids does James Hetfield have?\\nObservation: three children\\nThought: 1/4 Metallica band members have 3 children\\nAction: Search\\nAction Input: How many kids does Lars Ulrich have?\\nObservation: 3\\nThought: 2/4 Metallica band members have 6 children\\nAction: Search\\nAction Input: How many kids does Kirk Hammett have?\\nObservation: Hammett has been married to his second wife Lani since 1998. \\nThey have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June \\n28, 2008).\\nThought: 3/4 Metallica band members have 8 children\\nAction: Search\\nAction Input: How many kids does Robert Trujillo have?\\nObservation: 2\\nThought: 4/4 Metallica band members have 10 children\\nFinal Answer: 10\\n Snippet 2. Executing ReAct Agent in the terminal\\nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches \\neach band member to request the total of children and add up the total. Finally, it returns the \\ntotal number of children as the final answer.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 39, 'page_label': '40'}, page_content='Prompt Engineering\\nSeptember 2024\\n40\\nReAct prompting in practice requires understanding that you continually have to resend the \\nprevious prompts/responses (and do trimming of the extra generated content) as well as set \\nup the model with appropriate examples/instructions. Please refer to the notebook14 hosted \\nin the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the \\nactual LLM inputs and outputs with a more elaborate example\\nAutomatic Prompt Engineering\\nAt this point you might realize that writing a prompt can be complex. Wouldn’t it be nice to \\nautomate this (write a prompt to write prompts)? Well, there’s a method: Automatic Prompt \\nEngineering (APE). This method15 not only alleviates the need for human input but also \\nenhances the model’s performance in various tasks.\\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good \\nones. And repeat.\\nFor example, you can use automatic prompt engineering that will help with training a chatbot \\nfor a merchandise t-shirt webshop. We want to figure out all the various ways customers \\ncould phrase their order for buying a band merchandise t-shirt.\\n1. Write the prompt which will generate the output variants. In this example, I am using \\ngemini-pro to generate 10 instructions. See Table 15:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 40, 'page_label': '41'}, page_content='Prompt Engineering\\nSeptember 2024\\n41\\nPrompt We have a band merchandise t-shirt webshop, and to train a \\nchatbot we need various ways to order: \"One Metallica t-shirt \\nsize S\". Generate 10 variants, with the same semantics but keep \\nthe same meaning.\\nOutput 1. I’d like to purchase a Metallica t-shirt in size small.\\n2. Can I order a small-sized Metallica t-shirt?\\n3. I’d like to buy one Metallica t-shirt in small.\\n4. One Metallica shirt, size small, please.\\n5. I want one Metallica t-shirt in size small.\\n6. I’d like to place an order for one Metallica t-shirt in small.\\n7. Small Metallica t-shirt, one please.\\n8. I’m looking for one Metallica t-shirt in small.\\n9. Can you get me one Metallica t-shirt in small?\\n10. One Metallica small t-shirt, please.\\nTable 15. Use an LLM to generate prompts\\n2. Evaluate all of the instruction candidates by scoring the candidates based on a chosen \\nmetric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE \\n(Recall-Oriented Understudy for Gisting Evaluation).\\n3. Select the instruction candidate with the highest evaluation score. This candidate will be \\nthe final prompt you can use in your software application or chatbot. You can also tweak \\nthe select prompt and evaluate again.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 41, 'page_label': '42'}, page_content='Prompt Engineering\\nSeptember 2024\\n42\\nCode prompting\\nGemini focuses primarily on text-based prompts, which also includes writing prompts for \\nreturning code. Let’s go to the Vertex AI Studio and test these prompts to look at some \\ncoding examples.\\nPrompts for writing code\\nGemini can also be a developer and help you with writing code in any programming language \\nof choice. As a developer this can help you speed up the process of writing code.\\nImagine a folder on your machine with hundreds of files that needs to be renamed. \\nRenaming each file would take you a lot of time. You know a little Bash, and could write \\na script to automate this, but that may also take awhile. So let’s write a prompt. You can \\nwrite the prompt in the public consumer chatbot Gemini, or if you are more concerned \\nabout confidentiality, you can write these prompts within your Google Cloud account and \\nopen Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the \\ntemperature etc.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 42, 'page_label': '43'}, page_content='Prompt Engineering\\nSeptember 2024\\n43\\nName 1_bash_rename_files_in_folder\\nGoal Write a prompt to write code in Bash to rename files in a folder.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Write a code snippet in Bash, which asks for a folder name. \\nThen it takes the contents of the folder and renames all the \\nfiles inside by prepending the name draft to the file name.\\nOutput ```bash\\n#!/bin/bash\\n# Ask for the folder name\\necho \"Enter the folder name: \"\\nread folder_name\\n# Check if the folder exists\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\n# Get the list of files in the folder\\nfiles=( \"$folder_name\"/* )\\n# Rename each file by prepending \"draft\" to the file name\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nTable 16. Using Gemini to program code\\nThat seems like good code to me - it’s even documented! However, since LLMs can’t reason, \\nand repeat training data, it’s essential to read and test your code first. \\nThe moment we are all waiting for, does it really work?'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 43, 'page_label': '44'}, page_content='Prompt Engineering\\nSeptember 2024\\n44\\nLet’s try it first with a test folder with only a few files in it, which will need to be renamed from \\nfilename.txt to draft_filename.txt. \\n1. Copy the output from Table 16 (without the ```bash ``` text wrapper), and paste it in a new \\nfile called: “rename_files.sh”. \\n2. Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g. \\ntest. and hit enter. \\n3. The script seems to run fine. You will see the message: Files renamed successfully. \\nWhen you look into the test folder, notice that all the files were perfectly renamed to \\ndraft_filename.txt. \\nIt worked!\\nPrompts for explaining code\\nAs a developer when you work in teams you have to read someone else’s code. Gemini can \\nhelp you with this as well. Let’s take the code output from Table 16, remove the comments \\nand ask the large language model to explain what’s happening, See Table 17:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 44, 'page_label': '45'}, page_content='Prompt Engineering\\nSeptember 2024\\n45\\nName 1_bash_explain_the_code\\nGoal Write a prompt to explain Bash code\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Explain to me the below Bash code:\\n```\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput This Bash script performs the following tasks:\\n1. **User Input**: \\n- It prompts the user to enter a folder name using the `echo` and `read` \\ncommands. The entered folder name is stored in the `folder_name` variable.\\n2. **Folder Existence Check**: \\n- It checks if the folder with the specified name exists using the `[ ! -d \\n“$folder_name” ]` condition. If the folder does not exist, it displays an error \\nmessage (“Folder does not exist.”) and exits the script with an exit code of 1.\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 45, 'page_label': '46'}, page_content='Prompt Engineering\\nSeptember 2024\\n46\\nOutput 3. **File Listing**: \\n- It lists all the files inside the specified folder using the `files=( “$folder_\\nname”/* )` command. This creates an array called `files` containing the paths \\nto all the files in the folder.\\n4. **File Renaming**: \\n- It iterates through each file in the `files` array using a `for` loop. \\n- For each file, it generates a new file name using the `new_file_\\nname=”draft_$(basename “$file”)” ` command. This new file name is created \\nby prefixing the original file name with the string “draft_”. \\n- It then renames the file using the `mv “$file” “$new_file_name”` command, \\nwhich moves the file to the new file name.\\n5. **Success Message**: \\n- After renaming all the files, it displays a success message (“Files renamed \\nsuccessfully.”) to indicate that the renaming operation was successful\\nTable 17. A prompt for explaining code\\nPrompts for translating code\\nThe bash code from Table 16 seems to work fine. However this script could really be reusable \\nwhen it would prompt me for file names, ideally it should work as a separate application with \\nan UI. As a starting point, Python would be a better language for a (web) application than \\nBash. LLMs can help with translating code from one language to another. See the example in \\nTable 18:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 46, 'page_label': '47'}, page_content='Prompt Engineering\\nSeptember 2024\\n47\\nName 1_python_rename_files\\nGoal Write a prompt to translate Bash code to Python\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt Translate the below Bash code to a Python snippet.\\n```bash\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput ```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\nprint(\"Folder does not exist.\")\\n    exit(1)\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 47, 'page_label': '48'}, page_content='Prompt Engineering\\nSeptember 2024\\n48\\nOutput # Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_file_name = f\"draft_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nTable 18. A prompt for translating code from Bash to Python\\nRead and review the code. Copy the output from the prompt and paste it into a new file: \\nfile_renamer.py. Test the code by opening a Terminal window, and execute the following \\ncommand python file_renamer.py. \\nNOTE: When prompting for (Python) code in the Language Studio in Vertex AI, you will have \\nto click on the ‘Markdown’ button. Otherwise you will receive plain text which is missing the \\nproper indenting of lines, which is important for running Python code.\\nPrompts for debugging and reviewing code\\nLet’s manually write some edits to the code of Table 18. It should prompt the user for the \\nfilename prefix, and write this prefix in upper case characters. See the example code in \\nSnippet *3, but what a bummer. It now returns Python errors!'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 48, 'page_label': '49'}, page_content='Prompt Engineering\\nSeptember 2024\\n49\\nPython\\nimport os\\nimport shutil\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\nfiles = os.listdir(folder_name)\\nfor file in files:\\n    new_filename = f\"{text}_{file}\"\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\nprint(\"Files renamed successfully.\")\\nSnippet 3. A broken Python script\\nDoh! That looks like a bug:\\nUnset\\nThe below Python code gives an error:\\nTraceback (most recent call last):\\n  File “/Users/leeboonstra/Documents/test_folder/rename_files.py”, line 7, \\nin <module>\\n    text = toUpperCase(prefix)\\nNameError: name ‘toUpperCase’ is not defined\\nSnippet 4. I broke the Python code'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 49, 'page_label': '50'}, page_content='Prompt Engineering\\nSeptember 2024\\n50\\nLet’s see if we can ask the large language model to debug and review code. Have a look into \\nTable 19:\\nName 1_python_debug_code\\nGoal Write a prompt to debug and review Python code.\\nModel gemini-pro\\nTemperature 0.1 Token Limit 1024\\nTop-K N/A Top-P 1\\nPrompt The below Python code gives an error:\\nTraceback (most recent call last):\\n                                                   File \"/\\nUsers/leeboonstra/Documents/test_folder/rename_files.py\", line \\n7, in <module>\\n    text = toUpperCase(prefix)\\nNameError: name \\'toUpperCase\\' is not defined\\nDebug what\\'s wrong and explain how I can improve the code.\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 50, 'page_label': '51'}, page_content='Prompt Engineering\\nSeptember 2024\\n51\\nPrompt # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nOutput The code calls the `toUpperCase` function to convert `prefix` to uppercase, but \\nthat function is not defined. To fix this issue, you can use the `upper()` method of \\nthe string class which converts a given string into uppercase. The modified code \\nis shown below:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = prefix.upper()\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file),\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 51, 'page_label': '52'}, page_content='Prompt Engineering\\nSeptember 2024\\n52\\nOutput os.path.join(folder_name, new_filename))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nAdditionally, there are a few other improvements that can be made to the code:\\n1. The file extension of the new filenames is not kept. It’s better to extract the \\nextension from the original filename and append it to the new filename.\\n2. The code assumes that there are no spaces in the folder name. However, \\nit is a good practice to handle spaces gracefully by replacing them with \\nunderscores or hyphens.\\n3. Using the `f` string syntax for string interpolation is more readable and \\nconcise than the traditional `+` operator.\\n4. The code doesn’t handle errors that might occur during the renaming process. \\nIt would be better to wrap the `shutil.move` call in a `try...except` block to \\ncatch any potential errors.\\nHere is the improved code with these suggestions:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 52, 'page_label': '53'}, page_content='Prompt Engineering\\nSeptember 2024\\n53\\nOutput     # Get the file extension\\n    ext = os.path.splitext(file)[1]\\n# Create a new file name with the prefix and the file extension\\n    new_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"\\n# Build the full paths for the original and new files\\n    old_path = os.path.join(folder_name, file)\\n    new_path = os.path.join(folder_name, new_filename)\\n# Try to move the file\\n    try:\\n        shutil.move(old_path, new_path)\\nexcept Exception as e:\\n        print(f\"An error occurred while moving the file \\n{file}: {e}\")\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nThese improvements make the code more robust and flexible while also making \\nit easier to read and understand... (The response was truncated because \\nit has reached the token limit. Try to increase the token limit if you need a \\nlonger response.)\\nTable 19. A prompt for debugging and reviewing Python code\\nThat’s great. Not only did it tell me how to solve the problem, it also figured out that my code \\nhad more bugs and how to solve them, too. The last part of the prompt gave suggestions to \\nimprove the code in general.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 53, 'page_label': '54'}, page_content='Prompt Engineering\\nSeptember 2024\\n54\\nWhat about multimodal prompting?\\nPrompting for code still uses the same regular large language model. Multimodal prompting \\nis a separate concern, it refers to a technique where you use multiple input formats to guide \\na large language model, instead of just relying on text. This can include combinations of text, \\nimages, audio, code, or even other formats, depending on the model’s capabilities and the \\ntask at hand. \\nBest Practices\\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to \\nplay around with your prompts, with the ability to test against the various models.\\nUse the following best practices to become a pro in prompt engineering.\\nProvide examples\\nThe most important best practice is to provide (one shot / few shot) examples within a \\nprompt. This is highly effective because it acts as a powerful teaching tool. These examples \\nshowcase desired outputs or similar responses, allowing the model to learn from them \\nand tailor its own generation accordingly. It’s like giving the model a reference point or \\ntarget to aim for, improving the accuracy, style, and tone of its response to better match \\nyour expectations.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 54, 'page_label': '55'}, page_content=\"Prompt Engineering\\nSeptember 2024\\n55\\nDesign with simplicity\\nPrompts should be concise, clear, and easy to understand for both you and the model. As a \\nrule of thumb, if it’s already confusing for you it will likely be also confusing for the model. Try \\nnot to use complex language and don’t provide unnecessary information. \\nExamples:\\nBEFORE:\\nI am visiting New York right now, and I'd like to hear more about great \\nlocations. I am with two 3 year old kids. Where should we go during \\nour vacation?\\nAFTER REWRITE:\\nAct as a travel guide for tourists. Describe great places to visit in \\nNew York Manhattan with a 3 year old.\\nTry using verbs that describe the action. Here’s a set of examples:\\nAct, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, \\nEvaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, \\nPredict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, \\nSummarize, Translate, Write.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 55, 'page_label': '56'}, page_content='Prompt Engineering\\nSeptember 2024\\n56\\nBe specific about the output\\nBe specific about the desired output. A concise instruction might not guide the LLM enough \\nor could be too generic. Providing specific details in the prompt (through system or context \\nprompting) can help the model to focus on what’s relevant, improving the overall accuracy.\\nExamples:\\nDO: \\nGenerate a 3 paragraph blog post about the top 5 video game consoles. \\nThe blog post should be informative and engaging, and it should be \\nwritten in a conversational style.\\nDO NOT: \\nGenerate a blog post about video game consoles.\\nUse Instructions over Constraints\\nInstructions and constraints are used in prompting to guide the output of a LLM. \\n• An instruction provides explicit instructions on the desired format, style, or content of the \\nresponse. It guides the model on what the model should do or produce.\\n• A constraint is a set of limitations or boundaries on the response. It limits what the model \\nshould not do or avoid.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 56, 'page_label': '57'}, page_content='Prompt Engineering\\nSeptember 2024\\n57\\nGrowing research suggests that focusing on positive instructions in prompting can be more \\neffective than relying heavily on constraints. This approach aligns with how humans prefer \\npositive instructions over lists of what not to do. \\nInstructions directly communicate the desired outcome, whereas constraints might leave the \\nmodel guessing about what is allowed. It gives flexibility and encourages creativity within the \\ndefined boundaries, while constraints can limit the model’s potential. Also a list of constraints \\ncan clash with each other.\\nConstraints are still valuable but in certain situations. To prevent the model from generating \\nharmful or biased content or when a strict output format or style is needed.\\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to \\ndo instead. This can avoid confusion and improve the accuracy of the output. \\nDO: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nOnly discuss the console, the company who made it, the year, and total \\nsales. \\nDO NOT: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nDo not list video game names.\\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model \\nto do and only use constraints when necessary for safety, clarity or specific requirements. \\nExperiment and iterate to test different combinations of instructions and constraints to find \\nwhat works best for your specific tasks, and document these.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 57, 'page_label': '58'}, page_content='Prompt Engineering\\nSeptember 2024\\n58\\nControl the max token length\\nTo control the length of a generated LLM response, you can either set a max token limit in the \\nconfiguration or explicitly request a specific length in your prompt. For example:\\n\"Explain quantum physics in a tweet length message.\"\\nUse variables in prompts\\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be \\nchanged for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a \\ncity. Instead of hardcoding the city name in the prompt, use a variable. Variables can save \\nyou time and effort by allowing you to avoid repeating yourself. If you need to use the same \\npiece of information in multiple prompts, you can store it in a variable and then reference \\nthat variable in each prompt. This makes a lot of sense when integrating prompts into your \\nown applications.\\nPrompt VARIABLES\\n{city} = \"Amsterdam\"\\nPROMPT\\nYou are a travel guide. Tell me a fact about the city: {city}\\nOutput Amsterdam is a beautiful city full of canals, bridges, and narrow streets. It’s a \\ngreat place to visit for its rich history, culture, and nightlife.\\nTable 20. Using variables in prompts'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 58, 'page_label': '59'}, page_content='Prompt Engineering\\nSeptember 2024\\n59\\nExperiment with input formats and writing styles\\nDifferent models, model configurations, prompt formats, word choices, and submits can yield \\ndifferent results. Therefore, it’s important to experiment with prompt attributes like the style, \\nthe word choice, and the type prompt (zero shot, few shot, system prompt).\\nFor example a prompt with the goal to generate text about the revolutionary video game \\nconsole Sega Dreamcast, can be formulated as a question, a statement or an instruction, \\nresulting in different outputs:\\n• Question: What was the Sega Dreamcast and why was it such a revolutionary console?\\n• Statement: The Sega Dreamcast was a sixth-generation video game console released by \\nSega in 1999. It...\\n• Instruction: Write a single paragraph that describes the Sega Dreamcast console and \\nexplains why it was so revolutionary.\\nFor few-shot prompting with classification tasks, mix up \\nthe classes\\nGenerally speaking, the order of your few-shots examples should not matter much. However, \\nwhen doing classification tasks, make sure you mix up the possible response classes in the \\nfew shot examples. This is because you might otherwise be overfitting to the specific order of \\nthe examples. By mixing up the possible response classes, you can ensure that the model is \\nlearning to identify the key features of each class, rather than simply memorizing the order of \\nthe examples. This will lead to more robust and generalizable performance on unseen data.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 59, 'page_label': '60'}, page_content='Prompt Engineering\\nSeptember 2024\\n60\\nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy \\nfrom there.\\nAdapt to model updates\\nIt’s important for you to stay on top of model architecture changes, added data, and \\ncapabilities. Try out newer model versions and adjust your prompts to better leverage new \\nmodel features. Tools like Vertex AI Studio are great to store, test, and document the various \\nversions of your prompt.\\nExperiment with output formats\\nBesides the prompt input format, consider experimenting with the output format. For non-\\ncreative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try \\nhaving your output returned in a structured format like JSON or XML. \\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I don’t need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nTable 4 in the few-shot prompting section shows an example on how to return \\nstructured output.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 60, 'page_label': '61'}, page_content='Prompt Engineering\\nSeptember 2024\\n61\\nExperiment together with other prompt engineers\\nIf you are in a situation where you have to try to come up with a good prompt, you might \\nwant to find multiple people to make an attempt. When everyone follows the best practices \\n(as listed in this chapter) you are going to see a variance in performance between all the \\ndifferent prompt attempts.\\nCoT Best practices\\nFor CoT prompting, putting the answer after the reasoning is required because the \\ngeneration of the reasoning changes the tokens that the model gets when it predicts the \\nfinal answer.\\nWith CoT and self-consistency you need to be able to extract the final answer from your \\nprompt, separated from the reasoning. \\nFor CoT prompting, set the temperature to 0.\\nChain of thought prompting is based on greedy decoding, predicting the next word in a \\nsequence based on the highest probability assigned by the language model. Generally \\nspeaking, when using reasoning, to come up with the final answer, there’s likely one single \\ncorrect answer. Therefore the temperature should always set to 0.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 61, 'page_label': '62'}, page_content='Prompt Engineering\\nSeptember 2024\\n62\\nDocument the various prompt attempts\\nThe last tip was mentioned before in this chapter, but we can’t stress enough how important \\nit is: document your prompt attempts in full detail so you can learn over time what went well \\nand what did not. \\nPrompt outputs can differ across models, across sampling settings, and even across different \\nversions of the same model. Moreover, even across identical prompts to the same model, \\nsmall differences in output sentence formatting and word choice can occur. (For example, as \\nmentioned previously, if two tokens have the same predicted probability, ties may be broken \\nrandomly. This can then impact subsequent predicted tokens.).\\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of \\nthis approach are that you have a complete record when you inevitably have to revisit your \\nprompting work–either to pick it up in the future (you’d be surprised how much you can \\nforget after just a short break), to test prompt performance on different versions of a model, \\nand to help debug future errors.\\nBeyond the fields in this table, it’s also helpful to track the version of the prompt (iteration), \\na field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture \\nfeedback. If you’re lucky enough to be using Vertex AI Studio, save your prompts (using the \\nsame name and version as listed in your documentation) and track the hyperlink to the saved \\nprompt in the table. This way, you’re always one click away from re-running your prompts.\\nWhen working on a retrieval augmented generation system, you should also capture the \\nspecific aspects of the RAG system that impact what content was inserted into the prompt, \\nincluding the query, chunk settings, chunk output, and other information.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 62, 'page_label': '63'}, page_content='Prompt Engineering\\nSeptember 2024\\n63\\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the \\ncodebase, save prompts in a separate file from code, so it’s easier to maintain. Finally, ideally \\nyour prompts are part of an operationalized system, and as a prompt engineer you should \\nrely on automated tests and evaluation procedures to understand how well your prompt \\ngeneralizes to a task.\\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, \\nand document the results. Refine your prompt based on the model’s performance. Keep \\nexperimenting until you achieve the desired output. When you change a model or model \\nconfiguration, go back and keep experimenting with the previously used prompts.\\nName [name and version of your prompt]\\nGoal [One sentence explanation of the goal of this attempt]\\nModel [name and version of the used model]\\nTemperature [value between 0 - 1] Token Limit [number]\\nTop-K [number] Top-P [number]\\nPrompt [Write all the full prompt]\\nOutput [Write out the output or multiple outputs]\\nTable 21. A template for documenting prompts\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, \\nsuch as:\\n• Zero prompting\\n• Few shot prompting'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 63, 'page_label': '64'}, page_content='Prompt Engineering\\nSeptember 2024\\n64\\n• System prompting\\n• Role prompting\\n• Contextual prompting\\n• Step-back prompting\\n• Chain of thought\\n• Self consistency\\n• Tree of thoughts\\n• ReAct\\nWe even looked into ways how you can automate your prompts. \\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen \\nwhen your prompts are insufficient. We closed with best practices on how to become a better \\nprompt engineer.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:44:30-07:00', 'moddate': '2024-11-12T11:44:34-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Prompt Engineering_v4.pdf', 'total_pages': 65, 'page': 64, 'page_label': '65'}, page_content='Prompt Engineering\\nSeptember 2024\\n65\\nEndnotes\\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com .\\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html .\\n3. Google Cloud, 2023, Introduction to Prompting. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design .\\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body .\\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at:  \\nhttps://arxiv.org/pdf/2109.01652.pdf .\\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden .\\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at:  \\nhttps://arxiv.org/pdf/2005.14165.pdf.\\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.  \\nAvailable at: https://openreview.net/pdf?id=3bq3jsvcQ1\\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf.\\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb . \\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  \\nAvailable at: https://arxiv.org/pdf/2203.11171.pdf.\\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  \\nAvailable at: https://arxiv.org/pdf/2305.10601.pdf .\\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  \\nhttps://arxiv.org/pdf/2210.03629.pdf.\\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at:  \\nhttps://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-  \\non-vertex-ai/advanced_prompting_training/cot_react.ipynb .\\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt \\nEngineers. Available at: https://arxiv.org/pdf/2211.01910.pdf .'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 0, 'page_label': '1'}, page_content='Embeddings  \\n& Vector Stores\\nAuthors: Anant Nawalgaria  \\nand Xiaoqi Ren'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 1, 'page_label': '2'}, page_content='Embeddings & Vector Stores\\n2\\nSeptember 2024\\nReviewers and Contributors\\nAntonio Gulli\\nGrace Mollison\\nRuiqi Guo\\nIftekhar Naim\\nJinhyuk Lee\\nAlan Li\\nPatricia Florissi\\nAndrew Brook\\nOmid Fatemieh\\nZhuyun Dai\\nLee Boonstra\\nPer Jacobsson\\nSiddhartha Reddy Jonnalagadda\\nXi Cheng\\nRaphael Hoffmann\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nAcknowledgements'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 2, 'page_label': '3'}, page_content='Introduction 5\\nWhy embeddings are important 6\\n Types of embeddings 9\\n  Text embeddings 9\\n   Word embeddings 11\\n   Document embeddings 15\\n    Shallow BoW models 16\\n    Deeper pretrained large language models 18\\n  Image & multimodal embeddings 22\\n  Structured data embeddings 24\\n   General structured data 24\\n   User/item structured data 25\\n  Graph embeddings 25\\n Training Embeddings 26\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 3, 'page_label': '4'}, page_content='Vector search 28\\n Important vector search algorithms 29\\n  Locality sensitive hashing & trees 30\\n  Hierarchical navigable small worlds  33\\n  ScaNN 34\\nVector databases  37\\n Operational considerations 39\\nApplications 40\\n Q & A with sources (retrieval augmented generation) 42\\nSummary 46\\nEndnotes 48'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 4, 'page_label': '5'}, page_content=\"Embeddings & Vector Stores\\n5\\nSeptember 2024\\nIntroduction\\nModern machine learning thrives on diverse data—images, text, audio, and more. This \\nwhitepaper explores the power of embeddings, which transform this heterogeneous data into \\na unified vector representation for seamless use in various applications.\\nWe'll guide you through:\\n• Understanding Embeddings: Why they are essential for handling multimodal data and \\ntheir diverse applications.\\n• Embedding Techniques: Methods for mapping different data types into a common \\nvector space.\\nThese low-dimensional numerical \\nrepresentations of real-world data \\nsignificantly helps efficient large-\\nscale data processing and storage \\nby acting as means of lossy \\ncompression of the original data.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 5, 'page_label': '6'}, page_content='Embeddings & Vector Stores\\n6\\nSeptember 2024\\n• Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n• Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n• Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. They are expressed as low-dimensional vectors where the \\ngeometric distances of two vectors in the vector space is a projection of the relationships \\nbetween the two real-world objects that the vectors represent. In other words they help you \\nwith providing compact representations of data of different types, while simultaneously also \\nallowing you to compare two different data objects and tell how similar or different they are \\non a numerical scale: for example: The word ‘computer’ has a similar meaning to the picture \\nof a computer, as well as the word ’laptop’ but not to the word ‘car’. These low-dimensional \\nnumerical representations of real-world data significantly helps efficient large-scale data \\nprocessing and storage by acting as means of lossy compression of the original data while \\nretaining its important properties.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 6, 'page_label': '7'}, page_content='Embeddings & Vector Stores\\n7\\nSeptember 2024\\nOne of the key applications for embeddings is retrieval and recommendations, where the \\nresult is usually from a massive search space. For example, Google Search is a retrieval  with \\nthe  search space of the whole internet. Today’s retrieval and recommendation systems’ \\nsuccess depends on the following:\\n1. Precomputing the embeddings for billions items of the search space.\\n2. Mapping query embeddings to the same embedding space.\\n3. Efficient computing and retrieving of the nearest neighbors of the query embeddings in \\nthe search space.\\nEmbeddings also shine in the world of multimodality. Most applications work with large \\namounts of data of various modalities: text, speech, image, and videos to name a few. \\nBecause every entity or object is represented in its own unique format, it’s very difficult \\nto project these objects into the same vector space that is both compact and informative. \\nIdeally, such a representation would capture as much of the original object’s characteristics \\nas possible. An embedding refers to the projected vector of an object from an input space to \\na relatively low-dimensional vector space. Each vector is a list of floating point numbers.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 7, 'page_label': '8'}, page_content='Embeddings & Vector Stores\\n8\\nSeptember 2024\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nIdeally the embeddings are created so they place objects with similar semantic properties \\ncloser in the embedding space (a low-dimensional vector space where items can be \\nprojected). The embeddings can then be used as a condensed, meaningful input in \\ndownstream applications. For example, you can use them as features for ML models, \\nrecommender systems, search engines, and many more. So your data not only gets a \\ncompact numerical representation, but this representation also preserves the semantic \\nmeanings for a specific task or across a variety of tasks. The fact that these representations \\nare task-specific means you can generate different embeddings for the same object, \\noptimized for the task at hand.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 8, 'page_label': '9'}, page_content='Embeddings & Vector Stores\\n9\\nSeptember 2024\\nTypes of embeddings\\nEmbeddings aim to obtain a low dimensional representation of the original data while \\npreserving most of the ‘essential information’. The types of data an embedding represents \\ncan be of various different forms.  Below you’ll see some standard techniques used for \\ndifferent types of data, including text and image.\\nText embeddings\\nText embeddings are used extensively as part of natural language processing (NLP). They \\nare often used to embed the meaning of natural language in machine learning for processing \\nin various downstream applications such as text generation, classification, sentiment \\nanalysis, and more. These embeddings broadly fall into two categories: token/word and \\ndocument embeddings.\\nBefore diving deeper into these categories, it’s important to understand the entire lifecycle \\nof text: from its input by the user to its conversion to embeddings. \\nFigure 2. The process of turning text into embeddings\\nIt all starts with the input string which is split into smaller meaningful pieces called tokens.  \\nThis process is called tokenization. Commonly, these tokens are wordpieces, characters, \\nwords, numbers, and punctuations using one of the many existing tokenization techniques.1 \\nAfter the string is tokenized, each of these tokens is then assigned a unique integer value'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 9, 'page_label': '10'}, page_content='Embeddings & Vector Stores\\n10\\nSeptember 2024\\nusually in the the range: [0, cardinality of the total number of tokens in the corpus]. For \\nexample, for a 16 word vocabulary the IDs would range between 0-15. This value is also \\nreferred to as token ID. These tokens can be used to represent each string as a sparse \\nnumerical vector representation of documents used for downstream tasks directly, or after \\none-hot encoding. One-hot encoding is a binary representation of categorical values where \\nthe presence of a word is represented by 1, and its absence by 0. This ensures that the token \\nIDs are treated as categorical values as they are, but often results in a dense vector the size \\nof the vocabulary of the corpus. Snippet 1 and Figure 3 show an example of how this can be \\ndone using Tensorflow.\\n# Tokenize the input string data\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ndata = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\n# Filter the punctiations, tokenize the words and index them to integers  \\ntokenizer = Tokenizer(num_words=15, filters=\\'!\"#$%&()*+,-./:;<=>?[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\', lower=True, \\nsplit=\\' \\')\\ntokenizer.fit_on_texts(data)\\n# Translate each sentence into its word-level IDs, and then one-hot encode those IDs \\nID_sequences = tokenizer.texts_to_sequences(data)\\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\\nprint(\"ID dictionary:\\\\n\", tokenizer.word_index)\\nprint(\"\\\\nID sequences:\\\\n\", ID_sequences)\\nprint(\"\\\\n One-hot encoded sequences:\\\\n\", binary_sequences )\\nSnippet 1. Tokenizing,  indexing and one-hot encoding strings'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 10, 'page_label': '11'}, page_content='Embeddings & Vector Stores\\n11\\nSeptember 2024\\nFigure 3. Output of Snippet 1\\nHowever, since these Integer IDs (or their corresponding one-hot encoded vectors) are \\nassigned randomly to words, they lack any inherent semantic meaning. This is where \\nembeddings are much more useful. Although it’s possible to embed character and sub-word \\nlevel tokens as well, let us look at word and document embeddings to understand some of \\nthe methods behind them.\\nWord embeddings\\nIn this section, you’ll see a few word embedding techniques and algorithms to both train \\nand use word embeddings. While there are many ML driven algorithms developed over \\ntime optimized for different objectives, the most common ones are GloVe,2 SWIVEL,3 and \\nWord2Vec.4 Word embeddings or sub-word embeddings can also be directly obtained from \\nhidden layers of language models. However, the embeddings will be different for the same \\nword in different contexts of the text. This section focuses on lightweight, context-free \\nword embedding and leaves the context-aware document embeddings for the document \\nembeddings section. Word embedding can be directly applied to downstream tasks like \\nnamed entity extraction and topic modeling.\\nWord2Vec is a family of model architectures that operates on the principle of “the semantic \\nmeaning of a word is defined by its neighbors”, or words that frequently appear close to each \\nother in the training corpus. This method can be both used to train your own embeddings'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 11, 'page_label': '12'}, page_content='Embeddings & Vector Stores\\n12\\nSeptember 2024\\nfrom large datasets or be quickly integrated through one of the readily available pre-trained \\nembeddings available online.5 The embeddings for each word - which are essentially fixed \\nlength vectors - are randomly initialized to kick off the process, resulting in a matrix of shape \\n(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table \\nafter the training process is completed using one of the following methods (see Figure 4). \\n• The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using \\nthe embeddings of the surrounding words as input. This method is agnostic to the order \\nof the surrounding words in the context. This approach is fast to train and is slightly more \\naccurate for frequent words.\\n• The skip-gram approach: The setup is inverse of that of CBOW, with the middle word \\nbeing used to predict the surrounding words within a certain range. This approach is \\nslower to train but works well with small data and is more accurate for rare words.\\nFigure 4. Diagram explaining how CBOW and Skip-Gram methods work'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 12, 'page_label': '13'}, page_content='Embeddings & Vector Stores\\n13\\nSeptember 2024\\nThe Word2Vec algorithms can also be extended to the sub-word level, which has been the \\ninspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec \\nis that although it accounts well for local statistics of words within a certain sliding window, it \\ndoes not capture the global statistics (words in the whole corpus). This shortcoming is what \\nmethods like the GloVe algorithm address.\\nGloVe is a word embedding technique that leverages both global and local statistics of words. \\nIt does this by first creating a co-occurrence matrix, which represents the relationships \\nbetween words. GloVe then uses a factorization technique to learn word representations \\nfrom the co-occurrence matrix. The resulting word representations are able to capture both \\nglobal and local information about words, and they are useful for a variety of NLP tasks.\\nIn addition to GloVE, SWIVEL is another approach which leverages the co-occurrence \\nmatrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative \\nSampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into \\naccount the co-occurrence of words within a fixed window of its neighboring words. \\nFurthermore, SWIVEL also considers unobserved co-occurrences and handles it using a \\nspecial piecewise loss, boosting its performance with rare words. It is generally considered \\nonly slightly less accurate than GloVe on average, but is considerably faster to train. This is \\nbecause it leverages distributed training by subdividing the Embedding vectors into smaller \\nsub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 \\nbelow demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and \\nvisualizing them in a 2D space, and computing nearest neighbors.\\nWord embeddings can be directly used in some downstream tasks like Named Entity \\nRecognition (NER).'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 13, 'page_label': '14'}, page_content='Embeddings & Vector Stores\\n14\\nSeptember 2024\\nfrom gensim.models import Word2Vec \\nimport gensim.downloader as api\\nimport pprint\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport numpy as np\\ndef tsne_plot(models, words, seed=23):\\n  \"Creates a TSNE models & plots for multiple word models for the given words\"\\n  plt.figure(figsize=(len(models)*30, len(models)*30))\\n  model_ix = 0\\n  for model in models:\\n    labels = []\\n    tokens = []\\n    for word in words:\\n      tokens.append(model[word])\\n      labels.append(word)\\n    tsne_model = TSNE(perplexity=40, n_components=2, init=\\'pca\\', n_iter=2500, random_state=seed) \\n    new_values = tsne_model.fit_transform(np.array(tokens))\\n    x = []\\n    y = []\\n    for value in new_values:\\n      x.append(value[0])\\n      y.append(value[1])\\n    model_ix +=1\\n    plt.subplot(10, 10, model_ix)\\n    for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),\\n            textcoords=\\'offset points\\',\\n            ha=\\'right\\',\\n            va=\\'bottom\\')\\n  plt.tight_layout()\\n  plt.show()\\nv2w_model = api.load(\\'word2vec-google-news-300\\')\\nglove_model = api.load(\\'glove-twitter-25\\')\\nprint(\"words most similar to \\'computer\\' with word2vec and glove respectively:\")\\npprint.pprint( v2w_model.most_similar(\"computer\")[:3])\\npprint.pprint( glove_model.most_similar(\"computer\")[:3]) \\npprint.pprint(\"2d projection of some common words of both models\")\\nsample_common_words= list(set(v2w_model.index_to_key[100:10000]) \\n                        & set(glove_model.index_to_key[100:10000]))[:100]\\ntsne_plot([v2w_model, glove_model], sample_common_words)\\nSnippet 2. Loading and plotting GloVe and Word2Vec embeddings in 2D'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 14, 'page_label': '15'}, page_content='Embeddings & Vector Stores\\n15\\nSeptember 2024\\nFigure 5 Shows semantically similar words are clustered differently for the two algorithms\\nFigure 5. 2D visualization of pre-trained GloVe and Word2Vec word embeddings\\nDocument embeddings\\nEmbedding documents to low-dimensional dense embedding has attracted long-lasting \\ninterests since the 1980s. Document embeddings can be used in various applications, \\nincluding semantic search, topic discovery, classification, and clustering to embed \\nthe meaning of a series of words in paragraphs and documents and use it for various'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 15, 'page_label': '16'}, page_content='Embeddings & Vector Stores\\n16\\nSeptember 2024\\ndownstream applications. The evolution of the embeddings models can mainly be \\ncategorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained \\nlarge language models.\\nShallow BoW models\\nEarly document embedding works follow the bag-of-words (BoW) paradigm, assuming a \\ndocument is an unordered collection of words. These early works include latent semantic \\nanalysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses \\na co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a \\nbayesian network to model the document embeddings. Another famous bag-of-words family \\nof document embeddings is TF-IDF (term frequency-inverse document frequency) based \\nmodels, which are statistical models that use the word frequency to represent the document \\nembedding. TF-IDF-based models can either be a sparse embedding, which represents the \\nterm-level importance, or can be combined with word embeddings as a weighting factor to \\ngenerate a dense embedding for the documents. For example, BM25, a TF-IDF-based bag-\\nof-words model, is still a strong baseline in today’s retrieval benchmarks.9\\nHowever,  the bag-of-words paradigm also has two major weaknesses: both the word \\nordering and the semantic meanings are ignored. BoW models fail to capture the sequential \\nrelationships between words, which are crucial for understanding meaning and context. \\nInspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document \\nembeddings using (shallow) neural networks. The Doc2Vec model adds an additional \\n‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec \\nas illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other \\nword embeddings to predict a random word in the paragraph. After training, for existing \\nparagraphs or documents, the learned embeddings can be directly used in downstream \\ntasks. For a new paragraph or document, extra inference steps need to be performed to \\ngenerate the paragraph or document embedding.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 16, 'page_label': '17'}, page_content='Embeddings & Vector Stores\\n17\\nSeptember 2024\\nFigure 6. Doc2vec CBOW model\\nSnippet 3 below shows how you can train your own doc2Vec models on a custom corpus:\\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\\nfrom gensim.test.utils import get_tmpfile\\n#train model on a sequence of documents tagged with their IDs\\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\\n# persist model to disk, and load it to infer on new documents\\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\\nmodel.save(model_file)\\nmodel = Doc2Vec.load(model_file)  \\nmodel.infer_vector([\"human\", \"interface\"])\\nSnippet 3. Self-supervised Training and inference using Doc2Vec on private corpus'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 17, 'page_label': '18'}, page_content='Embeddings & Vector Stores\\n18\\nSeptember 2024\\nThe success of applying neural networks in the embedding world inspired an increasing \\ninterest in using deep neural networks to generate embeddings. \\nDeeper pretrained large language models\\nMotivated by the development of deep neural networks, different embedding models and \\ntechniques were proposed, and the state-of-the-art models are refreshed frequently. Main \\nchanges of the models include: \\n1. Using more complex learning models, especially bi-directional deep neural network \\nmodels. \\n2. The use of massive pre-training on unlabeled text. \\n3. The use of a subword tokenizer. \\n4. Using fine-tuning for various downstream NLP tasks. \\nIn 2018, BERT11 - which stands for bidirectional encoder representations from transformers - \\nwas proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm \\nBERT based on, has become the mainstream model paradigm until today. Besides using a \\ntransformer as the model backbone, another key of BERT’s success is from pre-training with \\na massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as \\nthe pre-training objective. It did this by randomly masking some tokens of the input and using \\nthe masked token id as the prediction objective. This allows the model to utilize both the \\nright and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next \\nsentence prediction task in pretraining. BERT outputs a contextualized embedding for every \\ntoken in the input. Typically, the embedding of the first token (a special token named [CLS]) is \\nused as the embedding for the whole input.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 18, 'page_label': '19'}, page_content='Embeddings & Vector Stores\\n19\\nSeptember 2024\\nFigure 7. The BERT architecture\\nBERT became the base model for multiple embedding models, including Sentence-\\nBERT,12 SimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large \\nlanguage models - never stops. T5 was proposed in 2019 with up to 11B parameters. PaLM \\nwas proposed in 2022 to push the large language model to a surprising 540B parameters. \\nModels like Gemini from Google, GPT models from OpenAI and Llama models from Meta are \\nalso evolving to newer generations at astonishing speed. Please refer to the whitepaper on \\nFoundational models for more information about some common LLMs.\\nNew embedding models based on large language models have been proposed. For example, \\nGTR and Sentence-T5 show better performance on retrieval and sentence similarity \\n(respectively) than BERT family models.\\nAnother approach to new embeddings models development is generating multi-vector \\nembeddings instead of a single vector to enhance the representational power of the models. \\nEmbedding models in this family include ColBERT15 and XTR.16'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 19, 'page_label': '20'}, page_content='Embeddings & Vector Stores\\n20\\nSeptember 2024\\nFigure 8. An illustration of the taxonomy diagram of the embedding models\\nAlthough the deep neural network models require a lot more data and compute time to train, \\nthey have much better performance compared to models using bag-of-words paradigms. \\nFor example, for the same word the embeddings would be different with different contexts. \\nSnippet 4 demonstrates how pre-trained document embedding models from Tensorflow-\\nhub17 (for example,Sentence t5)A and Vertex AIB can be used for training models with Keras \\nand TF datasets. Vertex Generative AI text embeddings can be used with the Vertex AI SDK, \\nLangchain, and Google’s BigQuery (Snippet 5) for embedding and advanced workflows.18\\nA. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage. \\nB. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 20, 'page_label': '21'}, page_content='Embeddings & Vector Stores\\n21\\nSeptember 2024\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\n# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\\nMODEL_NAME = \"text-embedding-004\"\\n# Set the task_type, text and optional title as the model inputs.\\n# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\", \\n# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\\nTASK_TYPE = \"RETRIEVAL_DOCUMENT\" \\nTITLE = \"Google\"\\nTEXT = \"Embed text.\"\\n# Use Vertex LLM text embeddings\\nembeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ndef LLM_embed(text):\\n    def embed_text(text):\\n        text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\",   text=text.numpy())\\n        return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\\n output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n output.set_shape(( 768,))\\n return  output\\n# Embed strings using vertex LLMs\\nLLM_embeddings=train_data.map(lambda x,y: (LLM_embed(x), y))\\n# Embed strings in the tf.dataset using one of the tf hub models\\nembedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\\n                          \\n# Train model \\nmodel = tf.keras.Sequential()\\nmodel.add(hub_layer) # omit this layer if using Vertex LLM embeddings\\nmodel.add(tf.keras.layers.Dense(16, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dense(1))\\nmodel.compile(optimizer=\\'adam\\',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n  metrics=[\\'accuracy\\'])\\nhistory = model.fit(train_data.shuffle(100).batch(8))\\nSnippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 21, 'page_label': '22'}, page_content='Embeddings & Vector Stores\\n22\\nSeptember 2024\\nSELECT * FROM ML.GENERATE_TEXT_EMBEDDING(\\nMODEL my_project.my_company.llm_embedding_model,\\n(\\nSELECT review as content\\nFROM bigquery-public-data.imdb.reviews));\\nSnippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table\\nImage & multimodal embeddings\\nMuch like text, it’s also possible to create both image and multimodal embeddings. \\nUnimodal image embeddings can be derived in many ways: one of which is by training a \\nCNN or Vision Transformer model on a large scale image classification task (for example, \\nImagenet), and then using the penultimate layer as the image embedding. This layer has \\nlearnt some important discriminative feature maps for the training task. It contains a set of \\nfeature maps that are discriminative for the task at hand and can be extended to other tasks \\nas well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image \\nembeddings and their semantic relationships learnt via another training process. This \\ngives you a fixed size semantic representation in the same latent space. The below snippet \\n(Snippet 6) can be used to compute image and multimodal embeddings for images and text \\nand be used with a keras model directly (much like the text embedding example).'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 22, 'page_label': '23'}, page_content='Embeddings & Vector Stores\\n23\\nSeptember 2024\\nimport base64\\nimport tensorflow as tf\\nfrom google.cloud import aiplatform\\nfrom google.protobuf import struct_pb2\\n#fine-tunable layer for image embeddings which can be used for downstream keras model image_\\nembed=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_\\nvector/2\",trainable=False) \\nclass EmbeddingPredictionClient:\\n  \"\"\"Wrapper around Prediction Service Client.\"\"\"\\n  def __init__(self, project : str,\\n    location : str = \"us-central1\",\\n    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\\n    client_options = {\"api_endpoint\": api_regional_endpoint}\\n    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  \\n    self.location = location\\n    self.project = project\\n  def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage\\n   with tf.io.gfile.GFile(gs_image_path, \"rb\") as f:\\n     image_bytes = f.read()\\n   if not text and not image_bytes:\\n    raise ValueError(\\'At least one of text or image_bytes must be specified.\\')\\n   #Initialize a protobuf data struct with the text and image inputs \\n   instance = struct_pb2.Struct()\\n    if text:\\n      instance.fields[\\'text\\'].string_value = text\\n      if image_bytes:\\n      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\\n      image_struct = instance.fields[\\'image\\'].struct_value\\n      image_struct.fields[\\'bytesBase64Encoded\\'].string_value = .string_value = encoded_content\\n     #Make predictions using the multimodal embedding model\\n     instances = [instance]\\n     endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")\\n     response = self.client.predict(endpoint=endpoint, instances=instances)\\n     text_embedding = None\\n     if text:    \\n      text_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      text_embedding = [v for v in text_emb_value]\\n     image_embedding = None\\n     if image_bytes:    \\n      image_emb_value = response.predictions[0][\\'imageEmbedding\\']\\n      image_embedding = [v for v in image_emb_value]\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 23, 'page_label': '24'}, page_content='Embeddings & Vector Stores\\n24\\nSeptember 2024\\n return EmbeddingResponse (text_embedding=text_embedding, image_embedding=image_embedding)\\n#compute multimodal embeddings for text and images\\nclient.get_embedding(text=\"sample_test\", gs_image_path=\"gs://bucket_name../image_filename..\")\\nSnippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings\\nStructured data embeddings\\nThere are two common ways to generate embeddings for structured data, one is more \\ngeneral while the other is more tailored for recommendation applications. \\nUnlike unstructured data, where a pre-trained embedding model is typically available, we \\nhave to create the embedding model for the structured data since it would be specific to \\na particular application.\\nGeneral structured data\\nGiven a general structured data table, we can create embedding for each row. This can be \\ndone by the ML models in the dimensionality reduction category, such as the PCA model.\\nOne use case for these embeddings are for anomaly detection. For example, we can create \\nembeddings for anomaly detection using large data sets of labeled sensor information \\nthat identify anomalous occurrences.20 Another case use is to feed these embeddings \\nto downstream ML tasks such as classification. Compared to using the original high-\\ndimensional data, using embeddings to train a supervised model requires less data. This is \\nparticularly important in cases where training data is not sufficient.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 24, 'page_label': '25'}, page_content='Embeddings & Vector Stores\\n25\\nSeptember 2024\\nUser/item structured data\\nThe input is no longer a general structured data table as above. Instead, the input includes \\nthe user data, item/product data plus the data describing the interaction between user and \\nitem/product, such as rating score. \\nThis category is for recommendation purposes, as it maps two sets of data (user dataset, \\nitem/product/etc dataset) into the same embedding space. For recommender systems, we \\ncan create embeddings out of structured data that correlate to different entities such as \\nproducts, articles, etc. Again, we have to create our own embedding model. Sometimes this \\ncan be combined with unstructured embedding methods when images or text descriptions \\nare found.\\nGraph embeddings\\nGraph embeddings are another embedding technique that lets you represent not \\nonly information about a specific object but also its neighbors (namely, their graph \\nrepresentation). Take an example of a social network where each person is a node, and the \\nconnections between people are defined as edges. Using graph embedding you can model \\neach node as an embedding, such that the embedding captures not only the semantic \\ninformation about the person itself, but also its relations and associations hence enriching \\nthe embedding. For example, if two nodes are connected by an edge, the vectors for those \\nnodes would be similar. You might then be able to predict who the person is most similar \\nto and recommend new connections. Graph embeddings can also be used for a variety of \\ntasks, including node classification, graph classification, link prediction, clustering, search, \\nrecommendation systems, and more. Popular algorithms21,22 for graph embedding include \\nDeepWalk, Node2vec, LINE, and GraphSAGE.23'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 25, 'page_label': '26'}, page_content='Embeddings & Vector Stores\\n26\\nSeptember 2024\\nTraining Embeddings\\nCurrent embedding models usually use dual encoder (two tower) architecture. For example, \\nfor the text embedding model used in question-answering, one tower is used to encode \\nthe queries and the other tower is used to encode the documents. For the image and text \\nembedding model, one tower is used to encode the images and the other tower is used \\nto encode the text. The model can have various sub architectures, depending on how the \\nmodel components are shared between the two towers. The following figure shows some \\narchitectures of the dual encoders.24 \\nFigure 9. Some architectures of dual encoders\\nThe loss used in embedding models training is usually a variation of contrastive loss, which \\ntakes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training \\nwith contrastive loss brings positive examples closer and negative examples far apart.\\nSimilar to foundation model training, training of an embedding model from scratch usually \\nincludes two stages: pretraining (unsupervised learning) and fine tuning (supervised \\nlearning). Nowadays, the embedding models are usually directly initialized from foundation \\nmodels such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the \\nmassive knowledge that has been learned from the large-scale pretraining of the foundation'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 26, 'page_label': '27'}, page_content='Embeddings & Vector Stores\\n27\\nSeptember 2024\\nmodels. The fine-tuning of the embedding models can have one or more phases. The fine-\\ntuning datasets can be created in various methods, including human labeling, synthetic \\ndataset generation, model distillation, and hard negative mining.\\nTo use embeddings for downstream tasks like classification or named entity recognition, \\nextra layers (for example, softmax classification layer) can be added on top of the embedding \\nmodels. The embedding model can either be frozen (especially when the training dataset is \\nsmall), trained from scratch, or fine-tuned together with the downstream tasks. \\nVertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can \\nalso choose to fine-tune the models directly. See26 for an example of fine tuning the BERT \\nmodel using tensorflow model garden. You can also directly load the embedding models from \\ntfhub and fine-tune on top of the model. Snippet 7 shows an example how to build a classifier \\nbased on tfhub models. \\n# Can switch the embedding to different embeddings from different modalities on # \\ntfhub. Here we use the BERT model as an example.\\ntfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\nclass Classifier(tf.keras.Model):\\n    def __init__(self, num_classes):\\n      super(Classifier, self).__init__(name=\"prediction\")\\n        self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\\n      self.dropout = tf.keras.layers.Dropout(0.1)\\n      self.dense = tf.keras.layers.Dense(num_classes)\\n    def call(self, preprocessed_text):\\n      encoder_outputs = self.encoder(preprocessed_text)\\n      pooled_output = encoder_outputs[\"pooled_output\"]\\n   x = self.dropout(pooled_output)\\n   x = self.dense(x)\\n   return x\\nSnippet 7. Creating a Keras model using trainable tfhub layer'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 27, 'page_label': '28'}, page_content='Embeddings & Vector Stores\\n28\\nSeptember 2024\\nSo far you’ve seen the various types of embeddings, techniques and best practices to train \\nthem for various data modalities, and some of their applications. The next section discusses \\nhow to persist and search the embeddings that have been created in a fast and scalable way \\nfor production workloads.\\nVector search\\nFull-text keyword search has been the lynchpin of modern IT systems for years. Full-text \\nsearch engines and databases (relational and non-relational) often rely on explicit keyword \\nmatching. For example, if you search for ‘cappuccino’ the search engine or database returns \\nall documents that mention the exact query in the tags or text description. However, if the \\nkey word is misspelled or described with a differently worded text, a traditional keyword \\nsearch returns incorrect or no results. There are traditional approaches which are tolerant of \\nmisspellings and other typographical errors. However, they are still unable to find the results \\nhaving the closest underlying semantic meanings to the query. This is where vector search is \\nvery powerful: it uses the vector or embedded semantic representation of documents.\\nVector search lets you to go beyond searching for exact query literals and allows you to \\nsearch for the meaning across various data modalities. This provides you more nuanced \\nresults. After you have a function that can compute embeddings of various items,  you \\ncompute the embedding of the items of interest and store this embedding in a database. \\nYou then embed the incoming query in the same vector space as the items. Next, you have \\nto find the best matches to the query. This process is analogous to finding the most ‘similar’ \\nmatches across the entire collection of searchable vectors: similarity between vectors can be \\ncomputed using a metric such as euclidean distance, cosine similarity, or dot product.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 28, 'page_label': '29'}, page_content='Embeddings & Vector Stores\\n29\\nSeptember 2024\\nFigure 10. Visualization of how different metrics compute vector similarity\\nEuclidean distance (i.e., L2 distance) is a geometric measure of the distance between two \\npoints in a vector space. This works well for lower dimensions. Cosine similarity is a measure \\nof the angle between two vectors. And inner/dot product, is the projection of one vector \\nonto another. They are equivalent when the vector norms are 1. This seems to work better \\nfor higher dimensional data. Vector databases store and help manage and operationalize the \\ncomplexity of vector search at scale, while also addressing the common database needs.\\nImportant vector search algorithms\\nThe most straightforward way to find the most similar match is to run a traditional linear \\nsearch by comparing the query vector with each document vector and return the one with \\nthe highest similarity. However, the runtime of this approach scales linearly (O(N)) with the \\namount of documents or items to search. This approach is unacceptably slow for most use \\ncases involving several millions of documents or more. Using approximate nearest neighbour'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 29, 'page_label': '30'}, page_content='Embeddings & Vector Stores\\n30\\nSeptember 2024\\n(ANN) search for that purpose is more practical.  ANN is a technique for finding the closest \\npoints to a given point in a dataset with a small margin of error - but with a tremendous boost \\nin performance. There are many approaches with varying trade-offs across scale, indexing \\ntime, performance, simplicity and more.27 They use one or more implementations of the \\nfollowing techniques: quantization, hashing, clustering and trees, among others. Some of the \\nmost popular approaches are discussed below.\\nLocality sensitive hashing & trees\\nLocality sensitive hashing (LSH) 28 is a technique for finding similar items in a large dataset. \\nIt does this by creating one or more hash functions that map similar items to the same hash \\nbucket with high probability. This means that you can quickly find all of the similar items to \\na given item by only looking at the candidate items in the same hash bucket (or adjacent \\nbuckets) and do a linear search amongst those candidate pairs. This allows for significantly \\nfaster lookups within a specific radius. The number of hash functions/tables and buckets \\ndetermine the search recall/speed tradeoff, as well as the false positive / true positive one. \\nHaving too many hash functions might cause similar items to different buckets, while too few \\nmight result in too many items falsely being hashed to the same bucket and the number of \\nlinear searches to increase.\\nAnother intuitive way to think about LSH is grouping residences by their postal code or \\nneighborhood name. Then based on where someone chooses to move you look at the \\nresidences for only that neighborhood and find the closest match.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 30, 'page_label': '31'}, page_content='Embeddings & Vector Stores\\n31\\nSeptember 2024\\nFigure 11. Visualization of how LSH uses random hyperplanes to partition the vector space\\nTree-based algorithms work similarly. For example, the Kd-tree approach works by creating \\nthe decision boundaries by computing the median of the values of the first dimension, then \\nthat of the second dimension and so on. This approach is very much like a decision tree. \\nNaturally this can be ineffective if searchable vectors are high dimensional. In that case, the \\nBall-tree algorithm is better suited. It is similar in functionality, except instead of going by \\ndimension-wise medians it creates buckets based on the radial distance of the data points \\nfrom the center. Here is an example of the implementation of these three approaches:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 31, 'page_label': '32'}, page_content='Embeddings & Vector Stores\\n32\\nSeptember 2024\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom vertexai.language_models import TextEmbeddingModel\\nfrom lshashing import LSHRandom\\nimport numpy as np\\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ntest_items= [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\nquery = \"the shape of earth\"\\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\\nembedded_query = np.array(model.get_embeddings([query])[0].values)\\n#Naive brute force search\\nn_neighbors=2\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'brute\\').fit(embedded_test_items) \\nnaive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'ball_tree\\').fit(embedded_test_items) \\ndistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#LSH\\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\\n#ANN retrieved the same ranking of items as brute force in a much scalable manner\\nSnippet 8. Using scikit-learn 29  and lshashing 30  for ANN with LSH, KD/Ball-tree and linear search\\nHashing and tree-based approaches can also be combined and extended upon to obtain \\nthe optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and \\nScaNN are good examples.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 32, 'page_label': '33'}, page_content='Embeddings & Vector Stores\\n33\\nSeptember 2024\\nHierarchical navigable small worlds \\nFigure 12. Diagram showing how HNSW ‘zooms in’ to perform ANN\\nOne of the FAISS (Facebook AI similarity search) implementations leverages the concept \\nof hierarchical navigable small world (HNSW) 31 to perform vector similarity search in sub-\\nlinear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a \\nhierarchical structure where the graph links are spread across different layers. The top layer \\nhas the longest links and the bottom layer has the shortest ones. As shown in Figure 9, the \\nsearch starts at the topmost layer where the algorithm greedily traverses the graph to find \\nthe vertex most semantically similar to the query. Once the local minimum for that layer is \\nfound, it then switches to the graph for the closest vertex on the layer below. This process \\ncontinues iteratively until the local minimum for the lowest layer is found, with the algorithm \\nkeeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm \\ncan be optionally augmented with quantization and vector indexing to boost speed and \\nmemory efficiency.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 33, 'page_label': '34'}, page_content='Embeddings & Vector Stores\\n34\\nSeptember 2024\\nimport faiss\\nM=32 #creating high degree graph:higher recall for larger index & searching time\\nd=768 # dimensions of the vectors/embeddings\\nindex = faiss.IndexHNSWFlat(d, M)\\nindex.add(embedded_test_items) #build the index using the embeddings in Snippet 9\\n#execute the ANN search\\nindex.search(np.expand_dims(embedded_query, axis=0), k=2)\\nSnippet 9. Indexing and executing ANN search with the FAISS library using HNSW\\nScaNN\\nGoogle developed the scalable approximate nearest neighbor (ScaNN)32,33 approach which is \\nused across a lot of its products and services. This includes being externally available to all \\ncustomers of Google Cloud through the Vertex AI Vector Search. Below is how ScaNN uses \\na variety of steps to perform efficient vector search, with each one of them having their own \\nsubset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple \\nalgorithms available to partition the vector store into logical partitions/clusters where \\nthe semantically related are grouped together. The partitioning step is optional for small \\ndatasets. However, for larger datasets with >100k embedding vectors, the partitioning step \\nis crucial since by pruning the search space it cuts down the search space by magnitudes \\ntherefore significantly speeds up the query. The space pruning is configured through the \\nnumber of partitions and the number of partitions to search. A larger number leads to better \\nrecall but larger partition creation time. A good heuristic is to set the number of partitions to \\nbe the square root of the number of vectors.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 34, 'page_label': '35'}, page_content='Embeddings & Vector Stores\\n35\\nSeptember 2024\\nFigure 13. Search space partitioning & pruning(left) & Approximate scoring (right)\\nAt query time ScaNN uses the user-specified distance measure to select the specified \\nnumber of top partitions (a value specified by the user), and then executes the scoring \\nstep next. In this step ScaNN compares the query with all the points in the top partitions \\nand selects the top K’. This distance computation can be configured as exact distance or \\napproximate distance. The approximate distance computation leverages either standard \\nproduct quantization or anisotropic quantization techniques, the latter of which is a specific \\nmethod employed by ScaNN which gives the better speed and accuracy tradeoffs.\\nFinally, as a last step the user can optionally choose to rescore the user specified top K \\nnumber of results more accurately. This results in an industry leading speed/accuracy \\ntradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a \\ncode example.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 35, 'page_label': '36'}, page_content='Embeddings & Vector Stores\\n36\\nSeptember 2024\\nFigure 14. Accuracy/speed tradeoffs for various SOTA ANN search algorithms\\nimport tensorflow as tf\\nimport tensorflow_recommenders as tfrs\\nfrom vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\\n# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\\nembedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\\nembedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\\nembedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\\n# Build index from tensorflow dataset and execute ANN search based on dot product metric\\nscann = tfrs.layers.factorized_top_k.ScaNN( \\n  distance_measure= \\'dot_product\\',\\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\\nscann = scann.index_from_dataset(embedded_docs)\\nscann(embedded_query, k=2)'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 36, 'page_label': '37'}, page_content='Embeddings & Vector Stores\\n37\\nSeptember 2024\\nSnippet 10. Using Tensorflow Recommenders 34  to perform ANN search using the ScaNN algorithm\\nIn this whitepaper we have seen both State-of-the-Art SOTA and traditional ANN search \\nalgorithms: ScaNN, FAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/\\naccuracy tradeoffs that they provide. However, to use these algorithms they need to \\nbe deployed in a scalable, secure and production-ready manner. For that we need \\nvector databases.\\nVector databases \\nVector embeddings embody semantic meanings of data, while vector search algorithms \\nprovide a means for efficiently querying them. Historically traditional databases lacked the \\nmeans to combine semantic meaning and efficient querying  in a way that the most relevant \\nembeddings can be both stored, queried, and retrieved in a secure, scalable, and flexible \\nmanner for complex analysis and real-time enterprise grade applications. This is what \\ngave rise to vector databases, which are built ground-up to manage these embeddings for \\nproduction scenarios. Due to the recent popularity of Generative AI, an increasing number \\nof traditional databases are starting to incorporate supporting vector search functionality \\nas well in addition to traditional search (‘hybrid search’) functionalities. Let’s look at the \\nworkflow for a simple Vector Database, with hybrid search capabilities.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 37, 'page_label': '38'}, page_content='Embeddings & Vector Stores\\n38\\nSeptember 2024\\nFigure 15. Populating and querying vector databases\\nEach vector database differs in its implementation, but the general flow is shown in Figure 15:\\n1. An appropriate trained embedding model is used to embed the relevant data points as \\nvectors with fixed dimensions. \\n2. The vectors are then augmented with appropriate metadata and complementary \\ninformation (such as tags) and indexed using the specified algorithm for efficient search.\\n3. An incoming query gets embedded with the same model, and used to query and return  \\nspecific amounts of the most semantically similar items and their associated unembedded \\ncontent/metadata. Some databases might provide caching and pre-filtering (based on \\ntags) and post-filtering capabilities (reranking using another more accurate model) to \\nfurther enhance the query speed and performance.\\nThere are quite a few vector databases available today, each tailored to different business \\nneeds and considerations. A few good examples of commercially managed vector databases \\ninclude Google Cloud’s Vertex Vector Search,35 Google Cloud’s AlloyDB & Cloud SQL \\nPostgres ElasticSearch,36 and Pinecone37 to name a few. Vertex AI Vector Search is a vector \\ndatabase built by Google that uses the ScaNN algorithm for fast vector search, while still \\nmaintaining all the security and access guarantees of Google Cloud. AlloyDB & Cloud SQL \\nPostgres supports vector search through the OSS pgvector38 extension, which allows for \\nSQL queries to combine ANN search with traditional predicates and the usual transactional \\nsemantics for ANN search index. AlloyDB also has a ScaNN index extension that is a native \\nimplementation of ScaNN and is pgvector-compatible. Similarly, many of the other traditional \\ndatabases have also started to add plugins to enable vector search. Pinecone and Weaviate \\nleverage HNSW for their fast vector search in addition to the ability to filter data using'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 38, 'page_label': '39'}, page_content='Embeddings & Vector Stores\\n39\\nSeptember 2024\\ntraditional search. Amongst their open source peers: Weaviate39 and ChromaDB40 provide a \\nfull suite of functionality upon deployment and can be tested in memory as well during the \\nprototyping phase.\\nOperational considerations\\nVector Databases are critical to managing the majority of technical challenges that arise \\nwith storing and querying embeddings at scale. Some of these challenges are specific to the \\nnature of vector stores, while others overlap with that of traditional databases. These include \\nhorizontal and vertical scalability, availability, data consistency, real time updates, backups, \\naccess control, compliance, and much more. However, there are also many more challenges \\nand considerations you need to take into account while using embedding and vector stores.\\nFirstly, embeddings, unlike traditional content, can mutate over time. This means that the \\nsame text, image, video or other content could and should be embedded using different \\nembedding models to optimize for the performance of the downstream applications. This is \\nespecially true for embeddings of supervised models after the model is retrained to account \\nfor various drifts or changing objectives. Similarly, the same applies to unsupervised models \\nwhen they are updated to a newer model. However, frequently updating the embeddings \\n- especially those trained on large amounts of data - can be prohibitively expensive. \\nConsequently, a balance needs to be struck. This necessitates a well-defined automated \\nprocess to store, manage, and possibly purge embeddings from the vector databases taking \\nthe budget into consideration.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 39, 'page_label': '40'}, page_content='Embeddings & Vector Stores\\n40\\nSeptember 2024\\nSecondly, while embeddings are great at representing semantic information, sometimes they \\ncan be suboptimal at representing literal or syntactic information. This is especially true for \\ndomain-specific words or IDs. These values are potentially missing or underrepresented \\nin the data the embeddings models were trained on. For example, if a user enters a query \\nthat contains the ID of a specific number along with a lot of text, the model might find \\nsemantically similar neighbors which match the meaning of the text closely, but not the ID, \\nwhich is the most important component in this context. You can overcome this challenge by \\nusing a combination of full-text search to pre-filter or post-filter the search space before \\npassing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which \\nthe semantic query occurs, it might be worth relying on different vector databases. For \\nexample, for OLTP workloads that require frequent reads/write operations, an operational \\ndatabase like Postgres or CloudSQL is the best choice. For large-scale OLAP analytical \\nworkloads and batch use cases, using Bigquery’s vector search is preferable.\\nIn conclusion, a variety of factors need to be considered when choosing a vector database. \\nThese factors include size and type of your dataset (some are good at sparse and others \\ndense), business needs, the nature of the workload,  budget, security, privacy guarantees, \\nthe needs for semantic and syntactic search as well as the database systems that are already \\nin use. In this section we have seen the various ANN search approaches as well the need and \\nbenefits of vector databases. The next section demonstrates an example of using a Vector AI \\nVector Search for semantic search.\\nApplications\\nEmbeddings models are one of the fundamental machine learning models that power a \\nvariety of applications. We summarize some popular applications in the following table.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 40, 'page_label': '41'}, page_content='Embeddings & Vector Stores\\n41\\nSeptember 2024\\nTask Description\\nRetrieval\\nGiven a query and a set of objects (for example, documents, images, \\nand videos), retrieve the most relevant objects. Based on the definition \\nof relevant objects, the subtasks include question answering and \\nrecommendations.\\nSemantic text similarity\\nDetermine whether two sentences have the same semantic meaning. \\nThe subtasks include: paraphrasing, duplicate detection, and bitext \\nmining.\\nClassification\\nClassify objects into possible categories. Based on the number of labels, \\nthe subtasks include binary classification, multi-class classification, and \\nmultilabel classifications.\\nClustering Cluster objects together.\\nReranking Rerank a set of objects based on a certain query.\\n \\nEmbeddings together with vector stores providing ANN can be powerful tools which can be \\nused for a variety of applications. These include Retrieval augmented Generation for LLMs, \\nSearch, Recommendation Systems, Anomaly detection, few shot- classification and much \\nmore. \\nFor ranking problems like search and recommendations, embeddings are normally used \\nat the first stage of the process. They retrieve the potentially good candidates that are \\nsemantically similar and consequently improve the relevance of search results. Since the \\namount of information to sort through can be quite large (in some cases even millions or \\nbillions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. \\nLet’s look at an application which combines both LLMs and RAG to help answer questions.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 41, 'page_label': '42'}, page_content='Embeddings & Vector Stores\\n42\\nSeptember 2024\\nQ & A with sources (retrieval augmented generation)\\nRetrieval augmented generation (RAG) for Q&A is a technique that combines the best of both \\nworlds from retrieval and generation. It first retrieves relevant documents from a knowledge \\nbase and then uses prompt expansion to generate an answer from those documents. Prompt \\nexpansion is a technique that when combined with database search can be very powerful. \\nWith prompt expansion the model retrieves relevant information from the database (mostly \\nusing a combination of semantic search and business rules), and augments the original \\nprompt with it. The model uses this augmented prompt to generate much more interesting, \\nfactual, and informative content than with retrieval or generation alone.\\nRAGs can help with a common problem with LLMs: their tendency to ‘hallucinate’ and \\ngenerate factually incorrect but plausible sounding responses. Although RAG can reduce \\nhallucinations, it does not completely eliminate them. What can help mitigate this problem \\nfurther is to also return the sources from the retrieval and do a quick coherence check either \\nby a human or an LLM. This ensures the LLM response is consistent with the semantically \\nrelevant sources. Let’s look at an example (Snippet 11) of RAG with sources, which can be \\nscalably implemented using Vertex AI LLM text embeddings and Vertex AI Vector Search in \\nconjunction with libraries like langchain.41'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 42, 'page_label': '43'}, page_content='Embeddings & Vector Stores\\n43\\nSeptember 2024\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai\\n# after running pip install make sure you restart your kernel\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = \"<my_project_id>\"\\nREGION = \"<my_region>\"\\nBUCKET = \"<my_gcs_bucket>\"\\nBUCKET_URI = f\"gs://{BUCKET}\"\\n# The number of dimensions for the textembedding-gecko@004 is 768\\n# If other embedder is used, the dimensions would probably need to change.\\nDIMENSIONS = 768\\n# Index Constants\\nDISPLAY_NAME = \"<my_matching_engine_index_id>\"\\nDEPLOYED_INDEX_ID = \"yourname01\" # you set this. Start with a letter.\\nfrom google.cloud import aiplatform\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\nfrom langchain_google_vertexai import VertexAI\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom IPython.display import display, Markdown\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nembedding_model = VertexAIEmbeddings(model_name= \"textembedding-gecko@003\")\\n# NOTE : This operation can take upto 30 seconds\\nmy_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\\n    display_name=DISPLAY_NAME,\\n    dimensions=DIMENSIONS,\\n    approximate_neighbors_count=150,\\n    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\\n    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\\n)\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 43, 'page_label': '44'}, page_content='Embeddings & Vector Stores\\n44\\nSeptember 2024\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\n# retrieve the id of the most recently deployed index or manually look up the index \\ndeployed above\\nindex_id=my_index_endpoint.deployed_indexes[-1].index.split(\"/\")[-1]\\nendpoint_id= my_index_endpoint.name\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(index_id)\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)\\n# Input texts\\ntexts= [\\n   \"The earth is spherical.\",\\n   \"The earth is a planet.\",\\n   \"I like to eat at a restaurant.\",\\n]\\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 44, 'page_label': '45'}, page_content='Embeddings & Vector Stores\\n45\\nSeptember 2024\\nSnippet 11. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\nretriever=vector_store.as_retriever(search_kwargs={\\'k\\':1 })\\n#create custom prompt for your use case\\nprompt_template=\"\"\"You are David, an AI knowledge bot.\\nAnswer the questions using the facts provided. Use the provided pieces of context to answer\\nthe users question.\\nIf you don\\'t know the answer, just say that \"I don\\'t know\", don\\'t try to make up an answer.\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(prompt_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\"question\": prompt}\\n#initialize your llm model\\nllm = VertexAI(model_name=\"gemini-pro\")\\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \\nretriever=retriever, return_source_documents=True)\\n#print your results with Markup language\\ndef print_result(result):\\n  output_text = f\"\"\"### Question:\\n  {query}\\n  ### Answer:\\n  {result[\\'result\\']}\\n  ### Source:\\n  {\\' \\'.join(list(set([doc.page_content for doc in result[\\'source_documents\\']])))}\\n  \"\"\"\\n  return(output_text)\\nchain= \"What shape is the planet where humans live?\"\\nresult = chain(query)\\ndisplay(Markdown(print_result(result)))'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 45, 'page_label': '46'}, page_content='Embeddings & Vector Stores\\n46\\nSeptember 2024\\nFigure 16. Model responses along with sources demonstrating the LLM being grounded in the database\\nAs we can infer from Figure 16, the output not only grounds LLM in the semantically similar \\nresults retrieved from the database (hence refusing to answer when context cannot be found \\nin the database). This not only significantly reduces hallucination, but also provides sources \\nfor verification, either human or using another LLM.\\nSummary\\nIn this whitepaper we have discussed various methods to create, manage, store, and retrieve \\nembeddings of various data modalities effectively in the context of production-grade \\napplications. Creating, maintaining and using embeddings for downstream applications can \\nbe a complex task that involves several roles in the organization. However, by thoroughly \\noperationalizing and automating its usage, you can safely leverage the incredible benefits \\nthey offer across some of the most important applications. Some key takeaways from this \\nwhitepaper include:\\n1. Choose your embedding model wisely for your data and use case. Ensure the data used in \\ninference is consistent with the data used in training. The distribution shift from training to \\ninference can come from various areas, including domain distribution shift or downstream \\ntask distribution shift. If no existing embedding models fit the current inference data \\ndistribution, fine-tuning the existing model can significantly help on the performance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 46, 'page_label': '47'}, page_content='Embeddings & Vector Stores\\n47\\nSeptember 2024\\nAnother tradeoff comes from the model size. The large deep neural network (large \\nmultimodal models) based models usually have better performance but can come with a \\ncost of longer serving latency. Using Cloud-based embedding services can conquer the \\nabove issue by providing both high-quality and low-latency embedding service. For most \\nbusiness applications using a pre-trained embedding model provides a good baseline, \\nwhich can be further fine-tuned or integrated in downstream models. In case the data has \\nan inherent graph structure, graph embeddings can provide superior performance.\\n2. Once your embedding strategy is defined, it’s important to make the choice of the \\nappropriate vector database that suits your budget and business needs. It might seem \\nquicker to prototype with available open source alternatives, but opting for a more secure, \\nscalable, and battle-tested managed vector database is certain to be better off in the long \\nterm. There are various open source alternatives using one of the many powerful ANN \\nvector search algorithms, but ScaNN and HNSW have proven to provide some of the best \\naccuracy and performance trade offs in that order.\\n3. Embeddings combined with an appropriate ANN powered vector database is an \\nincredibly powerful tool and can be leveraged for various applications, including \\nSearch, Recommendation systems, and Retrieval augment generation for LLMs. This \\napproach can mitigate the hallucination problem and bolster verifiability and trust of \\nLLM-based systems.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 47, 'page_label': '48'}, page_content=\"Embeddings & Vector Stores\\n48\\nSeptember 2024\\nEndnotes\\n1. Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. \\nAvailable at: https://doi.org/10.1007/978-981-15-6198-6_18\\n2. Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] \\nAvailable at: https://nlp.stanford.edu/pubs/glove.pdf .\\n3. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings \\nby noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215 .\\n4. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations \\nin vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf .\\n5. Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available \\nat: https://radimrehurek.com/gensim/intro.html .\\n6. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. \\nArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606.\\n7. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent \\nsemantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.\\n8. Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. \\nGhahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available \\nat: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html .\\n9. Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, \\nabs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .\\n10. Le, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. \\nAvailable at: https://arxiv.org/abs/1405.4053 .\\n11. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers \\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the \\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), \\npp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/ .\\n12. Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge \\ndistillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/ .\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 48, 'page_label': '49'}, page_content='Embeddings & Vector Stores\\n49\\nSeptember 2024\\n13. Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, \\nabs/2104.08821. Available at: https://arxiv.org/abs/2104.08821 .\\n14. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by \\nweakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279 .\\n15. Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late \\ninteraction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025 .\\n16. Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token \\nretrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .\\n17. TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available \\nat: https://tfhub.dev/.\\n18. Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  \\nGoogle Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing  \\n- bigquery-text-embeddings .\\n19. Google Cloud, 2024, Get multimodal embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings .\\n20. Pinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection .\\n21. Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph \\nembedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge \\nManagement. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568 .\\n22. Cai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, \\ntechniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf.\\n23. Hamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  \\nIn Advances in Neural Information Processing Systems 30. Available at:  \\nhttps://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf .\\n24. Dong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder \\narchitectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120 .\\n25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings .'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 49, 'page_label': '50'}, page_content='Embeddings & Vector Stores\\n50\\nSeptember 2024\\n26. TensorFlow, 2021, TensorFlow Models: NLP, Fine-tune BERT. Available at:  \\nhttps://www.tensorflow.org/tfmodels/nlp/fine_tune_bert .\\n27. Matsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), \\nArticle 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf .\\n28. Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at:  \\nhttps://dl.acm.org/doi/pdf/10.1145/355744.355745 .\\n29. Scikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. \\nAvailable at: https://scikit-learn.org/.\\n30. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at:  \\nhttps://pypi.org/project/lshashing/ .\\n31. Malkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using \\nhierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  \\nhttps://arxiv.org/pdf/1603.09320.pdf .\\n32. Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n33. Guo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector \\nquantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf .\\n34. TensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender \\nsystem models. Available at: https://www.tensorflow.org/recommenders .\\n35. Google Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. \\nAvailable at: https://cloud.google.com/vertex-ai/docs/vector-search/overview .\\n36. Elasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  \\nhttps://www.elastic.co/elasticsearch/ .\\n37. Pinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  \\nhttps://www.pinecone.io .\\n38. pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  \\nhttps://github.com/pgvector/pgvector.\\n39. Weaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T14:32:34-07:00', 'moddate': '2024-11-12T14:32:40-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Embeddings & vector stores.pdf', 'total_pages': 51, 'page': 50, 'page_label': '51'}, page_content='Embeddings & Vector Stores\\n51\\nSeptember 2024\\n40. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/ .\\n41. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language \\nmodel. Available at: https://langchain.com .'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 0, 'page_label': '1'}, page_content='Foundational \\nLarge Language \\nModels & \\nText Generation\\nAuthors: Mohammadamin Barektain,  \\nAnant Nawalgaria, Daniel J. Mankowitz,  \\nMajd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \\nMatan Kalman, Elena Buchatskaya,                                     \\nAliaksei Severyn, and Antonio Gulli'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 1, 'page_label': '2'}, page_content='Foundational Large Language Models & Text Generation\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nAdam Sadvovsky\\nYonghui Wu\\nAndrew Dai\\nEfi Kokiopolou\\nChuck Sugnet\\nAleksey Vlasenko\\nErwin Huizenga\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nMark Iverson\\nDesigner\\nMichael Lanning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nWhy language models are important 7\\nLarge language models 8\\n Transformer 9\\n  Input preparation and embedding 11\\n  Multi-head attention 12\\n   Understanding self-attention 12\\n   Multi-head attention: power in diversity 14\\n  Layer normalization and residual connections 15\\n  Feedforward layer  15\\n  Encoder and decoder 16\\n  Training the transformer 17\\n   Data preparation 17\\n   Training and loss function 18\\nThe evolution of transformers 19\\n GPT-1 19\\n BERT 21\\n GPT-2 22\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 3, 'page_label': '4'}, page_content='GPT-3/3.5/4 23\\n LaMDA 24\\n Gopher 25\\n GLaM 26\\n Chinchilla 27\\n PaLM 28\\n  PaLM 2 29\\n Gemini 29\\n Other open models 32\\n Comparison 34\\nFine-tuning large language models 37\\n Supervised fine-tuning  38\\n Reinforcement learning from human feedback 39\\n Parameter Efficient Fine-Tuning 41\\nUsing large language models 44\\n Prompt engineering  44\\n Sampling Techniques and Parameters 45\\nAccelerating inference 46\\n Trade offs 47\\n  The Quality vs Latency/Cost Tradeoff 48\\n  The Latency vs Cost Tradeoff 48\\n Output-approximating methods 49\\n  Quantization 49'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 4, 'page_label': '5'}, page_content='Distillation 50\\n Output-preserving methods 52\\n  Flash Attention 52\\n  Prefix Caching 53\\n  Speculative Decoding 55\\n Batching and Parallelization 57\\nApplications 58\\n Code and mathematics 61\\n Machine translation 62\\n Text summarization 63\\n Question-answering 63\\n Chatbots 64\\n Content generation 65\\n Natural language inference 65\\n Text classification 66\\n Text analysis 67\\n Multimodal applications 68\\nSummary 69\\nEndnotes 71'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 5, 'page_label': '6'}, page_content='Foundational Large Language Models & Text Generation\\n6\\nSeptember 2024\\nIntroduction\\nThe advent of Large Language Models (LLMs) represents a seismic shift in the world of \\nartificial intelligence. Their ability to process, generate, and understand user intent is \\nfundamentally changing the way we interact with information and technology. \\nAn LLM is an advanced artificial intelligence system that specializes in processing, \\nunderstanding, and generating human-like text. These systems are typically implemented as \\na deep neural network and are trained on massive amounts of text data. This allows them to \\nlearn the intricate patterns of language, giving them the ability to perform a variety of tasks, \\nlike machine translation, creative text generation, question answering, text summarization, \\nand many more reasoning and language oriented tasks. This whitepaper dives into the \\ntimeline of the various architectures and approaches building up to the large language \\nmodels and the architectures being used at the time of publication. It also discusses fine-\\nWe believe that this new crop of \\ntechnologies has the potential to \\nassist, complement, empower, \\nand inspire people at any time \\nacross almost any field.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 6, 'page_label': '7'}, page_content='Foundational Large Language Models & Text Generation\\n7\\nSeptember 2024\\ntuning techniques to customize an LLM to a certain domain or task, methods to make the \\ntraining more efficient, as well as methods to accelerate inference. These are then followed \\nby various applications and code examples. \\nWhy language models are important\\nLLMs achieve an impressive performance boost from the previous state of the art across \\na variety of different and complex tasks which require answering questions or complex \\nreasoning, making feasible many new applications. These include language translation, code \\ngeneration and completion, text generation, text classification, and question-answering, \\nto name a few. Although foundational LLMs trained in a variety of tasks on large amounts \\nof data perform very well out of the box and display emergent behaviors (e.g. the ability to \\nperform tasks they have not been directly trained for) they can also be adapted to solve \\nspecific tasks where performance out of the box is not at the level desired through a process \\nknown as fine-tuning. This requires significantly less data and computational resources than \\ntraining an LLM from scratch. LLMs can be further nudged and guided towards the desired \\nbehavior by the discipline of prompt engineering: the art and science of composing the \\nprompt and the parameters of an LLM to get the desired response.\\nThe big question is: how do these large language models work? The next section explores the \\ncore building blocks of LLMs, focusing on transformer architectures and their evolution from \\nthe original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \\nmost capable LLM. We also cover training and fine-tuning techniques, as well as methods to \\nimprove the speed of response generation. The whitepaper concludes with a few examples \\nof how language models are used in practice.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 7, 'page_label': '8'}, page_content='Foundational Large Language Models & Text Generation\\n8\\nSeptember 2024\\nLarge language models\\nA language model predicts the probability of a sequence of words. Commonly, when given \\na prefix of text, a language model assigns probabilities to subsequent words. For example, \\ngiven the prefix “The most famous city in the US is…”, a language model might predict high \\nprobabilities to the words “New York” and “Los Angeles” and low probabilities to the words \\n“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \\nmodern language models are often based on neural models, such as transformers.\\nBefore the invention of transformers1, recurrent neural networks (RNNSs) were the popular \\napproach for modeling sequences. In particular, “long short-term memory” (LSTM) and \\n“gated recurrent unit” (GRU) were common architectures.3 This area includes language \\nproblems such as machine translation, text classification, text summarization, and question-\\nanswering, among others. RNNs process input and output sequences sequentially. They \\ngenerate a sequence of hidden states based on the previous hidden state and the current \\ninput. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \\nduring training (though recent work in state space modeling is attempting to overcome \\nthese challenges).\\nTransformers, on the other hand, are a type of neural network that can process sequences \\nof tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \\ncan better model long-term contexts and are easier to parallelize than RNNs. This makes \\nthem significantly faster to train, and more powerful compared to RNNs for handling long-\\nterm dependencies in long sequence tasks. However, the cost of self-attention in the original \\ntransformers is quadratic in the context length which limits the size of the context, while \\nRNNs have a theoretically infinite context length. Transformers have become the most \\npopular approach for sequence modeling and transduction problems in recent years.\\nHerein, we discuss the first version of the transformer model and then move on to the more \\nrecent advanced models and algorithms.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 8, 'page_label': '9'}, page_content='Foundational Large Language Models & Text Generation\\n9\\nSeptember 2024\\nTransformer\\nThe transformer architecture was developed at Google in 2017 for use in a translation model.1 \\nIt’s a sequence-to-sequence model capable of converting sequences from one domain \\ninto sequences in another domain. For example, translating French sentences to English \\nsentences. The original transformer architecture consists of two parts: an encoder and a \\ndecoder. The encoder converts the input text (e.g., a French sentence) into a representation, \\nwhich is then passed to the decoder. The decoder uses this representation to generate the \\noutput text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \\nthe transformer encoder is linear in the size of its input. Figure 1 shows the design of the \\noriginal transformer architecture.\\nThe transformer consists of multiple layers. A layer in a neural network comprises a set of \\nparameters that perform a specific transformation on the data. In the diagram you can see \\nan example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \\nLinear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \\nThe input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \\nnetwork. Input embeddings are used to represent the input tokens to the model. Output \\nembeddings are used to represent the output tokens that the model predicts. For example, in \\na machine translation model, the input embeddings would represent the words in the source \\nlanguage, while the output embeddings would represent the words in the target language. \\nThe output layer (e.g., Softmax) is the final layer that produces the output of the network. The \\nhidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \\nwhere the magic happens!'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 9, 'page_label': '10'}, page_content='Foundational Large Language Models & Text Generation\\n10\\nSeptember 2024\\nFigure 1. Original Transformer 1 (P.C: 5 )'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 10, 'page_label': '11'}, page_content='Foundational Large Language Models & Text Generation\\n11\\nSeptember 2024\\nTo better understand the different layers in the transformer, let’s use a French-to-English \\ntranslation task as an example. Here, we explain how a French sentence is input into the \\ntransformer and a corresponding English translation is output. We will also describe each of \\nthe components inside the transformer from Figure 1.\\nInput preparation and embedding\\nTo prepare language inputs for transformers, we convert an input sequence into tokens and \\nthen into input embeddings. At a high level, an input embedding is a high-dimensional vector \\nthat represents the meaning of each token in the sentence. This embedding is then fed into \\nthe transformer for processing. Generating an input embedding involves the following steps:\\n1. Normalization (Optional): Standardizes text by removing redundant whitespace, \\naccents, etc.\\n2. Tokenization: Breaks the sentence into words or subwords and maps them to integer \\ntoken IDs from a vocabulary.\\n3. Embedding: Converts each token ID to its corresponding high-dimensional vector, \\ntypically using a lookup table. These can be learned during the training process.\\n4. Positional Encoding: Adds information about the position of each token in the sequence \\nto help the transformer understand word order.\\nThese steps help to prepare the input for the transformers so that they can better \\nunderstand the meaning of the text.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 11, 'page_label': '12'}, page_content='Foundational Large Language Models & Text Generation\\n12\\nSeptember 2024\\nMulti-head attention\\nAfter converting input tokens into embedding vectors, you feed these embeddings into \\nthe multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \\ntransformers; it enables them to focus on specific parts of the input sequence relevant to \\nthe task at hand and to capture long-range dependencies within sequences more effectively \\nthan traditional RNNs. \\nUnderstanding self-attention\\nConsider the following sentence: “The tiger jumped out of a tree to get a drink because it \\nwas thirsty.” Self-attention helps to determine relationships between different words and \\nphrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \\nso we would expect these two words to be strongly connected. Self-attention achieves this \\nthrough the following steps (Figure 2):\\n1. Creating queries, keys, and values: Each input embedding is multiplied by three learned \\nweight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \\nare like specialized representations of each word.\\n• Query: The query vector helps the model ask, “Which other words in the sequence are \\nrelevant to me?”\\n• Key: The key vector is like a label that helps the model identify how a word might be \\nrelevant to other words in the sequence.\\n• Value: The value vector holds the actual word content information.\\n2. Calculating scores: Scores are calculated to determine how much each word should \\n‘attend’ to other words. This is done by taking the dot product of the query vector of one \\nword with the key vectors of all the words in the sequence.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 12, 'page_label': '13'}, page_content='Foundational Large Language Models & Text Generation\\n13\\nSeptember 2024\\n3. Normalization: The scores are divided by the square root of the key vector dimension (dk) \\nfor stability, then passed through a softmax function to obtain attention weights. These \\nweights indicate how strongly each word is connected to the others.\\n4. Weighted values: Each value vector is multiplied by its corresponding attention weight. \\nThe results are summed up, producing a context-aware representation for each word.\\nFigure 2. The process of computing self-attention in the multi-head attention module 1 (P.C: 5 )'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 13, 'page_label': '14'}, page_content='Foundational Large Language Models & Text Generation\\n14\\nSeptember 2024\\nIn practice, these computations are performed at the same time, by stacking the query, key \\nand value vectors for all the tokens into Q, K and V matrices and multiplying them together as \\nshown in Figure 3.\\nFigure 3. The basic operation of attention, 1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \\nof queries and keys (P.C:5 )\\nMulti-head attention: power in diversity\\nMulti-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \\neach ‘head’ potentially focusing on different aspects of the input relationships. The outputs \\nfrom each head are concatenated and linearly transformed, giving the model a richer \\nrepresentation of the input sequence.\\nThe use of multi-head attention improves the model’s ability to handle complex language \\npatterns and long-range dependencies. This is crucial for tasks that require a nuanced \\nunderstanding of language structure and content, such as machine translation, text \\nsummarization, and question-answering. The mechanism enables the transformer to consider \\nmultiple interpretations and representations of the input, which enhances its performance on \\nthese tasks.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 14, 'page_label': '15'}, page_content='Foundational Large Language Models & Text Generation\\n15\\nSeptember 2024\\nLayer normalization and residual connections\\nEach layer in a transformer, consisting of a multi-head attention module and a feed-forward \\nlayer, employs layer normalization and residual connections. This corresponds to the Add \\nand Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \\ncorresponds to layer normalization. Layer normalization computes the mean and variance \\nof the activations to normalize the activations in a given layer. This is typically performed to \\nreduce covariate shift as well as improve gradient flow to yield faster convergence during \\ntraining as well as improved overall performance. \\nResidual connections propagate the inputs to the output of one or more layers. This has the \\neffect of making the optimization procedure easier to learn and also helps deal with vanishing \\nand exploding gradients. \\nThe Add and Norm layer is applied to both the multi-head attention module and the feed-\\nforward layer described in the following section.\\nFeedforward layer \\nThe output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \\nfed into the feedforward layer of each transformer block. This layer applies a position-wise \\ntransformation to the data, independently for each position in the sequence, which allows the \\nincorporation of additional non-linearity and complexity into the model’s representations. The \\nfeedforward layer typically consists of two linear transformations with a non-linear activation \\nfunction, such as ReLU or GELU, in between. This structure adds further representational \\npower to the model. After processing by the feedforward layer, the data undergoes \\nanother ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \\ntransformer models.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 15, 'page_label': '16'}, page_content='Foundational Large Language Models & Text Generation\\n16\\nSeptember 2024\\nEncoder and decoder\\nThe original transformer architecture relies on a combination of encoder and decoder \\nmodules. Each encoder and decoder consists of a series of layers, with each layer \\ncomprising key components: a multi-head self-attention mechanism, a position-wise feed-\\nforward network, normalization layers, and residual connections. \\nThe encoder’s primary function is to process the input sequence into a continuous \\nrepresentation that holds contextual information for each token. The input sequence is first \\nnormalized, tokenized, and converted into embeddings. Positional encodings are added to \\nthese embeddings to retain sequence order information. Through self-attention mechanisms, \\neach token in the sequence can dynamically attend to any other token, thus understanding \\nthe contextual relationships within the sequence. The output from the encoder is a series of \\nembedding vectors Z representing the entire input sequence. \\nThe decoder is tasked with generating an output sequence based on the context provided \\nby the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\\nof-sequence token. The decoder layers employ two types of attention mechanisms: masked \\nself-attention and encoder-decoder cross-attention. Masked self-attention ensures that \\neach position can only attend to earlier positions in the output sequence, preserving the \\nauto-regressive property. This is crucial for preventing the decoder from having access to \\nfuture tokens in the output sequence. The encoder-decoder cross-attention mechanism \\nallows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \\nembeddings generated by the encoder. This iterative process continues until the decoder \\npredicts an end-of-sequence token, thereby completing the output sequence generation.\\nMajority of recent LLMs adopted a decoder-only variant of transformer architecture. This \\napproach forgoes the traditional encoder-decoder separation, focusing instead on directly \\ngenerating the output sequence from the input. The input sequence undergoes a similar'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 16, 'page_label': '17'}, page_content='Foundational Large Language Models & Text Generation\\n17\\nSeptember 2024\\nprocess of embedding and positional encoding before being fed into the decoder. The \\ndecoder then uses masked self-attention to generate predictions for each subsequent \\ntoken based on the previously generated tokens. This streamlined approach simplifies the \\narchitecture for specific tasks where encoding and decoding can be effectively merged.\\nTraining the transformer\\nWhen talking about machine learning models, it’s important to differentiate between \\ntraining and inference. Training typically refers to modifying the parameters of the model, \\nand involves loss functions and backpropagation. Inference is when model is used only \\nfor the predicted output, without updating the model weights. The model parameters are \\nfixed during inference. Up until now we learned how transformers generate outputs during \\ninference. Next, we focus on how to train transformers to perform one or more given tasks.\\nData preparation\\nThe first step is data preparation, which involves a few important steps itself. First, clean the \\ndata by applying techniques such as filtering, deduplication, and normalization. The next \\nstep is tokenization where the dataset is converted into tokens using techniques such as \\nByte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \\nwhich is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \\n’language’ for processing and understanding text. Finally, the data is typically split into a \\ntraining dataset for training the model as well as a test dataset which is used to evaluate the \\nmodels performance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 17, 'page_label': '18'}, page_content='Foundational Large Language Models & Text Generation\\n18\\nSeptember 2024\\nTraining and loss function\\nA typical transformer training loop consists of several parts: First, batches of input \\nsequences are sampled from a training dataset. For each input sequence, there is a \\ncorresponding target sequence. In unsupervised pre-training, the target sequence is \\nderived from the input sequence itself. The batch of input sequences is then fed into the \\ntransformer. The transformer generates predicted output sequences. The difference \\nbetween the predicted and target sequences is measured using a loss function (often cross-\\nentropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \\nthe transformer’s parameters. This process is repeated until the transformer converges to a \\ncertain level of performance or until it has been trained on a pre-specified number of tokens. \\nThere are different approaches to formulating the training task for transformers depending \\non the architecture used:\\n• Decoder-only models are typically pre-trained on the language modeling task (e.g., see \\nendnote12, 13). The target sequence for the decoder is simply a shifted version of the input \\nsequence. Given a training sequence like ‘the cat sat on the mat’ various input/target \\npairs can be generated for the model. For example the input “the cat sat on” should \\npredict “the” and subsequently the input “the cat sat on the” should predict target \\nsequence “mat”.\\n• Encoder-only models (like BERT)14 are often pre-trained by corrupting the input sequence \\nin some way and having the model try to reconstruct it. One such approach is masked \\nlanguage modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \\non the mat” and the sequence target would be the original sentence.\\n• Encoder-decoder models (like the original transformer) are trained on sequence-to-\\nsequence supervised tasks such as translation (input sequence “Le chat est assis sur \\nle tapis” and target “The cat sat on the mat”), question-answering (where the input \\nsequence is a question and the target sequence is the corresponding answer), and'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 18, 'page_label': '19'}, page_content='Foundational Large Language Models & Text Generation\\n19\\nSeptember 2024\\nsummarization (where the input sequence is a full article and the target sequence is its \\ncorresponding summary). These models could also be trained in an unsupervised way by \\nconverting other tasks into sequence-to-sequence format. For example, when training \\non Wikipedia data, the input sequence might be the first part of an article, and the target \\nsequence comprises the remainder of the article.\\nAn additional factor to consider during training is the ‘context length’. This refers to the \\nnumber of previous tokens the model can ‘remember’ and use to predict the next token in \\nthe sequence. Longer context lengths allow the model to capture more complex relationships \\nand dependencies within the text, potentially leading to better performance. However, longer \\ncontexts also require more computational resources and memory, which can slow down \\ntraining and inference. Choosing an appropriate context length involves balancing these \\ntrade-offs based on the specific task and available resources.\\nThe evolution of transformers\\nThe next sections provide an overview of the various transformer architectures. These \\ninclude encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \\nGPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\\nGPT-1\\nGPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only model developed \\nby OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \\nseveral billion words) and is able to generate text, translate languages, write different kinds \\nof creative content, and answer questions in an informative way. The main innovations in \\nGPT-1 were:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 19, 'page_label': '20'}, page_content='Foundational Large Language Models & Text Generation\\n20\\nSeptember 2024\\n• Combining transformers and unsupervised pre-training: Unsupervised pre-training \\nis a process of training a language model on a large corpus of unlabeled data. Then, \\nsupervised data is used to fine-tune the model for a specific task, such as translation \\nor sentiment classification. In prior works, most language models were trained using a \\nsupervised learning objective. This means that the model was trained on a dataset of \\nlabeled data, where each example had a corresponding label. This approach has two main \\nlimitations. First, it requires a large amount of labeled data, which can be expensive and \\ntime-consuming to collect. Second, the model can only generalize to tasks that are similar \\nto the tasks that it was trained on. Semi-supervised sequence learning was one of the first \\nworks that showed that unsupervised pre-training followed by supervised training was \\nsuperior than supervised training alone.\\nUnsupervised pre-training addresses these limitations by training the model on a large \\ncorpus of unlabeled data. This data can be collected more easily and cheaply than labeled \\ndata. Additionally, the model can generalize to tasks that are different from the tasks that \\nit was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \\nwas used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \\nbooks, which provides the model with a large amount of data to learn from. Additionally, \\nthe corpus contains long stretches of contiguous text, which helps the model learn long-\\nrange dependencies. Overall, unsupervised pre-training is a powerful technique that can \\nbe used to train language models that are more accurate and generalizable than models \\nthat are trained using supervised learning alone. \\n• Task-aware input transformations: There are different kinds of tasks such as textual \\nentailment and question-answering that require a specific structure. For example, \\ntextual entailment requires a premise and a hypothesis; question-answering requires a \\ncontext document; a question and possible answers. One of the contributions of GPT-1 \\nis converting these types of tasks which require structured inputs into an input that the \\nlanguage model can parse, without requiring task-specific architectures on top of the \\npre-trained architecture. For textual entailment, the premise p and the hypothesis h are'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 20, 'page_label': '21'}, page_content='Foundational Large Language Models & Text Generation\\n21\\nSeptember 2024\\nconcatenated with a delimiter token ($) in between - [p, $, h]. For question answering, the \\ncontext document c is concatenated with the question q and a possible answer a with a \\ndelimiter token in between the question and answer - [c,q,$,a].\\nGPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \\nGPT-1 was a significant breakthrough in natural language processing (NLP), it had some \\nlimitations. For example, the model was prone to generating repetitive text, especially when \\ngiven prompts outside the scope of its training data. It also failed to reason over multiple \\nturns of dialogue and could not track long-term dependencies in text. Additionally, its \\ncohesion and fluency were limited to shorter text sequences, and longer passages would \\nlack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \\npre-training, which laid the foundation for larger and more powerful models based on the \\ntransformer architecture.\\nBERT\\nBERT14 which stands for Bidirectional Encoder Representations from Transformers, \\ndistinguishes itself from traditional encoder-decoder transformer models by being an \\nencoder-only architecture. Instead of translating or producing sequences, BERT focuses \\non understanding context deeply by training on a masked language model objective. In \\nthis setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \\nto predict the original word based on the surrounding context. Another innovative aspect \\nof BERT’s training regime is the next sentence prediction loss, where it learns to determine \\nwhether a given sentence logically follows a preceding one. By training on these objectives, \\nBERT captures intricate context dependencies from both the left and right of a word, and \\nit can discern the relationship between pairs of sentences. Such capabilities make BERT \\nespecially good at tasks that require natural language understanding, such as question-\\nanswering, sentiment analysis, and natural language inference, among others. Since this is an \\nencoder-only model, BERT cannot generate text.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 21, 'page_label': '22'}, page_content='Foundational Large Language Models & Text Generation\\n22\\nSeptember 2024\\nGPT-2\\nGPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \\nGPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \\nof its training dataset:\\n• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \\nconsists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \\nis a rating metric used on Reddit and a value of three means that all the posts were of a \\nreasonable level of quality.\\n• Parameters: GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \\nthan the previous model. More parameters increase the model’s learning capacity. The \\nauthors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \\n(GPT-2) parameters, and found that the model with the most parameters performed better \\non every subsequent task.\\nThis scaling up resulted in a model that was able to generate more coherent and realistic text \\nthan GPT-1. Its ability to generate human-like responses made it a valuable tool for various \\nnatural language processing tasks, such as content creation and translation. Specifically, \\nGPT-2 demonstrated significant improvement in capturing long-range dependencies and \\ncommon sense reasoning. While it performed well in some tasks, it did not outperform state-\\nof-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \\nachievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \\ntransfer is the ability of a model to generalize to a new task without being trained on it, which \\nrequires the model to understand the task based on the given instruction. For example, for \\nan English to German translation task, the model might be given an English sentence followed \\nby the word “German” and a prompt (“:”). The model would then be expected to understand \\nthat this is a translation task and generate the German translation of the English sentence. \\nGPT-2 was able to perform tasks such as machine translation, text summarization, and \\nreading comprehension without any explicit supervision.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 22, 'page_label': '23'}, page_content='Foundational Large Language Models & Text Generation\\n23\\nSeptember 2024\\nThe study discovered that performance on zero-shot tasks increased in a log-linear manner \\nas the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \\nmore parameters improved the model’s ability to understand tasks and surpass the state-of-\\nthe-art on many tasks in zero-shot settings.\\nGPT-3/3.5/4\\nGPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \\nsignificant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \\nflexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \\n175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \\nThis increase in model size allowed GPT-3 to store and recall an even more vast amount of \\ninformation, understand nuanced instructions, and generate more coherent and contextually \\nrelevant text over longer passages.\\nWhile GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \\nunderstand and execute tasks with just a few examples, or sometimes even without any \\nexplicit examples—simply based on the instruction provided. This highlights GPT-3’s more \\ndynamic understanding and adaptation abilities, reducing the need for task-specific fine-\\ntuning which was more prevalent in GPT-2.\\nFinally, GPT-3’s large model scale and diverse training corpus have led to better \\ngeneralization across a broader range of tasks. This means that out-of-the-box, without \\nany further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \\ntranslation to question-answering, compared to GPT-2. It’s also worth noting that the release \\napproach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \\nthey chose to make GPT-3 available as a commercial API, reflecting both its utility and the \\norganization’s evolving stance on deployment.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 23, 'page_label': '24'}, page_content='Foundational Large Language Models & Text Generation\\n24\\nSeptember 2024\\nInstruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\\ntuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \\nmodel behaviors. Outputs from this model were then ranked and it was then further fine-\\ntuned using Reinforcement Learning from Human Feedback. This led to improved instruction \\nfollowing in the model. A 1.3B parameter InstructGPT model had better human evaluations \\nthan the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \\nreductions in toxicity.\\nGPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \\nunderstanding and generating code. It’s been optimized for dialogue. And it’s capable of \\nreceiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \\ntokens. \\nGPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \\ntext inputs and producing text outputs.19 Specifically, accepting text or images as input \\nand outputting text. This model has broader general knowledge and advanced reasoning \\ncapabilities. It can receive context windows of up to 128,000 tokens and has a maximum \\noutput of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \\nacross diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \\nwithout specialized instructions. Its performance often matches or even exceeds human \\ncapabilities and significantly outperforms earlier models like GPT-3.5.\\nLaMDA\\nGoogle’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \\ncontribution to the arena of large-scale language models, designed primarily to engage in \\nopen-ended conversations. Unlike traditional chatbots which operate in more constrained \\nand predefined domains, LaMDA is engineered to handle a wide array of topics, delivering'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 24, 'page_label': '25'}, page_content='Foundational Large Language Models & Text Generation\\n25\\nSeptember 2024\\nmore natural and flowing conversations. LaMDA was trained on dialogue-focused data to \\nencourage ongoing conversational flow, not just isolated responses, ensuring users can have \\nmore extensive and explorative dialogues.\\nWhile GPT models, especially the later iterations like GPT-3, have strived to address a \\nmultitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \\nfocus is on maintaining and enhancing conversational depth and breadth. GPT models \\nshine on their ability to produce coherent long-form content and perform various tasks \\nwith minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \\nstriving to mimic the unpredictability and richness of human conversations. \\nGopher\\nGopher22 is a 280 billion parameter language model based on the decoder-only transformer \\narchitecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \\nwrite different kinds of creative content, and answer your questions in an informative way. \\nSimilar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\\n• Dataset: The researchers curated a high-quality text dataset called MassiveText, which \\ncontains over 10 terabytes of data and 2.45B documents from web pages, books, news \\narticles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \\nImportantly, they improved the quality of the data by filtering it, such as by removing \\nduplicate text and deduplicating similar documents. This significantly improved the \\nmodel’s performance on downstream tasks.\\n• Optimization: The researchers used a warmup learning rate for 1,500 steps and then \\ndecayed it using a cosine schedule. They also had an interesting rule that as they \\nincreased the model size, they decreased the learning rate and increased the number of \\ntokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \\nbased on the global gradient norm helped stabilize the training.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 25, 'page_label': '26'}, page_content='Foundational Large Language Models & Text Generation\\n26\\nSeptember 2024\\nGopher was evaluated on a variety of tasks, including mathematics, common sense, logical \\nreasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \\nGopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \\nGopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \\ntasks such as abstract algebra.\\nThe authors also conducted a study on the effect of model size on different types of \\ntasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \\nincreasing the number of parameters had a significant impact on logical reasoning and \\nreading comprehension, but it did not improve performance as much on tasks such as \\ngeneral knowledge, where performance eventually almost plateaued.\\nFigure 4. Ablation study 22  on the effect of model size on the performance of Gopher on different types \\nof tasks\\nGLaM\\nGLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \\nlanguage model. Mixture-of-experts based models are much more computationally efficient \\ngiven their parameter count. This is achieved by only activating a subset of their parameters'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 26, 'page_label': '27'}, page_content='Foundational Large Language Models & Text Generation\\n27\\nSeptember 2024\\n(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \\nof the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \\noverall performance compared to GPT-3.\\nChinchilla\\nUntil 2022, LLMs were primarily scaled by increasing the model size and using datasets that \\nare relatively small by current standards (up to 300 billion tokens for the largest models). \\nThis approach was informed by the Kaplan et al.24 study, which examined how performance \\nof a language model, measured by cross-entropy loss, varies with changes in computational \\nbudget, model size, and dataset size. Specifically, given a 100-fold increase in computational \\nresources (C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \\n(Nopt∝ C0.73 ), while increasing dataset size by only 3.5 times (Dopt∝ C0.27 ). \\nThe Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \\napproaches to find that near equal scaling in parameters and data is optimal with increasing \\ncompute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \\nboth data size and model size. \\nFigure 5. Overlaid predictions from three different approaches from Chinchilla paper, 25  along with \\nprojections from Kaplan et al 24'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 27, 'page_label': '28'}, page_content='Foundational Large Language Models & Text Generation\\n28\\nSeptember 2024\\nTo verify the updated scaling law, DeepMind trained a 70B parameter model (called \\nChinchilla) using the same compute budget as the previously trained Gopher model. \\nChinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \\nMegatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \\n4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \\nalso smaller.\\nThe findings of Chinchilla had significant ramifications for the development of future LLMs. \\nFocus shifted into finding ways to scale dataset size (while maintaining quality) alongside \\nincreasing parameter count. Extrapolating this trend suggests that training dataset size \\nmay soon be limited by the amount of text data available. This has led to new research by \\nMuennighoff et al.27 exploring scaling laws in data-constrained regimes.\\nPaLM\\nPathways language model (PaLM)28 is a 540-billion parameter transformer-based large \\nlanguage model developed by Google AI. It was trained on a massive dataset of text and \\ncode and is capable of performing a wide range of tasks, including common sense reasoning, \\narithmetic reasoning, joke explanation, code generation, and translation.\\nAt the time of its release, PaLM was also able to achieve state-of-the-art performance on \\nmany language benchmarks, for example GLUE and SuperGLUE.29\\nOne of the key features of PaLM is its ability to scale efficiently. This is thanks to the \\nPathways system, which Google developed to distribute the training of large language \\nmodels across two TPU v4 Pods.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 28, 'page_label': '29'}, page_content='Foundational Large Language Models & Text Generation\\n29\\nSeptember 2024\\nPaLM 2\\nPaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \\narchitectural and training enhancements, PaLM 2 is even more capable than PaLM, with \\nfewer total parameters. It excels at advanced reasoning tasks, including code generation, \\nmath, classification, question answering, and translation.\\nPaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \\nnumber of commercial models Google released as part of Google Cloud Generative AI.\\nGemini\\nFigure 6. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \\ntokenized and fed into its transformer model. The transformer generates an output that can contain images \\nand text'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 29, 'page_label': '30'}, page_content='Foundational Large Language Models & Text Generation\\n30\\nSeptember 2024\\nGemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \\ntake interleaved sequences of text, image, audio, and video as input. It’s built on top of \\ntransformer decoders and has architectural improvements for scale as well as optimized \\ninference on Google’s Tensor Processing Units (TPUs). In its current 1.5 version, these models \\nare trained to support contexts of different sizes, up to 2M tokens in the Gemini 1.5 Pro \\nversion on Vertex AI and employ mechanisms such as multi-query attention for efficiency. \\nGemini models also employ a Mixture of Experts architecture to optimize efficiency and \\ncapabilities of the models. Multimodality allows the models to process text, images and video \\nin input, with more modalities in input and output expected in the future.\\nThe Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \\nsize and configuration. The pre-training data consists of web documents, books, code, and \\nimage, audio, and video data. \\nLarger models are trained for the compute-optimal number of tokens using the same \\napproach as in Chinchilla paper,25 while small models are trained on significantly more tokens \\nthan compute optimal to improve performance for a given inference budget.\\nThe Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \\nNano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\\nart results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \\nGemini Nano is designed for on-device applications. The Gemini Nano models leverage \\nadvancements such as distillation to produce state-of-the-art performance for small \\nlanguage models on tasks such as summarization and reading comprehension. As the Gemini \\nmodels are natively multi-modal, it can be seen that training across multiple modalities does \\nindeed lead to a model that is capable of achieving strong capabilities in each domain.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 30, 'page_label': '31'}, page_content='Foundational Large Language Models & Text Generation\\n31\\nSeptember 2024\\nDuring the initial part of 2024, Google introduced the latest model of the Gemini family, \\nGemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \\nmodel  also dramatically increased the size of the context window to millions of tokens \\nand is capable of recalling and reasoning over those millions of tokens, including multiple \\nlong documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \\ncapabilities across different domains:\\n• Code understanding: It can process massive codebases and answer highly specific \\ncode-related questions.\\n• Language learning: The model can learn new languages never observed at training time \\nsolely based on reference materials provided within its input\\n• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \\nfrom the novel ‘Les Misérables’ based on a simple sketch.\\n• Video comprehension: It can analyze entire movies, answering detailed questions and \\npinpointing specific timestamps with remarkable accuracy.\\nGoogle’s Gemini 1.5 Pro model excels at retrieving information from even very long \\ndocuments. In their study,32 it demonstrated 100% recall on documents up to 530,000 \\ntokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \\naccuracy when finding information in documents up to 10 million tokens.\\nMoreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \\ninstructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \\nprevious Gemini models. The model accurately followed almost 90% of instructions and fully \\ncompleted 66% of the complex tasks.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 31, 'page_label': '32'}, page_content='Foundational Large Language Models & Text Generation\\n32\\nSeptember 2024\\nGemini Flash is a new addition to the Gemini model family and the fastest Gemini model \\nserved in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \\ncost-efficient to serve and features a breakthrough long context window of 1 million tokens. \\nAlthough it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \\nacross vast amounts of information and delivers impressive quality for its size.\\nFurthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \\nmodels built from the same research and technology used to create the Gemini models.33 The \\nfirst model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \\na massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \\nLLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \\na single GPU.\\nGemma 2,33 developed by Google AI, represents a significant advancement in the field of \\nopen large language models. Designed with a focus on efficiency, the 27-billion parameter \\nmodel boasts performance comparable to much larger models like Llama 3 70B33 on standard \\nbenchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \\ndevelopers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \\nto popular community tools, further enhances its versatility. With its strong performance, \\nefficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \\nand democratizing AI capabilities.\\nOther open models\\nThe landscape of open LLMs is rapidly evolving, with a growing number of models where \\nboth the code and pre-trained weights are publicly accessible. Below we highlight some of \\nthe known examples:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 32, 'page_label': '33'}, page_content='Foundational Large Language Models & Text Generation\\n33\\nSeptember 2024\\n• LLaMA 234: Released by Meta AI, LLaMA 2 is a family of pretrained and fine-tuned \\nLLMs ranging from 7B to 70B parameters. It shows significant improvements over its \\npredecessor, LLaMA 1, including a 40% larger pre-training dataset (2 trillion tokens), \\ndoubled context length (4096 tokens), and the use of grouped-query attention. The \\nfine-tuned version, LLaMA 2-Chat, is optimized for dialogue and shows competitive \\nperformance against closed-source models of the same size.\\n• LLaMA 3.221: Released by Meta AI, LLaMA 3.2 is the next generation of their open LLMs. \\nLlama 3.2 includes multilingual text-only models (1B, 3B) and vision LLMs (11B, 90B), with \\nquantized versions of 1B and 3B offering on average up to 56% smaller size and 2-3x \\nspeedup, ideal for on-device and edge deployments. LLaMA 3.2 utilizes grouped-query \\nattention and a 128K token vocabulary for enhanced performance and efficiency.\\n• Mixtral35: Developed by Mistral AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) \\nmodel. While its total parameter count is 47B, it utilizes only 13B active parameters per \\ntoken during inference, leading to faster inference and higher throughput. This model \\nexcels in mathematics, code generation, and multilingual tasks, often outperforming \\nLLaMA 2 70B in these domains. Mixtral also supports a 32k token context length, enabling \\nit to handle significantly longer sequences. Its instruction-tuned version, Mixtral 8x7B-\\nInstruct, surpasses several closed-source models on human evaluation benchmarks.\\n• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \\n72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \\nstrong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \\nLLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \\nlanguage understanding, reasoning, and math.\\n• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \\non a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \\nthrough rigorous cleaning and filtering processes. The 34B model achieves performance'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 33, 'page_label': '34'}, page_content='Foundational Large Language Models & Text Generation\\n34\\nSeptember 2024\\ncomparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\\ngrade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \\nvision-language model (Yi-VL), and a depth-upscaled 9B model.\\n• Grok-138: Developed by xAI, Grok-1 is a 314B parameter Mixture-of-Experts model with \\n25% of the weights active on a given token. It is the raw base model checkpoint from the \\npre-training phase and is not fine-tuned for specific tasks like dialogue. Grok-1 operates \\nwith a context length of 8k tokens.\\nThe pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \\nhave been many contributions to the field in both the academic and commercial settings. \\nWith over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \\nof the models and teams that have contributed to the development of LLMs. However, an \\nabbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \\nStanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \\nNVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai , Llama from \\nMeta mentioned above and many others. Some of notable companies developing commercial \\nfoundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \\nand many others in addition to Google and OpenAI mentioned in previous sections. It is \\nimportant when using a model to confirm that the license is appropriate for your use case as \\nmany models are provided with very specific terms of use.\\nComparison\\nIn this section, we observed how transformer-based language models have evolved. They \\nstarted as encoder-decoder architectures with hundreds of millions of parameters trained \\non hundreds of millions of tokens, and have grown to be massive decoder-only architectures \\nwith billions of parameters and trained on trillions of tokens. Table 1 shows how the \\nimportant hyperparameters for all the models discussed in this whitepaper have evolved'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 34, 'page_label': '35'}, page_content='Foundational Large Language Models & Text Generation\\n35\\nSeptember 2024\\nover time. The scaling of data and parameters has not only improved the performance of \\nLLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\\nshot generalizations to new tasks. However, even the best of these LLMs still have many \\nlimitations. For example, they are not good at engaging in human-like conversations, their \\nmath skills are limited, and they might not be aligned with human ethics (e.g., they might be \\nbiased or generate toxic responses). In the next section, we learn how a lot of these issues \\nare being addressed.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 35, 'page_label': '36'}, page_content='Foundational Large Language Models & Text Generation\\n36\\nSeptember 2024\\nModel\\nAttention\\n(2017)\\nGPT \\n(2018)\\nGPT-2\\n(2019)\\nGPT-3\\n(2020)\\nLaMDA  \\n(2021)\\nGopher\\n(2021)\\nChinchilla\\n(2022)\\nOptimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W\\n# Parameters 213M 117M 1.5B 175B 137B 280B 70B\\nVocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K\\nEmbedding \\ndimension 1024 768 1600 12288 8192 16384 8192\\nKey dimension 64 64 64 128 128 128 128\\n# heads (H) 16 12 25 96 128 128 64\\n# encoder \\nlayers 6 N/A N/A N/A N/A N/A N/A\\n# decoder \\nlayers 6 12 48 96 64 80 80\\nFeed forward \\ndimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192\\nContext Token \\nSize N/A 512 1024 2048 N/A 2048 2048\\nPre-Training \\ntokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T\\nTable 1. Important hyperparameters for transformers-based large language models\\nA. This number is an estimate based on the reported size of the dataset.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 36, 'page_label': '37'}, page_content='Foundational Large Language Models & Text Generation\\n37\\nSeptember 2024\\nFine-tuning large language models\\nLarge language models typically undergo multiple training stages. The first stage, often \\nreferred to as pre-training, is the foundational stage where an LLM is trained on large, \\ndiverse, and unlabelled text datasets where it’s tasked to predict the next token given the \\nprevious context. The goal of this stage is to leverage a large, general distribution of data \\nand to create a model that is good at sampling from this general distribution. After language \\nmodel pretraining, the resulting LLM usually demonstrates a reasonable level of language \\nunderstanding and language generation skills across a variety of different tasks which \\nare typically tested through zero-shot or few-shot prompting (augmenting the instruction \\nwith a few examples / demonstrations). Pretraining is the most expensive in terms of time \\n(from weeks to months depending on the size of the model) and the amount of required \\ncomputational resources, (GPU/TPU hours).\\nAfter training, the model can be further specialized via fine-tuning, typically called \\ninstruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \\nset of task-specific demonstration datasets where its performance is also measured across \\na set of domain-specific tasks. The following are some examples of behaviors that can be \\nimproved using fine-tuning:\\n• Instruction-tuning/instruction following: The LLM is provided as input an instruction to \\nfollow which might include summarizing a piece of text, writing a piece of code, or writing \\na poem in a certain style.17\\n• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \\non conversational data in the form of questions and responses. This is often called \\nmulti-turn dialogue.39'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 37, 'page_label': '38'}, page_content='Foundational Large Language Models & Text Generation\\n38\\nSeptember 2024\\n• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \\ntoxic outputs. It involves a multi-pronged approach encompassing careful data selection, \\nhuman-in-the-loop validation, and incorporating safety guardrails. Techniques like \\nreinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \\nand ethical responses.\\nFine-tuning is considerably less costly and more data efficient compared to pre-training. \\nNumerous techniques exist to optimize the costs further which are discussed later in \\nthis whitepaper.\\nSupervised fine-tuning \\nAs mentioned in the previous section, SFT is the process of improving an LLM’s performance \\non a specific task or set of tasks by further training it on domain-specific, labeled data. The \\ndataset is typically significantly smaller than the pre-training datasets, and is usually human-\\ncurated and of high quality. \\nIn this setting, each data point consists of an input (prompt) and a demonstration (target \\nresponse). For example, questions (prompt) and answers (target response), translations from \\none language (prompt) to another language (target response), a document to summarize \\n(prompt), and the corresponding summary (target response). \\nIt’s important to note that, while fine-tuning can be used to improve the performance on \\nparticular tasks as mentioned above, it can also serve the purpose of helping the LLM \\nimprove its behavior to be safer, less toxic, more conversational, and better at following \\ninstructions.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 38, 'page_label': '39'}, page_content='Foundational Large Language Models & Text Generation\\n39\\nSeptember 2024\\nReinforcement learning from human feedback\\nTypically, after performing SFT, a second stage of fine-tuning occurs which is called \\nreinforcement learning from human feedback (RLHF). This is a very powerful fine-tuning \\ntechnique that enables an LLM to better align with human-preferred responses (i.e. making \\nits responses more helpful, truthful, safer, etc.). \\nFigure 7. An example RLHF procedure \\nIn contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \\ndemonstration data), RLHF makes it possible to also leverage negative outputs thus \\npenalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \\nnegative output makes it less likely to generate unhelpful or unsafe responses. \\nTo leverage RLHF, a reward model (RM) typically needs to be trained with a procedure similar \\nto that in Figure 7. An RM is usually initialized with a pretrained transformer model, often also \\none that is SFT. Then it is tuned on human preference data which is either single sided (with a \\nprompt, response and a score) or composed of a prompt and a pair of responses along with'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 39, 'page_label': '40'}, page_content='Foundational Large Language Models & Text Generation\\n40\\nSeptember 2024\\na preference label indicating which of the two responses was preferred. For example, given \\ntwo summaries, A and B, of the same article, a human rater selects a preferred summary \\n(relying on the detailed guidance). We refer to the provided preference labels as human \\nfeedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \\nrank order when more than 2 candidates are evaluated, or a more detailed assessment of the \\nsummary quality. The preference signal can also incorporate many dimensions that capture \\nvarious aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \\ntruthfulness. \\nFigure 7 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \\npreference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \\n(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \\ngenerate responses that are better aligned with human preferences. \\nTo better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \\nfeedback to generate preference labels. It’s also possible to remove the need for training \\nRLHF by leveraging approaches such as direct preference optimization (DPO).45 Both RLHF \\nand RLAIF can be used on Google Cloud.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 40, 'page_label': '41'}, page_content='Foundational Large Language Models & Text Generation\\n41\\nSeptember 2024\\nParameter Efficient Fine-Tuning\\nBoth SFT and RLHF are still very costly in terms of compute time and accelerators required, \\nespecially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \\nthere are some really useful and effective techniques that can make fine-tuning significantly \\ncheaper and faster compared to pre-training and full fine-tuning. One such family of \\nmethods is parameter efficient fine-tuning (PEFT) techniques. \\nAt a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \\norder of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \\nThe perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \\nThis has the benefit of training a significantly smaller set of weights, compared to traditional \\nfine-tuning of the entire model. \\nSome common PEFT techniques include the adapter, low-rank adaptation, and \\nsoft prompting:\\n• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\\ntrained model. Only the adapter parameters are trained, resulting in significantly fewer \\nparameters than traditional SFT. \\n• Low-Rank Adaptation (LoRA)47 tackles efficiency differently. It uses two smaller matrices \\nto approximate the original weight matrix update instead of fine-tuning the whole LLM. \\nThis technique freezes the original weights and trains these update matrices, significantly \\nreducing resource requirements with minimum additional inference latency. Additionally, \\nLoRA has improved variants such as QLoRA,48 which uses quantized weights for even \\ngreater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \\nmeaning you can train a LoRA module that specializes in one task and easily replace it with \\nanother LoRA module trained on a different task. It also makes it easier to transfer the \\nmodel since assuming the receiver has the original matrix, only the update matrices need \\nto be provided.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 41, 'page_label': '42'}, page_content='Foundational Large Language Models & Text Generation\\n42\\nSeptember 2024\\n• Soft prompting49 is a technique for conditioning frozen large language models with \\nlearnable vectors instead of hand-crafted text prompts. These vectors, called soft \\nprompts, are optimized on the training data and can be as few as five tokens, making them \\nparameter-efficient and enabling mixed-task inference. \\nFor most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \\nprompting, but the order is reversed when it comes to cost. All three approaches are more \\nmemory efficient than traditional fine-tuning and achieve comparable performance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 42, 'page_label': '43'}, page_content='Foundational Large Language Models & Text Generation\\n43\\nSeptember 2024\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel\\nfrom vertexai.preview.tuning import sft\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = ‘<project_id>’\\nREGION = ‘<region>’\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n# define training & eval dataset.\\nTRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\\npeft_train_sample.jsonl’\\n# set base model and specify a name for the tuned model\\nBASE_MODEL = ‘gemini-1.5-pro-002’\\nTUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\\n# start the fine-tuning job\\nsft_tuning_job = sft.train(\\n   source_model=BASE_MODEL,\\n   train_dataset=TRAINING_DATASET,\\n   # # Optional:\\n   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\\n)\\n# Get the tuning job info.\\nsft_tuning_job.to_dict()\\n# tuned model endpoint name\\ntuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\\n# use the tuned model\\ntuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\\nprint(tuned_genai_model.generate_content(contents=’What is a LLM?’))\\nSnippet 1. SFT fine tuning on Google cloud'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 43, 'page_label': '44'}, page_content='Foundational Large Language Models & Text Generation\\n44\\nSeptember 2024\\nUsing large language models\\nPrompt engineering and sampling techniques have a strong influence on the performance of \\nLLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \\nthat you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \\ndetermine the way in which output tokens are chosen and influence the correctness, \\ncreativity and diversity of the resulting output. We next discuss different variants of prompt \\nengineering and sampling techniques as well as touch on some important parameters that \\ncan have a significant impact on LLM performance.\\nPrompt engineering \\nLLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \\nengineering is a critical component in guiding an LLM to yield desired outputs. This might \\ninclude grounding the model to yield factual responses or unleashing the creativity of the \\nmodel to tell a story or write a song. Examples of prompt engineering include providing \\nclear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \\nimportant information, providing additional background details etc. \\nYou will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \\ncontext of prompt engineering. We define these terms below: \\n• Few-shot prompting: This is when you provide the LLM with a task description, as well \\nas a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \\nresponse. For example, you might provide the model with the name of a few countries \\nand their capital cities, then ask it to generate the capital for a new country that isn’t in \\nthe examples.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 44, 'page_label': '45'}, page_content='Foundational Large Language Models & Text Generation\\n45\\nSeptember 2024\\n• Zero-shot prompting: This is when you provide the LLM directly with a prompt with \\ninstructions. You usually give the LLM a task description and the LLM relies heavily on its \\nexisting knowledge to output the correct response. This requires no additional data or \\nexamples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\\n• Chain-of-thought prompting: This technique aims to improve performance on complex \\nreasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \\nthat demonstrates how to solve similar problems using step-by-step reasoning. The \\nLLM then generates its own chain of thought for the new problem, breaking it down into \\nsmaller steps and explaining its reasoning. Finally, it provides an answer based on its \\nreasoning process.\\nPrompt engineering is an active area of research.\\nSampling Techniques and Parameters\\nA variety of sampling techniques can be employed to determine how the model chooses \\nthe next token in a sequence. They are essential for controlling the quality, creativity, and \\ndiversity of the LLM’s output. The following is a breakdown of different sampling techniques \\nand their important parameters:\\n• Greedy search50: Selects the token with the highest probability at each step. This is the \\nsimplest option but it can lead to repetitive and predictable outputs.\\n• Random sampling:50 Selects the next token according to the probability distribution, where \\neach token is sampled proportionally to its predicted probability. This can produce more \\nsurprising and creative text, but also a higher chance of nonsensical output.\\n• Temperature sampling:50 Adjusts the probability distribution by a temperature parameter. \\nHigher temperatures promote diversity, lower temperatures favor high-probability tokens.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 45, 'page_label': '46'}, page_content='Foundational Large Language Models & Text Generation\\n46\\nSeptember 2024\\n• Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \\ncontrols the degree of randomness.\\n• Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \\ncumulative probability adds up to P. This allows the model to adapt the number of potential \\ncandidates depending on its confidence, favoring more diversity when uncertain and \\nfocusing on a smaller set of highly probable words when confident.\\n• Best-of-N sampling: Generates N separate responses and selects the one deemed best \\naccording to a predetermined metric (e.g., a reward model or a logical consistency check). \\nThis is particularly useful for short snippets or situations where logic and reasoning \\nare key.\\nBy combining prompt engineering with sampling techniques and correctly calibrated \\nhyperparameters, you can greatly influence the LLM’s response, making it more relevant, \\ncreative, and consistent for your specific needs.\\nUntil now, we have seen the various types of LLM architectures, their underlying technology, \\nas well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \\nnow look at some key research about how the decoding process in LLMs can be sped up \\nconsiderably to generate faster responses. \\nAccelerating inference\\nThe scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \\nto hold today. Language models have been consistently increasing in size and this has been \\na direct contributor to the vast improvement in these models’ quality and accuracy over the \\nlast few years. As increasing the number of parameters has improved the quality of LLMs it'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 46, 'page_label': '47'}, page_content='Foundational Large Language Models & Text Generation\\n47\\nSeptember 2024\\nhas also increased the computational resources needed to run them. Numerous approaches \\nhave been used to try and improve the efficiency of LLMs for different tasks as developers \\nare incentivized to reduce cost and latency for model users. Balancing the expense of \\nserving a model in terms of time, money, energy is known as the cost-performance tradeoff \\nand often needs adjusting for particular use cases.\\nTwo of the main resources used by LLMs are memory and computation. Techniques for \\nimproving the efficiency or speed of inference focus primarily on these resources. The \\nspeed of the connection between memory and compute is also critical, but usually hardware \\nconstrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \\nAdditional parameters increase both the size of memory required to hold the model and \\ncomputations needed to produce the model results.\\nWith LLMs being increasingly adopted for large-scale and low-latency use cases, finding \\nways to optimize their inference performance has become a priority and an active research \\ntopic with significant advancements. We will explore a number of methods and a few \\ntradeoffs for accelerating inference.\\nTrade offs\\nMany of the high yielding inference optimisation methods mandate trading off a number of \\nfactors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \\ndifferent inference use cases and requirements. A number of the optimization methods we \\nwill discuss later fall somewhere on the spectrum of these tradeoffs.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 47, 'page_label': '48'}, page_content='Foundational Large Language Models & Text Generation\\n48\\nSeptember 2024\\nTrading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \\nwe’re completely sacrificing that factor, it just means that we’re accepting what might be \\na marginal degradation in quality, latency or cost for the benefit of substantially improving \\nanother factor.\\nThe Quality vs Latency/Cost Tradeoff\\nIt is possible to improve the speed and cost of inference significantly through accepting \\nwhat might be marginal to negligible drops in the model’s accuracy. One  example of this \\nis using a smaller model to perform the task. Another example is quantisation where we \\ndecrease the precision of the model’s parameters thereby leading to faster and less memory \\nintensive calculations.\\nOne important distinction when approaching this trade-off is between the theoretical \\npossibility of a quality loss versus the practical capability of the model to perform the desired \\ntask. This is use case specific and exploring it will often lead to significant speedups without \\nsacrificing quality in a meaningful or noticeable way. For example, if the task we want the \\nmodel to perform is simple, then a smaller model or a quantised one will likely be able to \\nperform this task well. Reduction in parametric capacity or precision does not automatically \\nmean that the model is less capable at that specific task.\\nThe Latency vs Cost Tradeoff\\nAnother name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \\nto the system’s ability at handling multiple requests efficiently. Better throughput on the same \\nhardware means that our LLM inference cost is reduced, and vice versa.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 48, 'page_label': '49'}, page_content='Foundational Large Language Models & Text Generation\\n49\\nSeptember 2024\\nMuch like traditional software systems, there are often multiple opportunities to tradeoff \\nlatency against the cost of LLM inference. This is an important tradeoff since LLM inference \\ntends to be the slowest and most expensive component in the entire stack; balancing latency \\nand cost intentionally is key to making sure we tailor LLM performance to the product or use \\ncase it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \\nwhere cost can be a more important factor than the latency of any particular request. On the \\nother hand, an LLM chatbot product will place much higher importance on request latency.\\nNow that we’ve covered some of the important tradeoffs to consider when optimizing \\ninference, let’s examine some of the most effective inference acceleration techniques. As \\ndiscussed in the tradeoffs section, some optimization techniques can have an impact on the \\nmodel’s output. Therefore we will split the methods into two types: output-approximating \\nand output-preserving.\\nOutput-approximating methods\\nQuantization\\nLLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \\nDuring inference, matrix operations are then applied to these model weights to produce \\nnumerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \\nprecision in which weights and activations are stored, transferred and operated upon. The \\ndefault representation of weights and activations is usually 32 bits floating numbers, with \\nquantization we can drop the precision to 8 or even 4 bit integers.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 49, 'page_label': '50'}, page_content='Foundational Large Language Models & Text Generation\\n50\\nSeptember 2024\\nQuantization has multiple performance benefits, it reduces the memory footprint of \\nthe model, allowing to fit larger models on the same hardware, it also reduces the \\ncommunication overhead of weights and activations within one chip and across chips in \\na distributed inference setup- therefore speeding up inference as communication is a \\nmajor contributor to latency. In addition, decreasing the precision of weights/activations \\ncan enable faster arithmetic operations on these models as some accelerator hardware \\n(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \\nprecision representations.\\nQuantization’s impact on quality can be very mild to non-existent depending on the use \\ncase and model.  Further, in cases where quantisation might introduce a quality regression, \\nthat regression can be small compared to the performance gain, therefore allowing for an \\neffective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \\nspeed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.\\nQuantization can be either applied as an inference-only operation, or it can be incorporated \\ninto the training (referred to as Quantisation Aware Training QAT). QAT is generally \\nconsidered to be a more resilient approach as the model is able to recover some of the \\nquantisation-related quality losses during training. To make sure we get the best cost/quality \\ntradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \\nvs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \\nor group-wise58).\\nDistillation\\nUsing a smaller model to perform a task is one of the most efficient inference optimization \\ntechniques, however, smaller models can demonstrate significant regressions on quality \\ncompared to their larger counterparts.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 50, 'page_label': '51'}, page_content='Foundational Large Language Models & Text Generation\\n51\\nSeptember 2024\\nDistillation is a set of training techniques that targets improving the quality of a smaller model \\n(the student) using a larger model (the teacher). This method can be effective because larger \\nmodels outperform smaller ones even if both are trained on the same data, mainly due to \\nparametric capacity and training dynamics. The gap in performance continues as the training \\ndataset grows as illustrated by Figure 8.\\nIt is worth noticing that even at low volumes of training data, large models can already \\ndemonstrate better performance than the correspondingly trained smaller models, this fact \\nleads us to the first variant of distillation which is referred to as data distillation or model \\ncompression.56 We use a large model which was trained on the data we have to generate \\nmore synthetic data to train the smaller student model, the increase in data volume will help \\nmove the the student further along the quality line compared to only training on the original \\ndata. Synthetic data needs to be approached carefully as it needs to be of high quality and \\ncan lead to negative effects otherwise.\\nFigure 8. An illustration of the performance of models of various sizes as a function of the training \\ndataset’s size'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 51, 'page_label': '52'}, page_content='Foundational Large Language Models & Text Generation\\n52\\nSeptember 2024\\nOther distillation techniques attempt to bring the student model closer to the teacher \\non a more granular level than just synthetic data generation. One prominent technique is \\nknowledge distillation57, in this approach we attempt to align the output token distribution \\nof the student model to that of the teacher’s, this can be much more sample efficient than \\ndata distillation. On-policy distillation59 is another technique that leverages feedback from \\nthe teacher model on each sequence generated by the student in a reinforcement learning \\nsetup. \\nOutput-preserving methods\\nThese methods are guaranteed to be quality neutral, they cause no changes to the model \\noutput which often makes them obvious first steps to optimize inference before facing the \\nmore nuanced tradeoffs of the approximating methods\\nFlash Attention\\nScaled Dot-product Attention, which is the predominant attention mechanism in the \\ntransformer architecture, is a quadratic operation on the input length. Optimizing the self-\\nattention calculation can bring significant latency and cost wins.\\nFlash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \\nthe attention algorithm IO Aware, particularly trying to minimize the amount of data we move \\nbetween the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \\nTPUs and GPUs. When calculating attention, the order of operations is changed and multiple \\nlayers are fused so we can utilize the faster memory tiers as efficiently as possible.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 52, 'page_label': '53'}, page_content='Foundational Large Language Models & Text Generation\\n53\\nSeptember 2024\\nFlash Attention is an exact algorithm, it maintains the numerical output of the attention \\ncomputation and can yield significant latency benefits due to reducing the IO overhead, Tri \\nDao et al.62 showed 2-4X latency improvements in the attention computation.\\nPrefix Caching\\nOne of the most compute intensive, and thus slowest, operations in LLM inference is \\ncalculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \\nLLM, this operation is often referred to as prefill. The final output of prefill is what is termed \\nKV Cache which includes the attention key and value scores for each layer of the transformer \\nfor the entire input. This cache is vital during the decoding phase which produces the output \\ntokens, the KV cache allows us to avoid recalculating attention scores for the input on each \\nautoregressive decode step.\\nPrefix Caching refers to the process of caching the KV Cache itself between subsequent \\ninference requests in order to reduce the latency and cost of the prefill operation. The way \\nthe self-attention mechanism works makes reusing KV caches possible because tokens will \\nonly pay attention to tokens that came before them in the sequence. If there’s new input \\nbeing appended to input that the model has seen before, then we can potentially avoid \\nrecalculating the prefill for the older input.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 53, 'page_label': '54'}, page_content='Foundational Large Language Models & Text Generation\\n54\\nSeptember 2024\\nFigure 9. An illustration of Prefix Caching in a chat scenario\\nFigure 9 illustrates how prefix caching works in a multi-turn scenario with a document upload. \\nOn the first user turn, the prefill operation has to process the entire document therefor taking \\n500ms, the resulting KV cache is then stored so that on the second user turn, we can retrieve \\nthe cache directly from storage and avoid recomputing it for the long doc, therefore saving \\nsubstantial amounts of compute and latency.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 54, 'page_label': '55'}, page_content='Foundational Large Language Models & Text Generation\\n55\\nSeptember 2024\\nPrefix caches can be stored either in memory or on disk and fetched on-demand. One \\nimportant consideration is making sure that the input structure/schema remains prefix-\\ncaching friendly, we should avoid changing the prefix in subsequent requests as that will \\ninvalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \\nthe very beginning of each request will invalidate the cache completely as every subsequent \\nrequest will have a new prefix.\\nMany LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \\nwhere users will have a multi-turn conversation that can span 10s of 1000s of tokens and \\nwe can avoid recalculating the KV cache for the previous parts of the conversation. Large \\ndocument/code uploads is another use case where the artifact the user uploads will remain \\nunchanged from one request to the next. All that’s changing are the questions the user is \\nasking, so caching the KV cache for the document (especially for larger artifacts) can result \\nin significant latency and cost savings.\\nPrefix caching is available as a service called Context Caching on Google AI studio52 and  \\nVertex AI on Google Cloud53.\\nSpeculative Decoding\\nThe first phase of LLM inference, known as prefill, is compute bound due large matrix \\noperations on many tokens occurring in parallel. The second phase, known as decode, is \\ngenerally memory bound as tokens are auto-regressively decoded one at a time.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 55, 'page_label': '56'}, page_content='Foundational Large Language Models & Text Generation\\n56\\nSeptember 2024\\nIt is not easy to naively use additional parallel compute capacity to speed up decode \\ngiven the  need to wait for the current token to be produced before we can calculate what \\nthe next token should be (as per the self-attention mechanism), the decode process is \\ninherently serial.\\nSpeculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \\na way to utilize the spare compute capacity to make each decode step faster. The main idea \\nis to use a much smaller secondary model (often referred to as the drafter) to run ahead of \\nthe main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \\nas the drafter is much faster and smaller than the main model. We then use the main model to \\nverify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \\nfirst two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \\nhypothesis with the maximum number of tokens. For example:\\nFigure 10. An illustration of speculative decoding over 3 tokens\\nNote that the 3 main model steps run in parallel. And because we are not compute bound in \\ndecode, we can use the spare capacity to get much better decode latencies. In the example \\nabove, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \\nspeculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 56, 'page_label': '57'}, page_content='Foundational Large Language Models & Text Generation\\n57\\nSeptember 2024\\ndecoding, there’s only one main model step on the critical path due to parallelization, so we \\nneed 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely \\nquality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \\nin the first place, so the only thing speculative decoding does is run ahead and present \\nhypotheses that the main model can accept or reject in parallel.\\nOne important condition for speculative decoding to work effectively is that the drafter model \\nhas good levels of alignment with the main model, otherwise we won’t be able to accept any \\nof the tokens. So investing in the training quality of the drafter model is worthwhile to get \\nbetter latencies.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nBatching and Parallelization\\nMost of the optimization techniques we’ve discussed so far are specific to Machine Learning \\nand Transformer architecture in particular. However, much like any software system, there \\nare opportunities to improve throughput and latency through a combination of 1) batching \\nless compute-intensive operations (i.e. we can run multiple requests on the same hardware \\nsimultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\\nintensive parts of the computations (i.e. we can divide the computation and split it amongst \\nmore hardware instances to get more compute capacity and therefore better latencies\\nBatching in LLMs is most useful on the decode side - as we explained in the Speculative \\nDecoding section, decode is not compute-bound and therefore there’s an opportunity \\nto batch more requests. We need to be careful that we batch computations in a way that'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 57, 'page_label': '58'}, page_content='Foundational Large Language Models & Text Generation\\n58\\nSeptember 2024\\nenables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \\nand GPUs). We also need to make sure we remain within the memory limits, as decode is a \\nmemory intensive operations, batching more requests will put more pressure on the free \\nmemory available. Batching has become an important component in most high-throughput \\nLLM inference setups.\\nParallelization is also a widely used technique given the variety of opportunities in \\ntransformers for horizontal scaling across more hardware instances. There are multiple \\nparallelism techniques across the model input (Sequence parallelism) the model layers \\n(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \\nconsiderations for parallelism is the cost of communication and synchronization between \\nthe different shards that we distribute to other machines. Communication is a significant \\noverhead and can erode the benefits of adding more computational capacity if we’re not \\ncareful about which parallelization strategy to use. On the other hand, selecting the right \\nstrategy to balance the need for additional compute and the communication cost can yield \\nsignificant latency wins.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nApplications\\nLarge language models are revolutionizing the way we interact with and process information. \\nWith their unprecedented ability to understand context and generate content, they’re \\ntransforming numerous applications in the worlds of text, code, images, audio and video. \\nHere we collected a few examples of application areas, but the reader should keep in mind \\nthat this is not a comprehensive list and that many new ideas are emerging continuously'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 58, 'page_label': '59'}, page_content='Foundational Large Language Models & Text Generation\\n59\\nSeptember 2024\\nabout how to best utilize the capabilities of these new tools. For more information about \\noptimally building and deploying functioning applications based on the following mentioned \\nuse cases, refer to the subsequent whitepapers. \\nIt is also very simple to generate text-based responses for your use case using either \\nthe Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \\ncode examples from these SDKs to generate responses to text prompts using the Gemini \\nmodel. Note that the multimodal aspects of Gemini are covered in their respective \\ndedicated whitepapers.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 59, 'page_label': '60'}, page_content='Foundational Large Language Models & Text Generation\\n60\\nSeptember 2024\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.language_models import TextGenerationModel\\nfrom vertexai.preview.generative_models import GenerationConfig,GenerativeModel\\n# Set values as per your requirements\\nPROJECT_ID = ‘<project_id>’ # set to your project_id\\nvertexai.init(project=PROJECT_ID, location=’us-central1’)\\nPROMPT= ‘What is a LLM?’ # set your prompt here\\nmodel = GenerativeModel(‘gemini-1.5-pro-002’)\\n# call the Gemini API\\nresponse = model.generate_content(\\n   PROMPT)\\nprint(response.text)\\n# google AI Studio SDK\\nimport google.generativeai as genai\\nimport os\\n# update with your API key\\ngenai.configure(api_key=os.environ[“GOOGLE_API_KEY”])\\n# choose the model\\nmodel = genai.GenerativeModel(‘gemini-pro’)\\nresponse = model.generate_content(‘What is a LLM?’) # set your prompt here\\nprint(response.text)\\nSnippet 3. Using Vertex AI and Google AI studio SDKs for unimodal text gene'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 60, 'page_label': '61'}, page_content='Foundational Large Language Models & Text Generation\\n61\\nSeptember 2024\\nCode and mathematics\\nGenerative models can comprehend and generate code and algorithms to supercharge \\ndevelopers by assisting them across many application areas. Some of the popular use cases \\nfor code include:\\n• Code generation: LLMs can be prompted in natural language to generate code in a \\nspecific programming language to perform certain operations. The output can be used as \\na draft.\\n• Code completion: LLMS can proactively suggest useful code as the user types it. This \\ncan save developers time and improve code quality.\\n• Code refactoring and debugging: LLMs can help reduce technical debt by refactoring \\nand debugging code to improve quality, efficiency and correctness.\\n• Code translation: LLMs can significantly help developer time and effort by helping to \\nconvert code from one programming language to another. For example, an LLM might \\nconvert Python code to Java.\\n• Test case generation: LLMs can be prompted to generate unit tests for a provided \\ncodebase which saves considerable time and reduces errors.\\n• Code documentation and understanding: LLMs can be used in a conversational manner \\nto engage in a natural language chat to help you understand a codebase. They can also \\ngenerate appropriate comments, understand copyright status, and create release notes.\\nRecently, a number of exciting advancements have been made in the space of competitive \\ncoding and mathematics. AlphaCode 2,64 combines Gemini’s reasoning capabilities with \\nsearch and the use of tools to solve competitive coding problems. It receives as input a \\ndescription of a problem to solve, and outputs a code solution that solves the problem. It'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 61, 'page_label': '62'}, page_content='Foundational Large Language Models & Text Generation\\n62\\nSeptember 2024\\nnow ranks among the top 15% competitive coders on the popular Codeforces competitive \\ncoding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \\na pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \\nproblem in mathematics, and also discovered more efficient bin-packing algorithms which \\nare used in many applications such as making data centers more efficient. Another recent \\napproach called AlphaGeometry tackles the problem of finding proofs for complex geometric \\ntheorems. It comprises a neuro-symbolic system made up of a neural language model and \\na symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \\ngeometry problems, where the average human gold medalist scores on average 25.9. 67\\nMachine translation\\nLLMs are capable of generating fluid, high-quality and contextually accurate translations. \\nThis is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \\ncontext. The following are some possible real world use cases:\\n• Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \\ntranslations that feel natural. Unlike previous algorithms that might translate word-\\nfor-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \\ncross-language communication.\\n• E-commerce: On global platforms like AliExpress, product descriptions are automatically \\ntranslated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \\ndetails are appropriately translated, leading to fewer misunderstandings.\\n• Travel apps: In apps like Google Translate, travelers get real-time spoken translations. \\nWith LLMs, the translated conversations are smoother, making interactions in foreign \\ncountries more effortless.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 62, 'page_label': '63'}, page_content='Foundational Large Language Models & Text Generation\\n63\\nSeptember 2024\\nText summarization\\nText summarization is a core capability of many of the LLMs mentioned in this whitepaper. \\nThere are a number of natural potential use cases which include:\\n• News aggregators: LLMs could craft summaries that capture not only the main \\nevents but also the sentiment and tone of the article, providing readers with a more \\nholistic understanding.\\n• Research databases: LLMs could help researchers generate abstracts that encapsulate \\nthe core findings and implications of scientific papers.\\n• Chat management: In platforms like Google Chat, LLM-based systems could generate \\nthread summaries that capture the urgency and tone, aiding users in prioritizing \\ntheir responses.\\nQuestion-answering\\nThe older generation of QA systems often worked by keyword matching, frequently missing \\nout on the contextual depth of user queries. LLMs, however, dive deep into context. They can \\ninfer user intent, traverse vast information banks, and provide answers that are contextually \\nrich and precise. Some of the examples where this could be used include:\\n• Virtual assistants: LLMs can offer detailed explanations of a weather forecast \\nconsidering the user’s location, time of year, and recent weather trends.\\n• Customer support: In business platforms, LLM-based bots could provide answers that \\ntake into account the user’s purchase history, past queries, and potential issues, offering \\npersonalized assistance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 63, 'page_label': '64'}, page_content='Foundational Large Language Models & Text Generation\\n64\\nSeptember 2024\\n• Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \\nuser queries by understanding the depth and context of academic questions, offering \\nanswers that suit everyone from a high school student to a postgraduate researcher.\\nThe quality of the generated answers, as well as the corresponding citations and sources \\ncan be significantly improved by using advanced search systems (such as those based on \\nRetrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \\ninformation, as well as post-hoc grounding after the response has been generated. Clear \\ninstructions, roles of what should and should not be used to answer the question, and \\nadvanced prompt engineering approaches (such as chain of thought and search/RAG \\narchitectures), combined with a lower temperature value amongst other things can also \\nhelp greatly.\\nChatbots\\nEarlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \\ntransform this space by offering dynamic, human-like interactions. They can analyze \\nsentiment, context, and even humor, making digital conversations feel more authentic. Some \\nexamples of where this can be used include:\\n• Customer service: A chatbot on retail platforms like Zara could not only answer product-\\nrelated queries but also offer fashion advice based on current trends.\\n• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \\nreacting to live events in the stream and moderating chats with contextual understanding.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 64, 'page_label': '65'}, page_content='Foundational Large Language Models & Text Generation\\n65\\nSeptember 2024\\nContent generation\\nText generation isn’t new, but what LLMs bring to the table is the unprecedented ability \\nto generate human-like text that’s contextually relevant and rich in detail. Earlier models \\nwould often lose context or coherence over longer passages. LLMs, with their vast \\nknowledge and nuanced understanding, can craft text spanning various styles, tones, and \\ncomplexities, mixing factuality with creativity (depending on the context) effectively bridging \\nthe gap between machine-generated and human-written content. The following are some \\nreal-world examples:\\n• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \\nInstead of generic content, the LLMs could generate creative, targeted, and \\naudience-specific messages.\\n• Scriptwriting: LLMs could potentially assist with producing scripts for movies or TV \\nshows. Writers could input themes or plot points, and the model can suggest dialogues or \\nscene descriptions, enhancing the creative process.\\nText generation is a wide task encompassing a variety of use cases that might range from \\nthose where correctness of the generated output is more or less important than its creativity/\\ndiversity of the language. The sampling methods and parameters like temperature should be \\ntuned accordingly. For more information, see the prompt engineering and architecting for \\nLLM applications whitepapers.\\nNatural language inference\\nNatural language inference (NLI) is the task of determining whether a given textual \\nhypothesis can be logically inferred from a textual premise.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 65, 'page_label': '66'}, page_content='Foundational Large Language Models & Text Generation\\n66\\nSeptember 2024\\nTraditional models struggled with nuanced relationships or those that require a deeper \\nunderstanding of context. LLMs, with their intricate grasp of semantics and context, excel \\nat tasks like these, bringing accuracy levels close to human performance. The following are \\nsome real-world examples:\\n• Sentiment analysis: Businesses could utilize LLMs to infer customer sentiment from \\nproduct reviews. Instead of just basic positive or negative tags, they could extract \\nnuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\\n• Legal document review: Law firms could employ LLMs to infer implications \\nand intentions in contracts, ensuring there are no contradictions or potentially \\nproblematic clauses.\\n• Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \\ndoctors in inferring potential diagnoses or health risks, ensuring early intervention.\\nThe whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \\napplications give further insight into these use cases.\\nText classification\\nText classification involves categorizing text into predefined groups. While traditional \\nalgorithms were efficient, they often struggled with ambiguous or overlapping categories. \\nLLMs, given their deep understanding of context, can classify text with higher precision, even \\nwhen faced with subtle distinctions. Some examples of this include:\\n• Spam detection: Email services could utilize LLMs to classify emails as spam or \\nlegitimate. Instead of just keyword-based detection, the models understand the context \\nand intent, potentially reducing false positives.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 66, 'page_label': '67'}, page_content='Foundational Large Language Models & Text Generation\\n67\\nSeptember 2024\\n• News categorization: News platforms could employ LLMs to categorize articles into \\ntopics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \\nbetween categories.\\n• Customer feedback sorting: Businesses could analyze customer feedback through \\nLLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \\nensuring targeted responses.\\n• Evaluating LLMs as autorater: LLMs could be used to rate, compare and rank the \\ngenerated outputs of other LLMs as well.\\nText analysis\\nLLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \\ninsights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \\ndelve deep, offering rich and actionable insights. Some potential real-world examples are:\\n• Market research: Companies could leverage LLMs to analyze consumer conversations on \\nsocial media, extracting trends, preferences, and emerging needs.\\n• Literary analysis: Academics could employ LLMs to understand themes, motifs, and \\ncharacter developments in literary works, offering fresh perspectives on classic and \\ncontemporary literature.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 67, 'page_label': '68'}, page_content='Foundational Large Language Models & Text Generation\\n68\\nSeptember 2024\\nMultimodal applications\\nMultimodal LLMs, capable of processing and generating text, images, audio, and video, have \\nopened up a new frontier in AI, offering a range of exciting and innovative applications across \\nvarious sectors. The following are some examples: \\nCreative content generation:\\n• Storytelling: An AI system could watch an image or video and spin a captivating narrative, \\nintegrating details from the visual with its knowledge base.\\n• Advertising and marketing: Generating targeted and emotionally resonant advertisements \\nbased on product photos or videos.\\nEducation and accessibility:\\n• Personalized learning: Tailoring educational materials to individual learning styles by \\ncombining text with interactive visual and audio elements.\\n• Assistive technology: Multimodal LLMs could power tools that describe images, videos, \\nand audio for visually or hearing-impaired individuals.\\nBusiness and industry:\\n• Document understanding and summarization: Automatically extracting key information \\nfrom complex documents, combining text and visuals like invoices and contracts.\\n• Customer service: Multimodal chatbots can understand and respond to customer queries \\ncombining text and images, offering a richer and more personalized experience. Science \\nand research:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 68, 'page_label': '69'}, page_content='Foundational Large Language Models & Text Generation\\n69\\nSeptember 2024\\n• Medical diagnosis: Analyzing medical scans and reports together, identifying potential \\nissues and providing insights for doctors.\\n• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources like \\nmedical images, protein structures, and research papers to accelerate research.\\nThese examples are just the tip of the iceberg. As research progresses, the applications \\nof multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \\nprofound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \\nUnimodal LLMs ( i.e., text based LLMs).\\nLLMs, thanks to their ability to understand and process language, are reshaping how we \\ninteract with, generate, and analyze text across diverse sectors. As they continue to evolve, \\ntheir applications will only grow, boosting the ability for machines and humans to have rich \\nnatural language interactions.\\nSummary\\nIn this whitepaper we have discussed the basics of transformers, upon which all modern-day \\nLLMs are based. We detailed the evolution of the various LLM model architectures and their \\ncomponents. We’ve also seen the various methodologies you can use to train and fine-tune \\nmodels efficiently and effectively. We briefly discussed prompt engineering and sampling \\ntechniques that greatly influence the output of an LLM, and also touched on possible \\napplications of this technology. There are a number of key takeaways to keep in mind:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 69, 'page_label': '70'}, page_content='Foundational Large Language Models & Text Generation\\n70\\nSeptember 2024\\n• The transformer architecture is the basis for all modern-day LLMs. Across the various \\narchitectures mentioned in this whitepaper we see that it’s important not only to add more \\nparameters to the model, but the composition of the dataset is equally important. \\n• The order and strategies used for fine-tuning is important and may include multiple steps \\nsuch as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \\nin capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \\ndistribution from the pretraining distribution to a more desired one through the power of \\nthe reward function, that can reward desirable behaviors and penalize undesirable ones.\\n• Making inference from neural models efficient is an important problem and an active \\nfield of research. Many methods exist to reduce serving costs and latency with minimal \\nimpact to model performance, and some exact acceleration methods guarantee identical \\nmodel outputs.\\n• Large language models can be used for a variety of tasks including summarization, \\ntranslation, question answering, chat, code generation, and many more. You can \\ncreate your own tasks using the Vertex and Makersuite text generation services which \\nleverage Google’s latest language models. After the model has been trained and tuned, \\nit is important to experiment with engineering prompts. You should use the technique \\nmost appropriate for the task-at-hand because LLMs can be sensitive to prompts k. \\nFurthermore, it is also possible to enhance task specific performance or creativity and \\ndiversity by tweaking the parameters corresponding to sampling techniques such as \\nTop-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \\nand creativity required for the task at hand.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 70, 'page_label': '71'}, page_content='Foundational Large Language Models & Text Generation\\n71\\nSeptember 2024\\nEndnotes\\n1. 1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin,\\xa0I., 2017, Attention is \\nall you need. Advances in Neural Information Processing Systems , 30.\\n2. Wikipedia, 2024, Word n-gram language model. Available at:  \\nhttps://en.wikipedia.org/wiki/Word_n-gram_language_model .\\n3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \\nNeural Information Processing Systems,  27.\\n4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\narXiv preprint arXiv:2111.00396.\\n5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  \\nhttps://jalammar.github.io/illustrated-transformer/ .\\n6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \\narXiv preprint arXiv:1607.06450.\\n7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition.\\n8. HuggingFace., 2024, Byte Pair Encoding. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .\\n9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n10. HuggingFace, 2024, Unigram tokenization. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .\\n11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .\\n12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\\n13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \\nProcessing Systems, 33, 1877-1901.\\n14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 71, 'page_label': '72'}, page_content='Foundational Large Language Models & Text Generation\\n72\\nSeptember 2024\\n15. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\\n16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \\nProcessing Systems.\\n17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \\nin Neural Information Processing Systems,  35, 27730-27744.-27744.\\n18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .\\n19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .\\n20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \\narXiv\\xa0preprint arXiv:2201.08239.\\n21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \\nat: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ .\\n22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \\nmodels: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf.\\n23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \\nmodels with mixture-of-experts. In International Conference on Machine Learning (pp. 2790-2800). PMLR.\\n24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \\nfor neural language models. arXiv preprint arXiv:2001.08361.\\n25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \\npreprint arXiv:2203.15556.\\n26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \\nmodel parallelism. arXiv preprint arXiv:1909.08053.\\n27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \\nLearning Research, 24(240), 1-113.\\n29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \\nsystems. Advances in Neural Information Processing Systems , 32.\\n30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 72, 'page_label': '73'}, page_content='Foundational Large Language Models & Text Generation\\n73\\nSeptember 2024\\n31. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .\\n32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .\\n33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \\nAvailable at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .\\n34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088.\\n36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .\\n37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652.\\n38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1.\\n39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \\narXiv\\xa0preprint arXiv:2310.13650.\\n40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .\\n41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .\\n43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\\n44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \\nmodel. arXiv preprint arXiv:2305.18290.\\n46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \\nMachine Learning (pp. 2790-2799). PMLR.\\n47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \\narXiv\\xa0preprint arXiv:2106.09685.\\n48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 73, 'page_label': '74'}, page_content='Foundational Large Language Models & Text Generation\\n74\\nSeptember 2024\\n49. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \\npreprint arXiv:2104.08691.\\n50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .\\n51. Google AI Studio Context caching. Available \\nat: https://ai.google.dev/gemini-api/docs/caching?lang=python .\\n52. Vertex AI Context caching overview. Available \\nat: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .\\n53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\nAvailable at: https://arxiv.org/abs/2111.00396 .\\n54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \\nactivations. Available at: https://arxiv.org/abs/1609.07061.\\n55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \\ninference. Available at: https://arxiv.org/abs/1712.05877 .\\n56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \\nMining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .\\n57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \\nAvailable at: https://arxiv.org/abs/1503.02531 .\\n58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \\nQuantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .\\n59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\\nPolicy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \\nat: https://arxiv.org/abs/2306.13649.\\n60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \\nnetworks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .\\n61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \\nhttps://arxiv.org/abs/2207.07061.\\n62. Tri Dao et al. “FlashAttention. Available at:  \\nhttps://arxiv.org/abs/2205.14135.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 74, 'page_label': '75'}, page_content='Foundational Large Language Models & Text Generation\\n75\\nSeptember 2024\\n63. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \\nspeculative decoding. Available at: https://arxiv.org/abs/2211.17192 .\\n64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \\ngeneration with AlphaCode. Science, 378(1092-1097). DOI: 10.1126/science.abq1158.\\n65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \\ndiscoveries from program search with large language models. Nature. DOI: 10.1038/s41586-023-06924-6.\\n66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .\\n67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. \\nNature, 625, 476–482. DOI: 10.1038/s41586-023-06747-5.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 0, 'page_label': '1'}, page_content='Operationalizing \\nGenerative AI on \\nVertex AI using \\nMLOps\\nAuthors: Anant Nawalgaria,  \\nGabriela Hernandez Larios, Elia Secchi,  \\nMike Styer, Christos Aniftos  \\nand Onofrio Petragallo'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 1, 'page_label': '2'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nNenshad Bardoliwalla\\nWarren Barkley\\nMikhail Chrestkha\\nChase Lyall\\nLakshmanan Sethu\\nErwan Menard\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 2, 'page_label': '3'}, page_content='Introduction 5\\n What are DevOps and MLOps? 6\\n Lifecycle of a gen AI system 7\\nDiscover 9\\nDevelop and experiment 10\\n The foundational model paradigm 11\\n The core component of LLM Systems: A prompted model component 13\\n Chain & Augment 16\\n Tuning & training 20\\n Data Practices 23\\n Evaluate 27\\nDeploy 30\\n Deployment of gen AI systems 31\\n  Version control 31\\n  Continuous integration of gen AI systems 32\\n  Continuous delivery of gen AI systems 33\\n Deployment of foundation models 34\\n  Infrastructure validation 34\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 3, 'page_label': '4'}, page_content='Compression and optimization 35\\n  Deployment, packaging, and serving checklist 36\\n Logging and monitoring 37\\n Govern 41\\nThe role of an AI platform for gen AI operations 42\\n Key components of Vertex AI for gen AI 43\\n Discover: Vertex Model Garden 44\\n Prototype: Vertex AI Studio & Notebooks 47\\n Customize: Vertex AI training & tuning  48\\n  Train  49\\n  Tune  49\\n  Orchestrate 51\\n Chain & Augment: Vertex AI Grounding, Extensions, and RAG building blocks 52\\n Evaluate: Vertex AI Experiments, Tensorboard, & evaluation pipelines 55\\n  Experiment 56\\n  Evaluation 57\\n Predict: Vertex AI endpoints & monitoring 57\\n Govern: Vertex AI Feature Store, Model Registry, and Dataplex 59\\nConclusion 61\\nEndnotes 63'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 4, 'page_label': '5'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n5\\nSeptember 2024\\nIntroduction\\nThe emergence of foundation models and generative AI (gen AI) has introduced a new era \\nfor building AI systems. Selecting the right model from a diverse range of architectures \\nand sizes, curating data, engineering optimal prompts, tuning models for specific tasks, \\ngrounding model outputs in real-world data, optimizing hardware - these are just a few of the \\nnovel challenges that large models introduce.  \\nThis whitepaper delves into the fundamental tenets of MLOps and the necessary adaptations \\nrequired for the domain of gen AI and Foundation Models. We also examine the diverse range \\nof Vertex AI products, specifically tailored to address the unique demands of foundation \\nmodels and gen AI-based applications. Through this exploration we uncover how Vertex AI, \\nwith its solid foundations of AI infrastructure and MLOps tools, expands its capabilities to \\nprovide a comprehensive MLOps platform for gen AI.\\nEmergence of foundation models \\nand generative AI (gen AI) has \\nintroduced a new era for building \\nAI systems.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 5, 'page_label': '6'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n6\\nSeptember 2024\\nWhat are DevOps and MLOps?\\nDevOps is a software engineering methodology that aims to bridge the gap between \\ndevelopment (Dev) and operations (Ops). It promotes collaboration, automation, and \\ncontinuous improvement to streamline the software development lifecycle, introducing \\npractices such as continuous integration and continuous delivery. \\nMLOps builds upon DevOps principles to address the unique challenges of operationalizing \\nMachine Learning systems rapidly and reliably. In particular, MLOps tackles the experimental \\nnature of ML through practices like:\\n• Data validation: Ensuring the quality and integrity of training data.\\n• Model evaluation: Rigorously assessing model performance with appropriate metrics.\\n• Model monitoring: Tracking model behavior in production to detect and mitigate drift.\\n• Tracking & reproducibility: Maintaining meticulous records for experiment tracking and \\nresult reproduction.\\nFigure 1. Machine learning workflow'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 6, 'page_label': '7'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n7\\nSeptember 2024\\nLifecycle of a gen AI system\\nImagine deploying your first chatbot after months of dedicated work, and it's now interacting \\nwith users and answering questions. Behind this seemingly simple interaction lies the \\ncomplex and fascinating life cycle of a gen AI System, which can be broken down into five \\nkey moments.\\nFirst in the discovery phase, developers and AI engineers must navigate the expanding \\nlandscape of available models to identify the most suitable one for their specific gen AI \\napplication. They must consider each model's strengths, weaknesses, and costs to make an \\ninformed decision.\\nNext, development and experimentation become paramount, with prompt engineering \\nplaying a crucial role in crafting and refining input prompts to elicit desired outputs based on \\nan understanding of the model's intricacies. Few-shot learning, where examples are provided, \\ncan further guide model behavior, while additional customization may involve parameter-\\nefficient fine-tuning (PEFT). Most gen AI systems also involve model chaining, which refers to \\norchestrating calls to multiple models in a specific sequence to create a workflow.\\nData engineering practices have a critical role across all development stages, with factual \\ngrounding (ensuring the model's outputs are based on accurate, up-to-date information) and \\nrecent data from internal and enterprise systems being essential for reliable outputs. Tuning \\ndata is often needed to adapt models to specific tasks, styles, or to rectify persistent errors.\\nDeployment needs to manage many new artifacts in the deployment process, including \\nprompt templates, chain definitions, embedding models, retrieval data stores, and fine-tuned \\nmodel adapters among others. These artifacts each have unique governance requirements, \\nnecessitating careful management throughout development and deployment. Gen AI system \\ndeployment also needs to account for the technical capabilities of the target infrastructure, \\nensuring that system hardware requirements are fulfilled.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 7, 'page_label': '8'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n8\\nSeptember 2024\\nContinuous monitoring in production ensures improved application performance and \\nmaintains safety standards through responsible AI techniques, such as ensuring fairness, \\ntransparency, and accountability in the model's outputs.\\nContinuous Improvement  as a concept is still key for Gen AI-based applications, though \\nwith a twist. For most Gen AI applications, instead of training models from scratch, we’re \\ntaking foundation models (FMs) and then adapting them to our specific use case. This means \\nconstantly tweaking these FMs through prompting techniques, swapping them out for newer \\nversions, or even combining multiple models for enhanced performance, cost efficiency, or \\nreduced latency. Traditional continuous training still holds relevance for scenarios when \\nrecurrent fine-tuning or incorporating human feedback loops are still needed.\\nNaturally, this lifecycle assumes that the foundational model powering the gen AI system is \\nalready operationalized. It's important to recognize that not all organizations will be directly \\ninvolved in this part of the process. In particular, the operationalization of foundational \\nmodels is a specialized set of tasks that is typically only relevant for a select few companies \\nwith the necessary resources and expertise.\\nBecause of that, this whitepaper will focus on practices required to operationalize gen AI \\napplications using and adapting existing foundation models, referring to other whitepapers in \\nthe book should you want to deepdive into how foundational models are operationalized. \\nThis includes active areas of research such as model pre-training, alignment (ensuring the \\nmodel's outputs align with the desired goals and values), evaluation or serving.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 8, 'page_label': '9'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n9\\nSeptember 2024\\nFigure 2. Lifecycle of a Foundational Model & gen AI system and relative operationalization practices \\nDiscover\\nAs mentioned before, building foundational models from scratch is resource-intensive. \\nTraining costs and data requirements are substantial, pushing most practitioners towards \\nadapting existing foundation models through techniques like fine-tuning and prompt \\nengineering. This shift highlights a crucial need: efficiently discovering the optimal foundation \\nmodel for a given use case.\\nThese two characteristics of the gen AI landscape make model discovery an essential \\nMLOps practice:\\n1. An abundance of models: The past year has witnessed an explosion of open-source \\nand proprietary foundation models. Navigating this complex landscape, each with varying \\narchitectures, sizes, training datasets, and licenses, requires a systematic approach to \\nidentify suitable candidates for further evaluation.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 9, 'page_label': '10'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n10\\nSeptember 2024\\n2. No one-size-fits-all solution: Each use case presents unique requirements, demanding a \\nnuanced analysis of available models across multiple dimensions.\\nHere are some factors to consider when exploring models:\\n1. Quality: Early assessments can involve running test prompts or analyzing public \\nbenchmarks and metrics to gauge output quality.\\n2. Latency & throughput: These factors directly impact user experience. A chatbot \\ndemands lower latency than batch-processed summarization tasks.\\n3. Development & maintenance time: Consider the time investment for both initial \\ndevelopment and ongoing maintenance. Managed models often require less effort than \\nself-deployed open-source alternatives.\\n4. Usage cost: Factor in infrastructure and consumption costs associated with using the \\nchosen model.\\n5. Compliance: Assess the model's ability to adhere to relevant regulations and \\nlicensing terms.\\nBecause the activity of discovery has become so important for gen AI systems, many model \\ndiscoverability platforms were created to support this need. An example of that is Vertex \\nModel Garden,1 which is explored later in this whitepaper.\\nDevelop and experiment\\nThe process of development and experimentation remains iterative and orchestrated \\nwhile building gen AI applications. Each experimental iteration involves a tripartite \\ninterplay between data refinement, foundation model(s) selection and adaptation, and\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 10, 'page_label': '11'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n11\\nSeptember 2024\\nrigorous evaluation. Evaluation provides crucial feedback, guiding subsequent iterations \\nin a continuous feedback loop. Subpar performance might call for gathering more data, \\naugmenting data, or further curating the data. Similarly, the adaptation of the foundation \\nmodel itself might need tweaking - optimizing prompts, applying fine-tuning techniques, or \\neven swapping it out for a different one altogether. This iterative refinement cycle, driven by \\nevaluation insights, is just as critical for optimizing gen AI applications as it’s always been for \\ntraditional machine learning.\\nThe foundational model paradigm\\nFoundation models differ from predictive models most importantly because they are multi-\\npurpose models. Instead of being trained for a single purpose, on data specific to that \\ntask, foundation models are trained on broad datasets, and therefore can be applied to \\nmany different use cases. This distinction brings with it several more important differences \\nbetween foundation models and predictive models.\\nFoundation models also exhibit what are known as ‘emergent properties’,2 capabilities that \\nemerge in response to specific input without additional training. Predictive models are \\nonly able to perform the single function they were trained for; a traditional French-English \\ntranslation model, for instance, cannot also solve math problems.\\nFoundation models are also highly sensitive to changes in their input. The output of the \\nmodel and the task it performs are strongly affected, indeed determined, by the input to the \\nmodel. A foundation model can be made to perform translation, generation, or classification \\ntasks simply by changing the input. Even insignificant changes to the input can affect its \\nability to correctly perform that task.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 11, 'page_label': '12'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n12\\nSeptember 2024\\nThese new properties of foundation models have created a corresponding paradigm shift \\nin the practices required to develop and operationalize Gen AI systems. While models in \\nthe predictive AI context are self-sufficient and task-specific, gen AI models are multi-\\npurpose and need an additional element beyond the user input to function as part of a \\ngen AI Application: a prompt, and more specifically, a prompt template, defined as a set of \\ninstructions and examples along with placeholders to accommodate user input. A prompt \\ntemplate, along with dynamic data such as user input, can be combined to create a complete \\nprompt, the text that is passed as input to the foundation model.\\nFigure 3.  How Prompt Template and User input can be combined to create a prompt'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 12, 'page_label': '13'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n13\\nSeptember 2024\\nThe core component of LLM Systems: A prompted \\nmodel component\\nThe presence of the prompt element is a distinguishing feature of gen AI applications. \\nNeither the model nor the prompt is sufficient for the generation of content; gen AI needs the \\ncombination of both. We refer to the combination as a ‘prompted model component’. This \\nis the smallest independent component sufficient to create an LLM application. The prompt \\ndoes not need to be very complicated. It can be a simple instruction, such as “translate \\nthe following sentence from English to French“, followed by the sentence to be translated. \\nWithout that preliminary instruction, though, a foundation model would not perform the \\ndesired translation task. So a prompt, even just a basic instruction, is necessary along with \\nthe input to get the foundation model to do the task required by the application.\\nFigure 4.  Predictive AI unit compared with the gen AI unit\\nThis introduces an important distinction when it comes to MLOps practices for gen AI. In \\nthe development of a gen AI System, experimentation and iteration need to be done in the \\ncontext of a prompted model component, the combination of a model and a prompt. The Gen'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 13, 'page_label': '14'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n14\\nSeptember 2024\\nAI experimentation cycle typically begins with testing variations of the prompt – changing the \\nwording of the instructions, providing additional context, or including relevant examples, etc., \\nand evaluating the impact of those changes. This practice is commonly referred to as prompt \\nengineering. \\nPrompt engineering involves two iterative steps:\\n1. Prompting: Crafting and refining prompts to elicit desired behaviors from a foundational \\nmodel for a specific use case.\\n2. Evaluation: Assessing the model's outputs, ideally programmatically, to gauge its \\nunderstanding and success in fulfilling the prompt's instructions.\\nFigure 5.  The activity of prompt engineering\\nResults of an evaluation can be optionally registered as part of an experiment, to allow for \\nresult tracking. Since the prompt itself is a core element of the prompt engineering process, \\nit becomes a first class citizen within the artifacts part of the experiment. \\nHowever, we need to identify which type of artifacts they are. In the good old days of \\nPredictive AI, we had clear lines - data was one thing, pipelines and code another. But with \\nthe “Prompt” paradigm in gen AI, those lines get blurry. Think about it: prompts can include \\nanything from context, instructions, examples, guardrails to actual internal or external data \\npulled from somewhere else. So, are prompts data? Are they code?\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 14, 'page_label': '15'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n15\\nSeptember 2024\\nTo address these questions, a hybrid approach is needed, recognizing that a prompt has \\ndifferent components and requires different management strategies. Let’s break it down:\\nPrompt as Data: Some parts of the prompt will act just like data. Elements like few-shot \\nexamples, knowledge bases, and user queries are essentially data points. For these \\ncomponents, we need data-centric MLOps practices such as data validation, drift detection, \\nand lifecycle management.\\nPrompt as Code: Other components such as context, prompt templates, guardrails are mode \\ncode-like. They define the structure and rules of the prompt itself. Here, we need code-\\ncentric practices such as approval processes, code versioning, and testing.\\nAs a result, when applying MLOps practices to gen AI, it becomes important to have in place \\nprocesses that give developers easy storage, retrieval, tracking, and modification of prompts. \\nThis allows for fast iteration and principled experimentation. Often one version of a prompt \\nwill work well with a specific version of the model and less well with a different version. In \\ntracking the results of an experiment, both the prompt and its components version, and the \\nmodel version must be recorded and stored along with metrics and output data produced by \\nthe prompted model.\\nThe fact that development and experimentation in gen AI requires working with the prompt \\nand the model together introduces changes in some of the common MLOps practices, \\ncompared to the predictive AI case in which experimentation is done by changing the model \\nalone. Specifically, several of the MLOps practices need to be expanded to consider the \\nprompted model component together as a unit. This includes practices like evaluation, \\nexperiment tracking, model adaptation and deployment, and artifact management, \\nwhich will be discussed below in this whitepaper.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 15, 'page_label': '16'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n16\\nSeptember 2024\\nChain & Augment\\nGen AI models, particularly large language models (LLMs), face inherent challenges in \\nmaintaining recency and avoiding hallucinations. Encoding new information into LLMs \\nrequires expensive and data-intensive pre-training, posing a significant hurdle. Additionally, \\nLLMs might be unable to solve complex challenges, especially when step-by-step reasoning \\nis required. Depending on the use case, leveraging only one prompted model to perform \\na particular generation might not be sufficient. To solve this issue, leveraging a divide and \\nconquer approach, several prompted models can be connected together, along with calls \\nto external APIs and logic expressed as code. A sequence of prompted model components \\nconnected together in this way is commonly known as a chain. \\nFigure 6.  Components of a chain and relative development process'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 16, 'page_label': '17'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n17\\nSeptember 2024\\nTwo common chain-based patterns that have emerged to mitigate recency and \\nhallucinations are retrieval augmented generation (RAG)3 and Agents. \\n• RAG addresses these challenges by augmenting pre-trained models with \\n“knowledge” retrieved from databases, bypassing the need for pre-training. This \\nenables grounding and reduces hallucinations by incorporating up-to-date factual \\ninformation directly into the generation process. \\n• Agents, popularized by the ReAct prompting technique,4 leverage LLMs as mediators \\ninteracting with various tools, including RAG systems, internal or external APIs, \\ncustom extensions, or even with other agents. This enables complex queries and \\nreal-time actions by dynamically selecting and utilizing relevant information sources. \\nThe LLM, acting as an agent, interprets the user’s query, decides which tool to utilize, \\nand how to formulate the response based on the retrieved information.\\nRAG and Agents approaches can be combined to create multi-agent systems connected \\nto large information networks, enabling sophisticated query handling and real-time \\ndecision-making. \\nThe orchestration of different models, logic and APIs is not a novelty of gen AI \\nApplications. For example, recommendation engines have long combined collaborative \\nfiltering models, content-based models, and business rules to generate personalized \\nproduct recommendations for users. Similarly, in fraud detection, machine learning \\nmodels are integrated with rule-based systems and external data sources to identify \\nsuspicious activities.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 17, 'page_label': '18'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n18\\nSeptember 2024\\nWhat makes these chains of gen AI components different, is that, we can't a priori \\ncharacterize or cover the distribution of component inputs, which makes the individual \\ncomponents much harder to evaluate and maintain in isolation.\\nThis results in a paradigm shift in how AI applications are being developed for gen AI.\\nUnlike Predictive AI where it is often possible to iterate on the separate models and \\ncomponents in isolation to then chain in the AI application, in gen AI it’s often easier to \\ndevelop a chain in integration, performing experimentation on the chain end-to-end, iterating \\nover chaining strategies, prompts, the underlying foundational models and other APIs in \\na coordinated manner to achieve a specific goal. No feature engineering, data collection, \\nor further model training cycles is often needed; just changes to the wording of the \\nprompt template.\\nThe shift towards MLOps for gen AI, in contrast to predictive AI, brings forth a new set of \\ndemands. Let's break down these key differences:\\n1. Evaluation: Because of their tight coupling, chains need end-to-end evaluation, not just \\non a per-component basis, to gauge their overall performance and the quality of their \\noutput. In terms of evaluation techniques and metrics, evaluating chains is not dissimilar \\nto evaluating prompted models. Please refer to the below segment on evaluation for more \\ndetails on these approaches.\\n2. Versioning: A chain needs to be managed as a complete artifact in its entirety. The chain \\nconfiguration should be tracked with its own revision history for analysis, reproducibility, \\nand understanding the impact of changes on output. Logging should also include the \\ninputs, outputs, and intermediate states of the chain, and any chain configurations used \\nduring each execution.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 18, 'page_label': '19'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n19\\nSeptember 2024\\n3. Continuous Monitoring: Establishing proactive monitoring systems is vital for detecting \\nperformance degradation, data drift, or unexpected behavior in the chain. This ensures \\nearly identification of potential issues to maintain the quality of the generated output. The \\nactivity of monitoring Chains is discussed in detail in the section ‘Logging and Monitoring’. \\n4. Introspection: The ability to inspect the internal data flows of a chain (inputs and outputs \\nfrom each component) as well as the inputs and outputs of the entire chain is paramount. \\nBy providing visibility into the data flowing through the chain and the resulting content, \\ndevelopers can pinpoint the sources of errors, biases, or undesirable behavior.\\nFigure 7.  Putting together chains, prompted models and model tuning\\nThere are several products in Vertex AI that can support the need for chaining and \\naugmentation, including Grounding as a service,5 Extensions,6 and Vector Search,7 Agent \\nBuilder.8 We discuss the products in the  section “Role of a AI Platform”. Langchain9 is also \\nintegrated with the Vertex SDK,10 and can be used alongside the core Vertex products to \\ndefine and configure gen AI chained applications.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 19, 'page_label': '20'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n20\\nSeptember 2024\\nTuning & training\\nWhen developing a gen AI use case and a specific task that involves LLMs, it can be difficult, \\nespecially for complex tasks, to rely on only prompt engineering and chaining to solve it.      \\nTo improve task performance practitioners often also need to fine-tune the model directly. \\nFine-tuning lets you actively change the layers or a subset of layers of the LLM to optimize \\nthe capability of the model to perform a certain task. Two of the most common ways of \\ntuning a model are:\\n1. Supervised fine-tuning: This is where we train the model in a supervised manner, teaching \\nit to predict the right output sequence for a given input. \\n2. Reinforcement Learning from Human Feedback (RLHF): In this approach, we first train \\na reward model to predict what humans would prefer as a response. Then, we use this \\nreward model to nudge the LLM in the right direction during the tuning process. Like \\nhaving a panel of human judges guiding the model's learning. \\nFigure 8.  Putting together chains, prompted models and model tuning\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 20, 'page_label': '21'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n21\\nSeptember 2024\\nWhen viewed through the MLOps lens, fine-tuning shares similar requirements with \\nmodel training:\\n1. The capability to track artifacts being part of the tuning job. This includes for example the \\ninput data or the parameters being used to tune the model.\\n2. The capability to measure the impact of the tuning. This translates into the capability \\nto perform evaluation of the tuned model for the specific tasks it was trained on and to \\ncompare results with previously tuned models or frozen models for the same task.\\nPlatforms like Vertex AI11 (and the Google Cloud platform more broadly) provide a robust \\nsuite of services designed to address these MLOps requirements: Vertex Model Registry,12 \\nfor instance, provides a centralized storage location for all the artifacts created during the \\ntuning job, and Vertex Pipelines13 streamlines the development and management of these \\ntuning jobs. Dataplex,14 meanwhile, provides an organization-wide data fabric for data lineage \\nand governance and integrates well with both Vertex AI and BigQuery.15 What’s more, these \\nproducts provide the same governance capability for both predictive and gen AI applications, \\nmeaning customers do not need separate products or configurations to manage generative \\nversus AI development.\\nContinuous Training & Tuning\\nIn machine learning operations (MLOps), continuous training is the practice of repeatedly \\nretraining machine learning models in a production environment. This is done to ensure \\nthat the model remains up-to-date and performs well as real-world data patterns change \\nover time. For gen AI models, continuous tuning of the models is often more practical than \\nretraining from scratch due to the high data and computational costs involved.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 21, 'page_label': '22'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n22\\nSeptember 2024\\nThe approach to continuous tuning depends on the specific use case and goals. For relatively \\nstatic tasks like text summarization, the continuous tuning requirements may be lower. But \\nfor dynamic applications like chatbots that need constant human alignment, more frequent \\ntuning using techniques like RLHF based on human feedback is necessary. \\nTo determine the right continuous tuning strategy, AI practitioners must carefully evaluate \\nthe nature of their use case and how the input data evolves over time. Cost is also a major \\nconsideration, as the compute infrastructure greatly impacts the speed and expense of \\ntuning.  We discuss in detail monitoring of GenAI systems in the Logging and Monitoring \\nsection of this whitepaper.\\nGraphics processing units (GPUs) and tensor processing units (TPUs) are key hardware for \\nfine-tuning. GPUs, known for their parallel processing power, are highly effective in handling \\nthe computationally intensive workloads and often associated with training and running \\ncomplex machine learning models. TPUs, on the other hand, are specifically designed \\nby Google for accelerating machine learning tasks. TPUs excel in handling large matrix \\noperations common in deep learning neural networks.\\nTo manage costs, techniques like model quantization can be applied. This represents model \\nweights and activations using lower-precision 8-bit integers rather than 32-bit floats, which \\nreduces computational and memory requirements.\\nWe discuss in detail the support for tuning in Vertex AI in the Customize: Vertex AI Training & \\nTuning section.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 22, 'page_label': '23'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n23\\nSeptember 2024\\nData Practices\\nTraditionally, ML model behavior was dictated solely by its training data. While this still holds \\ntrue for foundation models – trained on massive, multilingual, multimodal datasets – gen AI \\napplications built on top of them introduce a new twist: model behavior is now determined by \\nhow you adapt the model using different types of input data (Figure. 9).\\nFigure 9.  Examples of data spectrum for foundation models – creation (left) vs. adaptation (right)\\nThe key difference between traditional predictive ML and gen AI lies in where you start. In \\npredictive ML, the data is paramount. You spend a lot of time on data engineering, and if you \\ndon’t have the right data, you cannot build an application. Gen AI takes a unique approach to \\nthis matter. You start with a foundation model, some instructions and maybe a few example \\ninputs (in-context learning). You can prototype and launch an application with surprisingly \\nlittle data.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 23, 'page_label': '24'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n24\\nSeptember 2024\\nThis ease of prototyping, however, comes with a challenge. Traditional predictive AI relies on \\napriori well-defined dataset(s). In gen AI, a single application can leverage various data types, \\nfrom completely different data sources, all working together (Figure 10). Let’s explore some \\nof these data types:\\n• Conditioning prompts: These are essentially instructions given to the Foundation Model \\n(FM) to guide its output, setting boundaries of what it can generate.\\n• Few-shot examples: A way to show the model what you want to achieve through input-\\noutput pairs. This helps the model grasp the specific task(s) at hand, and in many cases, it \\nboosts performances.\\n• Grounding/augmentation data: Data coming from either external APIs (like Google \\nSearch) or internal APIs and data sources. This data  permits the FM to produce answers \\nfor a specific context, keeping responses current, relevant without retraining the entire \\nFM. This type of data also supports reducing hallucinations.\\n• Task-specific datasets: These are used to fine-tune an existing FM for a particular task, \\nimproving its performance in that specific area.\\n• Human preference datasets: These capture feedback on generated outputs, helping \\nrefine the model’s ability to produce outputs that align with human preferences. \\n• Full pre training corpora: These are massive datasets used to initially train foundation \\nmodels. While application builders may not have access to them nor the tokenizers, \\nthe information encoded in the model itself will influence the application’s output \\nand performance.\\nThis is not an exhaustive list. The variety of data used in gen AI applications is constantly \\ngrowing and evolving.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 24, 'page_label': '25'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n25\\nSeptember 2024\\nFigure 10.  Example of high-level data and adaptations landscape for developing gen AI applications using \\nexisting foundation models\\nThis diverse range of data adds another complexity layer in terms of data organization, \\ntracking and lifecycle management. Take a RAG-based application as an example: it might \\ninvolve rewriting user queries, dynamically gathering relevant examples using a curated set \\nof examples, querying a vector database, and combining it all with a prompt template. This \\ninvolves managing multiple data types: user queries,  vector databases with curated few-shot \\nexamples and company information, and prompt templates.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 25, 'page_label': '26'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n26\\nSeptember 2024\\nEach data type needs careful organization and maintenance. For example, the vector \\ndatabase requires processing data into embeddings, optimizing chunking strategies, and \\nensuring only relevant information is available. The prompt template itself needs versioning \\nand tracking, the user queries need rewriting, etc. This is where  traditional MLOps and \\nDevOps best practices come into play, with a twist. We need to ensure reproducibility, \\nadaptability, governance, and continuous improvement using all the data required in an \\napplication as a whole but also individually. Think of it this way: in predictive AI, the focus \\nwas on well-defined data pipelines for extraction, transformation, and loading. In gen AI, \\nit's about building pipelines to manage, evolve, adapt and integrate different data types in a \\nversionable, trackable, and reproducible way. \\nAs mentioned earlier, fine-tuning foundation models (FMs) can boost gen AI app \\nperformance, but it needs data. You can get this data by launching your app and gathering \\nreal-world data, generating synthetic data, or a mix of both. Using large models to generate \\nsynthetic data is becoming popular because it speeds things up, but it's still good to have a \\nhuman check the results for quality assurance. Here are few ways to leverage large models \\nfor data engineering purposes:\\n1. Synthetic data generation: This process involves creating artificial data that closely \\nresembles real-world data in terms of its characteristics and statistical properties, often \\nbeing done with a large and capable model. This synthetic data serves as additional \\ntraining data for gen AI, enabling it to learn patterns and relationships even when labeled \\nreal-world data is scarce.\\n2. Synthetic data correction: This technique focuses on identifying and correcting errors \\nand inconsistencies within existing labeled datasets. By leveraging the power of larger \\nmodels, gen AI can flag potential labeling mistakes and propose corrections, improving the \\nquality and reliability of the training data.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 26, 'page_label': '27'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n27\\nSeptember 2024\\n3. Synthetic data augmentation: This approach goes beyond simply generating new \\ndata. It involves intelligently manipulating existing data to create diverse variations while \\npreserving essential features and relationships. Thus, gen AI can encounter a broader \\nrange of scenarios during training, leading to improved generalization and ability to \\ngenerate nuanced and relevant outputs.\\nEvaluating gen AI, unlike predictive AI, is tricky. You don't usually know the training data \\ndistribution of the foundational models. Building a custom evaluation dataset reflecting your \\nuse case is essential. This dataset should cover essential, average, and edge cases. Similar \\nto fine-tuning data, you can leverage powerful language models to generate, curate, and \\naugment data for building robust evaluation datasets.\\nEvaluate\\nEven if only prompt engineering is performed, as any experimental process, it does require \\nevaluation in order to iterate and improve. This makes the evaluation process a core activity \\nof the development of any gen AI systems.\\nIn the context of gen AI systems, evaluation might have different degrees of automation: from \\nentirely driven by humans to entirely automated by a process. \\nIn the early days of a project, when you're still prototyping, evaluation is often a manual \\nprocess. Developers eyeball the model's outputs, getting a qualitative sense of how it's \\nperforming. But as the project matures and the number of test cases balloons, manual \\nevaluation becomes a bottleneck. That's when automation becomes key.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 27, 'page_label': '28'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n28\\nSeptember 2024\\nAutomating evaluation has two big benefits. First, it lets you move faster. Instead of spending \\ntime manually checking each test case, you can let the machines do the heavy lifting. \\nThis means more iterations, more experiments, and ultimately, a better product. Second, \\nautomation makes evaluation more reliable. It takes human subjectivity out of the equation, \\nensuring that results are reproducible.\\nBut automating evaluation for gen AI comes with its own set of challenges. \\nFor one, both the inputs (prompts) and outputs can be incredibly complex. A single prompt \\nmight include multiple instructions and constraints that the model needs to juggle. And the \\noutputs themselves are often high-dimensional - think a generated image or a block of text. \\nCapturing the quality of these outputs in a simple metric is tough.\\nThere are some established metrics, like BLEU for translations and ROUGE for summaries, \\nbut they don't always tell the full story. That's where custom evaluation methods come in. \\nOne approach is to use another foundational model as a judge. For example, you could \\nprompt a large language model to score the quality of generated texts across various \\ndimensions. This is the idea behind techniques like AutoSxS.16\\nAnother challenge is the subjective nature of many evaluation metrics for gen AI. What \\nmakes one output ‘better’ than another can often be a matter of opinion. The key here is to \\nmake sure your automated evaluation aligns with human judgment. You want your metrics \\nto be a reliable proxy for what people would think. And to ensure comparability between \\nexperiments, it's crucial to lock down your evaluation approach and metrics early in the \\ndevelopment process.\\nLack of ground truth data is another common hurdle, especially in the early stages of a \\nproject. One workaround is to generate synthetic data to serve as a temporary ground truth, \\nwhich can be refined over time with human feedback.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 28, 'page_label': '29'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n29\\nSeptember 2024\\nFinally, comprehensive evaluation is essential for safeguarding gen AI applications against \\nadversarial attacks. Malicious actors can craft prompts to try to extract sensitive information \\nor manipulate the model's outputs. Evaluation sets need to specifically address these attack \\nvectors, through techniques like prompt fuzzing (feeding the model random variations on \\nprompts) and testing for information leakage.\\nAutomating the evaluation process ensures speed, scalability and reproducibility\\nAn automation of the evaluation process can be considered a proxy for the \\nhuman judgmen\\nDepending on the use case, the evaluation process will require a high degree \\nof customization.\\nTo ensure comparability it is essential to stabilize the evaluation approach, metrics, \\nand ground truth data as early as possible in the development phase.\\nIt is possible to generate synthetic ground truth data to accommodate for the lack of \\nreal ground truth data.\\nIt is important to include test cases of adversarial prompting as part of the evaluation \\nset to test the reliability of the system itself for these attacks.\\nTable 1.  Key suggestions to approach evaluation of gen AI systems\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 29, 'page_label': '30'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n30\\nSeptember 2024\\nDeploy\\nIt should be clear by this point that production gen AI applications are complex systems with \\nmany interacting components. Some of the common components discussed include multiple \\nprompts, models, adapter layers and external data sources. In deploying a gen AI system to \\nproduction, all these components need to be managed and coordinated with the previous \\nstages of gen AI system development. Given the novelty of these systems, best practices \\nfor deployment and management are still evolving, but we can discuss observations and \\nrecommendations for these components and indicate how to address the major concerns.\\nDeploying gen AI solutions necessarily involves multiple steps. For example, a single \\napplication might utilize several large language models (LLMs) alongside a database, all \\nfed by a dynamic data pipeline. Each of these components potentially requires its own \\ndeployment process.\\nFor clarity, we distinguish between two main types of deployment:\\n1. Deployment of gen AI systems: This focuses on operationalizing a complete system \\ntailored for a specific use case. It encompasses deploying all the necessary elements \\n-  the application, chosen LLMs, database, data pipelines, and any other relevant \\ncomponents - to create a functioning end-user solution.\\n2. Deployment of foundational models: This applies to open-weight models, where the \\nmodel weights are publicly available on platforms like Vertex Model Garden or Hugging \\nFace, or privately trained models. Deployment in this scenario centers around making \\nthe foundational model itself accessible to users. Given their multipurpose nature, these \\ndeployments often aim to support various potential use cases.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 30, 'page_label': '31'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n31\\nSeptember 2024\\nDeployment of gen AI systems\\nDeployment of gen AI systems is broadly similar to deployment of any other complex \\nsoftware system. Most of the system components – databases, Python applications, etc. – \\nare also found in other non-gen AI applications. As a result, our general recommendation is \\nto manage these components using standard software engineering practices such as version \\ncontrol17 and Continuous Integration / Continuous Delivery (CI/CD).18\\nVersion control\\nGen AI experimentation is an iterative process involving repeated cycles of development, \\nevaluation, and modification. To ensure a structured and manageable approach, it's crucial to \\nimplement strict versioning for all modifiable components. These components include:\\n• Prompt templates: Unless leveraging specific prompt management solutions, version \\nthem through standard version control tools like Git.\\n• Chain definitions: The code defining the chain (including API integrations, database calls, \\nfunctions, etc.) should also be versioned using tools like Git. This provides a clear history \\nand enables easy rollback if needed.\\n• External datasets: In retrieval augmented generation (RAG) systems, external datasets \\nplay a key role. It’s important to track these changes and versions of these datasets for \\nreproducibility. You can do that by leveraging existing data analytics solutions such as \\nBigQuery, AlloyDB, Vertex Feature Store. \\n• Adapter models: The landscape of techniques like LoRA tuning for adapter models is \\nconstantly evolving. . You can leverage established data storage solutions (e.g. cloud \\nstorage) to manage and version these assets effectively.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 31, 'page_label': '32'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n32\\nSeptember 2024\\nContinuous integration of gen AI systems\\nIn a continuous integration framework, every code change goes through automatic testing \\nbefore merging to catch issues early. Here, unit and integration testing are key for quality \\nand reliability. Unit tests act like a microscope, zooming in on individual code pieces, while \\nintegration testing verifies that different components work together.\\nThe benefits of continuous integration in traditional software development are well-\\nunderstood. Implementing a CI system helps to do the following:\\n1. Ensure reliable, high-quality outputs: Rigorous testing increases confidence in the \\nsystem's performance and consistency.  \\n2. Catch bugs early: Identifying issues through testing prevents them from causing bigger \\nproblems downstream. It also makes the system more robust and resilient to edge cases \\nand unexpected inputs.\\n3. Lower maintenance costs: Well-documented test cases simplify troubleshooting and \\nenable smoother modifications in the future, reducing overall maintenance efforts\\nThese benefits are applicable to gen AI Systems as much as any software product. \\nContinuous Integration should be applied to all elements of the system, including the prompt \\ntemplates, the chain and chaining logic, and any embedding models and retrieval systems.\\nHowever, applying CI to gen AI comes with challenges:\\n1. Difficult to generate comprehensive test cases: The complex and open-ended nature of \\ngen AI outputs makes it hard to define and create an exhaustive set of test cases that \\ncover all possibilities.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 32, 'page_label': '33'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n33\\nSeptember 2024\\n2. Reproducibility issues: Achieving deterministic, reproducible results is tricky since \\ngenerative models often have intrinsic randomness and variability in their outputs, even for \\nidentical inputs. This makes it harder to consistently test for specific expected behaviors.\\nThese challenges are closely related to the broader question of how to evaluate gen AI \\nsystems. Many of the same techniques discussed in the Evaluation section above can also \\nbe applied to the development of CI systems for gen AI. This is an ongoing area of research, \\nhowever, and more techniques will undoubtedly emerge in the near future.\\nContinuous delivery of gen AI systems\\nOnce the code is merged, a continuous delivery process begins to move the built and tested \\ncode through environments that closely resemble production for further testing before the \\nfinal deployment. \\nAs mentioned in the \"\\'\"Develop and Experiment\"\\'\" segment, chain elements become one \\nof the main components to deploy, as they fundamentally constitute the gen AI application \\nserving users.\\nThe delivery process of the gen AI application containing the chain may vary depending on \\nthe latency requirements and whether the use case is batch or online:\\n1. Batch use cases require deploying a batch process executed on a schedule in production. \\nThe delivery process should focus on testing the entire pipeline in integration in an \\nenvironment close to production before deployment. As part of the testing process, \\ndevelopers can assert specific requirements around the throughput of the batch process \\nitself and checking that all components of the application are functioning correctly (e.g., \\npermissioning, infrastructure, code dependencies).'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 33, 'page_label': '34'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n34\\nSeptember 2024\\n2. Online use cases require deploying an API, in this case, the application containing the \\nchain, capable of responding to users at low latency. The delivery process should involve \\ntesting the API in integration in an environment close to production, with tests to assert \\nthat all components of the application are functioning correctly (e.g., permissioning, \\ninfrastructure, code dependencies). Non-functional requirements (e.g., scalability, \\nreliability, performance) can be verified through a series of tests, including load tests.\\nDeployment of foundation models\\nBecause foundation models are so large and complex, deployment and serving of these \\nmodels raises a number of issues – most obviously, the compute and storage resources \\nneeded to run these massive models successfully. At a minimum, a foundation model \\ndeployment needs to include several key considerations: selecting and securing necessary \\ncompute resources, such as GPUs or TPUS; choosing appropriate data storage services \\nlike BigQuery or Google Cloud Storage that can scale to deal with the large datasets; and \\nimplementing model optimization or compression techniques.\\nInfrastructure validation\\nOne technique that can be applied to address the resource requirements of gen AI systems is \\ninfrastructure validation. This refers to the introduction of an additional verification step, prior \\nto deploying the training and serving systems, to check both the compatibility of the model \\nwith the defined serving configuration and the availability of the required hardware. There \\nare a number of optional infrastructure validation layers that can perform some of these \\nchecks automatically. For instance, TFX19 has an infrastructure validation layer that checks'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 34, 'page_label': '35'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n35\\nSeptember 2024\\nwhether the model will run correctly on a specified hardware configuration, which can help \\ncatch configuration issues before deployment. Nevertheless, the availability of the required \\nhardware still needs to be verified by hand by the engineer or the system administrator.\\nCompression and optimization\\nAnother way of addressing infrastructure challenges is to optimize the model itself. \\nCompressing and/or optimizing the model can often significantly reduce the storage and \\ncompute resources needed for training and serving, and in many cases can also decrease \\nthe serving latency.\\nSome techniques for model compression and optimization include quantization, distillation \\nand model pruning. Quantization reduces the size and computational requirements of the \\nmodel by converting its weights and activations from higher-precision floating-point numbers \\nto lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers. \\nThis can significantly reduce the memory footprint and computational overhead of the model. \\nModel Pruning is a technique for eliminating unnecessary weight parameters or by selecting \\nonly important subnetworks within the model. This reduces model size while maintaining \\naccuracy as high as possible. Finally, distillation trains a smaller model, using the responses \\ngenerated by a larger LLM, to reproduce the output of the larger LLM for a specific domain. \\nThis can significantly reduce the amount of training data, compute, and storage resources \\nneeded for the application.\\nIn certain situations, model distillation can also improve the performance of the model itself \\nin addition to reducing resource requirements. This happens because the smaller model can \\ncombine the knowledge of the larger model with labeled data, which can help it to generalize \\nbetter to new data on a limited use case.The process of distillation usually involves training \\na large foundational LLM (teacher model) and having it generate responses to certain tasks,'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 35, 'page_label': '36'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n36\\nSeptember 2024\\nand then having the smaller LLM (student model) use a combination of the LLMs knowledge \\nas well as task specific supervised dataset to learn. The size and complexity of the smaller \\nLLM can be adjusted to achieve the desired trade-off between performance and resource \\nrequirements. A technique known as step-by-step distillation20 has proven to achieve \\ngreat results.\\nDeployment, packaging, and serving checklist\\nFollowing are the important steps to take when deploying a model on Vertex AI.\\n □ Configure version control: Implement version control practices for LLM deployments. \\nThis allows you to roll back to previous versions if necessary and track changes made to \\nthe model or deployment configuration.\\n □ Optimize the model: Perform any model optimization (distillation, quantization, pruning, \\netc.) before packaging or deploying the model.\\n □ Containerize the model: Package the trained LLM model into a container. \\n □ Define target hardware requirements: Ensure the target deployment environment \\nmeets the requirements for optimal performance of the LLM model, such as GPUs, as well \\nas TPUs and other specialized hardware accelerators.\\n □ Define model endpoint: Define the endpoint configuration using Vertex AI's endpoint \\ncreation interface or the Vertex AI SDK. Specify the model container, input and output \\nformats, and any additional configuration parameters.\\n □ Allocate resources: Allocate the appropriate compute resources for the endpoint based \\non the expected traffic and performance requirements.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 36, 'page_label': '37'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n37\\nSeptember 2024\\n □ Configure access control: Set up access control mechanisms to restrict access to \\nthe endpoint based on authentication and authorization policies. This ensures that only \\nauthorized users or services can interact with the deployed LLM.\\n □ Create model endpoint: Create a Vertex AI endpoint to deploy21 the LLM as a REST API \\nservice. This allows clients to send requests to the endpoint and receive responses from \\nthe LLM..\\n □ Configure monitoring and logging: Establish monitoring and logging systems to track \\nthe endpoint's performance, resource utilization, and error logs.\\n □ Deploy custom integrations: Integrate the LLM into custom applications or services \\nusing the model's SDK or APIs. This provides more flexibility for integrating the LLM into \\nspecific workflows or frameworks.\\n □ Deploy Real-time Applications: For real-time applications, consider using Cloud \\nFunctions and Cloud Run in combination with LLMs hosted in Vertex AI to create a \\nstreaming pipeline that processes data and generates responses in real time.\\nLogging and monitoring\\nMonitoring gen AI applications and, as a result, their components, presents unique \\nchallenges that require additional techniques and approaches on top of those in traditional \\nMLOps. The use of gen AI requires the chaining of components in order to produce results \\nfor practical applications. Additionally, to your application user, all the components are \\nhidden. Therefore, the interface they have to your application is their input and the final \\noutput. This creates the need to log and monitor your application end-to-end: that is, logging \\nand monitoring the input and output of your application overall as well as the input and \\noutput of every single component.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 37, 'page_label': '38'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n38\\nSeptember 2024\\nLogging is necessary for applying monitoring and debugging on your gen AI system in \\nproduction. An input to the application triggers multiple components. Imagine the output \\nto a given input is factually inaccurate. How can you find out which of the components are \\nthe ones that didn’t perform well? To answer this question it is necessary to apply logging \\non the application level and at the component level. We need lineage in our logging for all \\ncomponents executed. For every component we need to log their inputs and outputs. We \\nalso need to be able to map those with any additional artifacts and parameters they depend \\non so we can easily analyze those inputs and outputs.\\nMonitoring can be applied to the overall gen AI application and to individual components. We \\nprioritize monitoring at the application level. This is because if the application is performant \\nand monitoring proves that, it implies that all components are also performant. You can also \\napply the same practices to each of the prompted model components to get more granular \\nresults and understanding of your application.\\nSkew detection in traditional ML systems refers to training-serving skew that occurs when \\nthe feature data distribution in production deviates from the feature data distribution \\nobserved during model training. In the case of Gen AI systems using pretrained models in \\ncomponents chained together to produce the output, we need to modify our approach. We \\ncan measure skew by comparing the distribution of the input data we used to evaluate our \\napplication (the test set as described under the Data Curation and Principles section above) \\nand the distribution of the inputs to our application in production. Once the two distributions \\ndrift apart,further investigation is needed. The same process can be applied to the output \\ndata as well.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 38, 'page_label': '39'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n39\\nSeptember 2024\\nFigure 11.  Drift/skew detection process overview\\nLike skew detection, the drift detection process checks for statistical differences between \\ntwo datasets. However, instead of comparing evaluations and serving inputs, drift looks for \\nchanges in input data. This allows you to check how the inputs and therefore the behavior of \\nyour users changed over time. This is the same as traditional MLOps. \\nGiven that the input to the application is typically text, there are a few approaches to \\nmeasuring skew and drift. In general all the methods are trying to identify significant \\nchanges in production data, both textual (size of input) and conceptual (topics in input), \\nwhen compared to the evaluation dataset. All these methods are looking for changes that \\ncould potentially indicate the application might not be prepared to successfully handle the \\nnature of the new data that are now coming in. Some common approaches are calculating \\nembeddings and distances, counting text length and number of tokens, and tracking \\nvocabulary changes, new concepts and intents, prompts and topics in datasets, as well \\nas statistical approaches such as least-squares density difference,22 maximum mean \\ndiscrepancy (MMD),23 learned kernel MMD,24 or context-aware MMD.25 As gen AI use cases \\nare so diverse, it is often necessary to create additional custom metrics that better capture \\nabnormal changes in your data.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 39, 'page_label': '40'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n40\\nSeptember 2024\\nContinuous evaluation is another common approach to GenAI application monitoring. In \\na continuous evaluation system, you capture the model's production output and run an \\nevaluation task using that output, to keep track of the model's performance over time. One \\napproach is collecting direct user feedback, such as ratings (for example thumbs up/down), \\nwhich provides immediate insight into the perceived quality of outputs. In parallel, comparing \\nmodel-generated responses against established ground truth, often collected through \\nhuman assessment or as a result of an ensemble AI Model approach, allows for deeper \\nanalysis of performance. Ground truth metrics can be used to generate evaluation metrics \\nas described in the Evaluation section. This process provides a view on how your evaluation \\nmetrics changed from when you developed your model to what you have in production today.\\nAs with traditional monitoring in MLOps an alerting process should be deployed for notifying \\napplication owners when a drift, skew or performance decay from evaluation tasks is \\ndetected. This can help you promptly intervene and resolve issues. This is achieved by \\nintegrating alerting and notification tools into your monitoring process.\\nMonitoring expands beyond drift, skew and evaluation tasks. Monitoring in MLOps includes \\nefficiency metrics like resources utilization and latency. Efficiency metrics are as relevant and \\nimportant in gen AI as they are in any other AI application.\\nVertex AI provides a set of tools that can help with monitoring. Model Evaluation for gen AI26 \\ntasks can be used for classification, summarization, question answering, and text generation \\ntasks. Vertex Pipelines can be used to allow the recurrent execution of evaluation jobs in \\nproduction as well as running pipelines for skew and drift detection processes.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 40, 'page_label': '41'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n41\\nSeptember 2024\\nGovern\\nIn the context of MLOps governance encompasses all the practices, and policies that \\nestablish control, accountability, and transparency over the development, deployment, and \\nongoing management of machine learning (ML) models, including all the activities related to \\nthe code, data and models lifecycle.\\nAs mentioned in the Develop & Experiment section the chain element and the relative \\ncomponents become a new type of assets that need to be governed over the full lifecycle \\nfrom development to deployment, to monitoring. \\nThe governance of the chain element lifecycle extends to lineage tracking practices as well.  \\nWhile for predictive AI systems lineage focuses on tracking and understanding the complete \\njourney of a machine learning model, in gen AI, lineage goes beyond the model artifact \\nextending to all the components in the chain. This includes the data and models used and \\ntheir lineage, the code involved and the relative evaluation data and metrics. This can help \\nauditing, debugging and improvements of the models\\nAlong with these new practices, existing MLOps and DevOps practices still apply to MLOps \\nfor gen AI:\\n1. The need to govern the data lifecycle; see “Data Practices”.\\n2. The need to govern the tuned model lifecycle; see “Tuning and Training”.\\n3. The need to govern the code lifecycle; see “Deployment of GenAI \\nSystem components”.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 41, 'page_label': '42'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n42\\nSeptember 2024\\nThe next segment will introduce a set of products that allow developers to perform \\ngovernance of the data, model and code assets. We will discuss products like Google \\nCloud Dataplex, which centralizes the governance of model and data, Vertex ML Metadata \\nand Vertex Experiment, which allows developers to register experiments, their metrics \\nand artifacts.\\nThe role of an AI platform for gen \\nAI operations\\nAlongside the explosion of both predictive and gen AI applications, AI platforms, like Vertex \\nAI,11 have emerged as indispensable tools for organizations seeking to leverage the power of \\nArtificial Intelligence (AI). These comprehensive platforms provide a unified environment that \\nstreamlines the entire AI lifecycle, from data preparation and model training to deployment, \\nautomation, continuous integration/continuous delivery (CI/CD), governance, and monitoring.\\nAt the heart of an AI platform lies its ability to support diverse AI development needs. \\nWhether you seek to utilize pre-trained AI solutions, adapt existing models through tuning \\nor transfer learning, or embark on training your own large models, AI platforms provide the \\ninfrastructure and tools necessary to support these journeys. The advent of these platforms \\nhas revolutionized the way organizations approach AI, enabling them to productionize AI \\napplications in a secure, enterprise-ready, responsible, controlled and scalable manner. \\nThese platforms accelerate innovation as well as foster reproducibility and collaboration \\nwhile reducing costs and maximizing Return on Investment (ROI).'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 42, 'page_label': '43'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n43\\nSeptember 2024\\nThe new gen AI paradigm discussed in prior sections demands a robust and reliable AI \\nplatform that can seamlessly integrate and orchestrate a wide range of functionalities.   \\nThese functionalities include model tuning for specific tasks; leveraging paradigms like \\nretrieval augmented generation3 (RAG) to connect to internal and external data sources; \\nand pre-training or instruction fine-tuning large models from scratch. Complex applications \\nalso often require chaining with other models, such as classifiers to route inputs to the \\nappropriate LLM/ML model, extraction of customer information from a knowledge base, \\ninclusion of safety checks, or even creation of caching systems for cost optimization.  \\nFigure 12.  Key components of Vertex AI for gen AI\\nKey components of Vertex AI for gen AI\\nVertex AI eliminates the complexities of managing the entire infrastructure required for AI \\ndevelopment and deployment. Instead, Vertex AI offers a user-centric approach, providing \\non-demand access to the needed resources. This flexibility empowers organizations to \\nfocus on innovation and collaboration, rather than infrastructure management, and up-\\nfront hardware purchase. The features of Vertex AI that support gen AI development can be \\ngrouped into eight areas.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 43, 'page_label': '44'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n44\\nSeptember 2024\\nDiscover: Vertex Model Garden\\nAs discussed before, there is already a wide variety of available foundation models, trained \\non a broad range of datasets, and the cost of training a new foundation model can be \\nprohibitive. Thus it often makes sense for companies to adapt existing foundation models \\nrather than creating their own from scratch. As a result, a platform facilitating seamless \\ndiscovery and integration of diverse model types is critical.\\nVertex AI Model Garden1 supports these needs, offering a curated collection of over \\n150 Machine Learning and gen AI models from Google, Google partners, and the open-\\nsource community. It simplifies the discovery, customization, and deployment of both \\nGoogle’s proprietary foundational models and diverse open-source models across a \\nvast spectrum of modalities, tasks, and features. This comprehensive repository permits \\ndevelopers to leverage the collective research on artificial intelligence models within a single \\nstreamlined environment.\\nModel Garden encompasses a diverse range of modalities such as Language, Vision, Tabular, \\nDocument, Speech, Video, and Multimodal data. This broad coverage enables developers \\nto tackle a multitude of tasks, including generation, classification, regression, extraction, \\nrecognition, segmentation, tracking, translation, and embedding. Model Garden houses \\nGoogle’s proprietary and foundational models (like Gemini,27 PaLM 2,28 Imagen29) alongside \\nnumerous popular open source and third-party partner models like like Llama 3,30 T5 Flan,31 \\nBERT,32 Stable Diffusion,33 Claude 3 (Anthropic),34 and Mistral AI.35 Additionally, it offers task-\\nspecific models for occupancy analysis, watermark detection, text-moderation, text-to-video, \\nhand-gesture recognition, product identification, and tag recognition, among others. Every \\nmodel36 in Vertex Model Garden has a model card which includes a description of the model, \\nthe main use cases that can cover, and the option (if available) to tune the model or deploy \\nit directly.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 44, 'page_label': '45'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n45\\nSeptember 2024\\nModel Garden fosters experimentation by facilitating access to Google’s proprietary \\nfoundational models through the Vertex AI Studio UI,37 a playground where you can play \\naround with prompts, models, and open-source models using provided Colab notebooks. \\nOne-click deployment is available for some external models, and there are more than 40 \\nmodels available for fine-tuning for specific needs. Furthermore, the platform allows users to \\nleverage technologies like vLLM38 and quantization techniques for optimizing deployments for \\nefficiency and reduced costs. We present below an overview of some of the models in Model \\nGarden. For an up-to-date list, please visit.36'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 45, 'page_label': '46'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n46\\nSeptember 2024\\nModel Type Description Details\\nFirst-party models Foundation models\\nLeverage multimodal models \\nfrom Google across vision, \\ndialog, code generation, and \\ncode completion.\\nGemini39 and Palm240\\nImagen for text-to-image41\\nCodey for code generation \\nand completion42\\nChirp for speech-to-text43\\nFirst-party models Pre-trained APIs\\nBuild and deploy AI \\napplications faster with our \\npre-trained APIs powered by \\nthe best Google AI research \\nand technology.\\nText-to-Speech44\\nNatural Language processing 45\\nTranslation46\\nVision47\\nOpen  models Open source models\\nAccess a wide variety of \\nenterprise-ready open \\nsource models\\nGoogle’s Gemma,48 PaliGemma,16 \\nCodeGemma49 \\nMeta's Llama30\\nTII's Falcon50\\nMistral AI51\\nBERT,32 T-5 FLAN,31 \\nViT,52 EfficientNet53\\nThird-party models Third-party models\\nModel Garden will support \\nthird-party models \\nfrom partners with \\nfoundation models.\\nAnthropic’s Claude 3 Haiku,  \\nSonnet and Opus54,55\\nTable 2.  An overview of some of the models in Model Garden [Last Updated: March 18th, 2024]\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 46, 'page_label': '47'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n47\\nSeptember 2024\\nPrototype: Vertex AI Studio & Notebooks\\nRapid development and prototyping capabilities are also essential for developing gen AI \\napplications. Vertex AI prioritizes inclusivity and flexibility in its development environments, \\ncatering to a wide range of developer preferences and proficiency levels. This platform \\nprovides options for both console-driven and programmatic development workflows. Users \\ncan leverage the intuitive web interface for end-to-end application creation or utilize various \\nAPIs for deeper customization and control. These include the REST API56 and dedicated \\nSDKs for Python,57 NodeJS58 and Java,59 ensuring compatibility with diverse programming \\nlanguages and ecosystems. Developers can choose to use the tools and IDEs of their \\nchoice for interacting with the platform, or take advantage of Vertex-native tools like Vertex \\nColab Enterprise or Vertex Workbench to explore and experiment with code within familiar \\nnotebook environments.\\nVertex AI Studio60 provides a unified console-driven entry point to access and leverage the \\nfull spectrum of Vertex AI's gen AI services. It facilitates exploration and experimentation with \\nvarious Google first party foundation models (for example, PaLM 2, Gemini, Codey, Imagen, \\nand Universal Speech Model). Additionally, it offers prompt examples and functionalities \\nfor testing distinct prompts and models with diverse parameters. It’s also possible to adapt \\nexisting models through various techniques like supervised fine-tuning (SFT), reinforcement \\nlearning tuning techniques, and Distillation, and deploy gen AI applications in just a few \\nclicks. Vertex AI Studio considerably simplifies and democratizes gen AI adoption, catering \\nto a variety of users, from business analysts to machine learning engineers. You can see the \\nhomepage of Vertex AI Studio in Figure 13.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 47, 'page_label': '48'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n48\\nSeptember 2024\\nFigure 13.  Vertex AI Studio - Homepage\\nCustomize: Vertex AI training & tuning \\nWhile prompt engineering and augmentation are sufficient for some gen AI use cases, other \\ncases require training, tuning and adapting the models to get the best results. Vertex AI \\nprovides a comprehensive platform for training and adapting LLMs, supporting a range of \\ntechniques and approaches from prompt engineering to training models from scratch.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 48, 'page_label': '49'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n49\\nSeptember 2024\\nTrain \\nFor full-scale LLM training, TPUs and GPUs are vital because of their superior processing \\npower and memory capacity compared to CPUs. GPUs excel at parallel processing, enabling \\nfaster model training. TPUs, specifically designed for machine learning tasks, offer even \\nfaster processing and higher energy efficiency. This makes them ideal for large-scale, \\ncomplex models. Google Cloud provides a range of offerings to support LLM training, \\nincluding TPU VMs with various configurations, pre-configured AI platforms like Vertex AI, \\nand dedicated resources like Cloud TPU Pods for scaling up training. These offerings allow \\nusers to choose the right infrastructure for their needs, accelerating LLM development and \\nenabling cutting-edge research and applications.\\nTune \\nVertex AI also provides a comprehensive solution for adapting pre-trained LLMs. It supports \\na spectrum of techniques from a non-technical prompt engineering playground at inference \\ntime, to data-driven approaches involving tuning, reinforcement learning and distillation \\nmethods during the development or adaptation phase. The following five techniques – many \\nof which are unique to Vertex AI – enable users to explore and implement them effectively. \\nThis applies to both proprietary and open-source LLMs, allowing you to achieve superior \\nresults while optimizing for costs and latency requirements.\\n• Prompt engineering61 leverages carefully crafted natural language prompts, potentially \\nchained and enriched with external knowledge and examples, to nudge the LLM towards \\ndesired outputs without necessitating further training. Vertex AI through Vertex AI Studio \\noffers a dedicated playground for crafting, testing, comparing and managing diverse \\nprompts and techniques. Users can access various pre-built prompt templates within the \\nplatform and leverage public prompting guidelines62 for Google’s proprietary large models.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 49, 'page_label': '50'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n50\\nSeptember 2024\\n• Supervised fine-tuning (SFT) 63 on Vertex AI facilitates model adaptation by leveraging a \\nset of labeled examples (even a few hundred is enough) to tune a model on specific tasks \\nand contexts within domain-specific datasets. The required examples resemble the one-\\nshot example structure employed in the construction of a prompt. This effectively extends \\nthe few-shot learning approach for enhanced optimization. This focused tuning enables \\nthe model to encode additional parameters in the model necessary for mimicking desired \\nbehaviors such as improved complex prompt comprehension, adaptation to specific \\noutput formats, correcting errors, and learning new tasks. The SFT tuning approach on \\nVertex AI, minimizes computational overhead and time while yielding an updated model \\nthat integrates the newly acquired parameters with the original model’s core parameters.\\n• Reinforcement learning with human feedback (RLHF) ,64 available on Vertex AI for \\nfoundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2, \\nleverages human feedback to train large models to align with human preferences. This \\ntechnique is well-suited in complex tasks involving preference modeling and optimizes \\nLLMs on intricate, sequence-level objectives not easily addressed by traditional \\nsupervised fine-tuning. The process involves first training a reward model using a human \\npreference dataset, then utilizing it to score the output from the LLM, and finally applying \\nreinforcement learning to optimize the LLM. This approach is recognized as a key driver of \\nsuccess in conversational large language models.\\n• Distillation step-by-step20 is an advanced distillation technique transferring knowledge \\nfrom a significantly larger model (known as teacher model) to a smaller task-specific \\nmodel (known as student model), preserving important information while reducing model \\nsize. Step-by-Step Distillation20 surpasses common techniques by requiring significantly \\nless data. This method, accessible on Vertex AI,65 significantly reduces inference costs and \\nlatencies while minimizing performance impact in the resulting smaller LLM.66'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 50, 'page_label': '51'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n51\\nSeptember 2024\\nOrchestrate\\nAny training or tuning job you run can be orchestrated and then operationalized using Vertex \\nPipelines,13 a service that aims to simplify and automate the deployment, management, and \\nscaling of your ML workflows. \\nIt provides a platform for building, orchestrating, scheduling and monitoring complex and \\ncustom ML pipelines, enabling you to efficiently translate your models from prototypes \\nto production.\\nVertex Pipelines is also the platform behind all the managed tuning and evaluation services \\nfor the Google Foundation Models on Vertex AI. This ensures consistency as you can \\nconsume and extend those pipelines easily, without having to familiarize yourself with \\nmany services.\\nGetting started with Vertex Pipelines is simple: you define the pipeline’s step sequence in \\na Python file utilizing Kubeflow SDK.67 For further details and comprehensive onboarding, \\nconsult the official documentation.68'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 51, 'page_label': '52'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n52\\nSeptember 2024\\nChain & Augment: Vertex AI Grounding, Extensions, and RAG \\nbuilding blocks\\nBeyond training, tuning and adapting models and prompts directly, Vertex AI offers a \\ncomprehensive ecosystem for augmenting LLMs, to address the challenges of factual \\ngrounding and hallucination. The platform incorporates emerging techniques like RAG and \\nagent-based approaches.\\nRAG overcomes limitations by enriching prompts with data retrieved from vector databases, \\ncircumventing pre-training requirements and ensuring the integration of up-to-date \\ninformation. Agent-based approaches, popularized by ReAct prompting, leverage LLMs as \\nmediators interacting with tools like RAG systems, APIs, and custom extensions. Vertex AI \\nfacilitates this dynamic information source selection, enabling complex queries, real-time \\nactions, and the creation of multi-agent systems connected to vast information networks for \\nsophisticated query processing and real-time decision-making.\\nVertex AI function calling69 empowers users by enhancing the capabilities of language \\nmodels (LLMs). It enables LLMs to access real-time data and interact with external systems, \\nproviding users with more accurate and up-to-date information. To do that, users need to \\nprovide function definitions such as description, inputs, outputs to the gen AI model. Instead \\nof directly executing functions, the LLM intelligently analyzes user requests and generates \\nstructured data outputs. These outputs propose which function to call and what arguments \\nto use.\\nVertex AI Grounding5 helps users connect large models with verifiable information by \\ngrounding them to internal data corpora on Vertex AI Agent Builder70 or external sources \\nusing Google Search. This enables two key functionalities: verifying model-generated outputs \\nagainst internal or external sources and creating RAG systems using Google’s advanced \\nsearch capabilities that produce quality content grounded in your own or web search data.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 52, 'page_label': '53'}, page_content=\"Operationalizing Generative AI on Vertex AI using ML Ops\\n53\\nSeptember 2024\\nVertex AI extensions 6 let developers integrate Vertex Foundation Models with real-time \\ndata and real-world actions through APIs and functions, enabling task execution and allowing \\nenhanced capabilities. This extends to leveraging 1st party extensions like Vertex AI Search7 \\nand Code Interpreter,71 or 3rd party extensions for triggering and completing transactions. \\nImagine building an application that leverages the LLM's knowledge to plan a trip and \\nseamlessly utilizes internal APIs to book hotels and flights, all within a single interface. \\nAdditionally, Vertex Extensions facilitate function calling with the gemini-pro model, enabling \\nyou to generate descriptions, pass them to the large model, receive JSON with function \\narguments, and automatically call the function.\\nVertex AI Agent Builder70 is an out-of-the-box solution that allows you to quickly build gen \\nAI agents, to be used as conversational chatbots or as part of a search engine. With Vertex \\nAI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range \\nof data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL, \\nunstructured sources like website content crawling and cloud storage as well as connectors \\nto Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search \\ntechnologies, encompassing semantic search, content chunking, ranking, algorithms, \\nand user intent understanding. Under the hood it optimizes document loading, chunking, \\nembedding models, and ranking strategies. It abstracts away these complexities and allows \\nusers to simply specify their data source to initiate the gen AI-powered agent.This approach \\nis ideal for organizations seeking to build robust search experiences for standard use cases \\nwithout extensive technical expertise. \\nVector databases are specialized systems for managing multi-dimensional data. This data, \\nencompassing images, text, audio, video, and other structured or unstructured formats, \\nis represented as vectors capturing its semantic meaning. Vector databases accelerate \\nsearching and retrieval within these high-dimensional spaces, enabling efficient tasks like \\nfinding similar images from billions or extracting relevant text snippets based on various\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 53, 'page_label': '54'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n54\\nSeptember 2024\\ninputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible \\nsolutions for storing and serving embeddings at scale, catering to diverse use cases and \\nuser profiles.\\nVertex AI Vector Search7 is a highly scalable low-latency similarity search and fully \\nmanaged vector database scaling to billions of vector embeddings with auto-scaling. This \\ntechnology, built upon ScaNN72 (a Google-developed technology used in products like \\nSearch, YouTube, and Play), allows you to search from billions of semantically similar or \\nrelated items within your stored data. In the context of gen AI, the most common use cases \\nwhere Vertex Vector Search can be used are:\\n1. Finding similar items (either text or image) based solely on their semantic meaning, in \\nconjunction with an embedding model.\\n2. Creating a hybrid search approach that combines semantic and keyword or metadata \\nsearch to refine the results.\\n3. Extracting relevant information from the database to feed into LLMs, enabling them to \\ngenerate more accurate and informed responses.\\nVertex AI Vector Search primarily functions as a vector database for storing pre-generated \\nembeddings. These embeddings must be created beforehand using separate models like \\nVertex Embedding models73 (namely textembedding-gecko, text-embedding-gecko-\\nmultilingual, or multimodalembedding). Choosing Vertex Vector Search is optimal \\nwhen you require control over aspects like the chunk, retrieval, query and models strategy. \\nThis includes fine-tuning an embedding model for your specific data. However, if your use \\ncase is a standard one requiring little customization, a readily available solution like Vertex \\nSearch might be a better choice.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 54, 'page_label': '55'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n55\\nSeptember 2024\\nVertex AI Feature Store 74 is a centralized and fully managed repository for ML features \\nand embedding. It enables teams to share, serve, and reuse machine learning features and \\nembeddings effortlessly alongside other data. Its native BigQuery23 integration eliminates \\nduplication, simplifies lineage tracking and preserves data governance. Vertex AI Feature \\nStore supports offline retrieval and an easy and fast online serving for machine learning \\nfeatures and embeddings. Vertex AI Feature Store is a good choice when you want to iterate \\nand maintain different embedding versions alongside other machine learning features in a \\nsingle place.\\nVertex AI offers the flexibility to seamlessly create and connect various products to build \\nyour own custom grounding, RAG, and Agent systems. This includes utilizing diverse \\nembedding models (multimodal, multilingual), various vector stores (Vector Search, Feature \\nStore) and search engines like Vertex AI Agent Builder, extensions, grounding, and even SQL \\nquery generation for complex natural language queries. Moreover, Vertex AI provides SDK \\nintegration with LangChain9 to easily build and prototype applications using the umbrella \\nof Vertex AI products. For further details and integration information, consult the official \\ndocumentation75 and official examples.76\\nEvaluate: Vertex AI Experiments, Tensorboard, & \\nevaluation pipelines\\nIn the dynamic world of gen AI, experimentation and evaluation are the cornerstones of \\niterative development and continuous improvement. With a multitude of variables influencing \\nGen AI models (prompt engineering, model selection, data interaction, pretraining, \\nand tuning), evaluation goes hand-in-hand with experimentation. The more seamlessly \\nexperiments and evaluations can be integrated into the development process, the'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 55, 'page_label': '56'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n56\\nSeptember 2024\\nsmoother and more efficient the overall development becomes. Vertex AI provides cohesive \\nexperimentation and evaluation products permitting connected iterations over applications \\nand models alongside their evaluations.\\nExperiment\\nThe process of selecting, creating, and customizing machine learning (including large \\nmodels) and its applications involves significant experimentation, collaboration, and iteration. \\nVertex AI seamlessly integrates experimentation and collaboration into the development \\nlifecycle of AI/ML and gen AI models and applications. Its Workbench Instances77 provide \\nJupyter-based development environments for the entire data science workflow, connected \\nto other Google Cloud services and with GitHub synchronization capabilities. Vertex Colab \\nEnterprise78 accelerates the AI workflow by enabling collaborative coding and leveraging \\ncode completion and generation features.\\nVertex AI also provides two tools for tracking and visualizing the output of many experiment \\ncycles and training runs. Vertex AI Experiments79 facilitates meticulous tracking and \\nanalysis of model architectures, hyperparameters, and training environments. It logs \\nexperiments, artifacts, and metrics, enabling comparison and reproducibility across multiple \\nruns. This comprehensive tracking permits data scientists to select the optimal model \\nand architecture for their specific use case. Vertex AI TensorBoard80 complements the \\nexperimentation process by providing detailed visualizations for tracking, visualizing, and \\nsharing ML experiments. It offers a range of visualizations, including loss and accuracy \\nmetrics tracking, model computational graph visualization, and weight and bias histograms, \\nwhich - for example - can be used for tracking various metrics pertaining to training and \\nevaluation of gen AI models with different prompting and tuning strategies. It also projects \\nembeddings to lower-dimensional space, and displays image, text, and audio samples.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 56, 'page_label': '57'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n57\\nSeptember 2024\\nEvaluation\\nVertex AI also provides a comprehensive set of evaluation tools for gen AI, from ground truth \\nmetrics to using LLMs as raters. \\nFor Ground Truth-based metrics, Automatic Metrics in Vertex AI81 lets you evaluate a model \\nbased on a defined task and “ground truth” dataset. For LLM-based evaluation, Automatic \\nSide by Side (Auto SxS) in Vertex AI82 uses a large model to evaluate the output of multiple \\nmodels or configurations being tested, helping to augment human evaluation at scale. \\nIn addition to that, users can also leverage Rapid Evaluation API, which offers a set of pre-\\nbuilt metrics for evaluating gen AI applications and relative SDK, integrated into the Vertex \\nAI Python SDK for rapid and flexible, notebook-based, prototyping. To get started with Rapid \\nEvaluation Vertex AI SDK see example in the official documentation.83\\nPredict: Vertex AI endpoints & monitoring\\nOnce developed, a production gen AI application must be deployed, including all its model \\ncomponents. If the application uses any models that have been trained or adapted, those \\nmodels need to be deployed to their own serving endpoints. You can serve any model in the \\nModel Garden through Vertex AI Endpoints21,which acts as the gateway for deploying your \\ntrained machine learning models. They allow you to serve online predictions with low latency, \\nmanage access controls, and monitor model performance easily through Model Monitoring. \\nEndpoints also offer scaling options to handle varying traffic demands, ensuring optimal user \\nexperience and reliability.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 57, 'page_label': '58'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n58\\nSeptember 2024\\nAlong with the prediction service, Vertex AI offers the following features for all Google \\nmanaged models:\\n• Citation checkers: Gen AI on Vertex performs Citation checks71. Citations are important \\nfor LLMs and gen AI for several reasons. Citing sources ensures proper acknowledgment \\nof sources and prevents plagiarism and demonstrates transparency and accountability. \\nCiting sources is essential for LLMs and gen AI also because they help identify, \\nunderstand potential biases, and enable reproducibility and verification. For example in \\nGoogle Cloud,84 the gen AI models are designed to produce original content, limiting the \\npossibility of copying existing contents. If this happens, Google Cloud provides quotes for \\nwebsites and code repositories. \\n• Safety scores: Safety attributes are crucial for LLMs and gen AI to mitigate potential \\nrisks like bias, lack of explainability, and misuse. These attributes help detect and mitigate \\nbiased outputs and mitigate misuse, enabling these tools to be used responsibly. As \\nLLMs and gen AI evolve, incorporating safety attributes will be increasingly essential for \\nresponsible and ethical use. For example, Google Cloud added safety scores in Vertex \\nAI PaLM API and Vertex AI Gemini API85: content processed through the API is checked \\nagainst a list of safety attributes, including \"harmful categories\" and sensitive topics. Each \\nattribute has a confidence score between 0.0 and 1.0, indicating the likelihood of the \\ninput belonging to that category. These safety filters can be used in conjunction with all \\nmodels: be it proprietary ones like Palm2 and Gemini or OSS ones like the ones available in \\nModel garden.\\n• Watermarking: With AI-based tools becoming increasingly popular for creation of \\ncontent, it’s very important to identify if an image has been created using AI. Vertex AI \\noffers digital watermarking and verification for AI-generated images86 using the algorithm \\nSynthID87 developed by Google DeepMind.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 58, 'page_label': '59'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n59\\nSeptember 2024\\n• Content moderation and bias detection: By using the Content moderation88 and Bias89 \\ndetection tools on Vertex AI, you can add an extra layer of security on the responses \\nof the LLMs to mitigate the risk that the model training and tuning may sway a model to \\ngenerate outputs that aren’t fair or appropriate for the task.\\nGovern: Vertex AI Feature Store, Model Registry, \\nand Dataplex\\nAddressing the multifaceted requirements of data and model lineage and governance in \\ngen AI requires a comprehensive strategy that tackles both conventional challenges and \\nnovel regulatory or technical complexities associated with large models. By adopting robust \\ngovernance, observability, and lineage practices in the development of gen AI solutions, \\norganizations can ensure comprehensive tracking, iteration, and evolution of data. They \\ncan also track the large models used, prompt adaptations, tuning, and other artifacts. This \\nfacilitates reproducibility of results, transparency and understanding of generated content \\nsources, troubleshooting, compliance enforcement, and enhanced reliability and security. \\nThese practices collectively enable the ethical and responsible development and deployment \\nof gen AI solutions. This fosters internal and external trust and fairness in gen AI models and \\npractices. Vertex AI and Google Cloud offer the following comprehensive suite of tools for \\nunified lineage, governance and monitoring, effectively addressing these critical concerns.\\nIn the context of governance and lineage, Vertex AI Feature Store74 offers:\\n• Track feature and embeddings versions and lineage, ensuring transparency\\n• Monitor feature (prompt) and embedding, response drift, and identify potential \\nissues proactively\\n• Store feature formulas and discover relevant features or embeddings for different \\nuse cases'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 59, 'page_label': '60'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n60\\nSeptember 2024\\n• Utilize feature selection algorithms to optimize model performance\\n• Consolidate and unify all machine learning data within a singular repository encompassing \\nnumerical data, categorical data, textual data, and embeddings representations\\nVertex AI Model Registry12 serves as a centralized repository for comprehensive lifecycle \\nmanagement of both Google proprietary foundational and open-source Machine Learning \\nmodels. This includes gen AI models in addition to predictive models. This unified platform \\nenables registration, storage, and version control of diverse model types, including various \\niterations of tuning for large models. Vertex AI Model Registry seamlessly integrates with \\nVertex Pipelines,13 facilitating orchestration and management of training and tuning jobs \\nwhile leveraging lineage capabilities for recording and documenting the lineage from \\ndatasets to models and associated artifacts. It also couples with Vertex AI Experiments79 \\nand Vertex AI Model Evaluation,90 enabling performance monitoring and comparison of \\ndifferent model versions alongside their artifacts – all within a single interface. Furthermore, \\nVertex AI Model Registry bolsters observability by providing integrated configuration and \\naccess to Vertex AI Model Monitoring91 and logging functionalities. This enables proactive \\nidentification and mitigation of both training-serving skew and prediction drift, ensuring \\nreliability and accuracy of deployed models. Users can directly assign desired model versions \\nto endpoints for one-click deployment from Vertex Model Registry or leverage aliases for \\nsimplified deployment.\\nGoogle Cloud Dataplex14 provides an organization-wide lineage across product boundaries \\nin Google Cloud. Within the domains of AI and gen AI (and more broadly across data analytics \\nand AI/ML) Dataplex seamlessly integrates with BigQuery and Vertex AI. Dataplex facilitates \\nthe unification, management, discovery, and governance of both data and models. Through \\ncomprehensive data lineage, quality, and metadata management capabilities it provides \\nactionable insights for comprehensive data and model understanding. This promotes \\ncompliance, facilitates data analysis, and guarantees the training of machine learning \\nmodels on trusted data sources. This in turn leads to enhanced accuracy and reliability. This'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 60, 'page_label': '61'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n61\\nSeptember 2024\\nintegration permits users across an organization to identify ‘champion models’ and ‘golden \\ndatasets and features’ across projects and regions in a secure way by adhering to identity \\naccess management (IAM)92 boundaries. In short, Dataplex encapsulates a framework within \\nan organization that governs the interaction between people, processes and technology \\nacross all the products in Google Cloud.\\nConclusion\\nThe explosion of gen AI in the last several years introduced fundamental changes in the way \\nAI applications are developed – but far from upending the MLOps discipline, these changes \\nhave only reinforced its basic principles and processes. As we have seen, the principles of \\nMLOps that emphasize reliability, repeatability, and dependability in ML systems development \\nare comfortably extended to include the innovations of gen AI. Some of the necessary \\nchanges are deeper and more far-reaching than others, but nowhere do we find any change \\nthat MLOps cannot accommodate.\\nAs a result, many tools and processes built to support traditional MLOps can also support \\nthe requirements of gen AI. Vertex AI, for instance, is a powerful platform that can be used to \\nbuild and deploy machine learning models and AI applications. It provides a comprehensive \\nsuite of functions for developing both Predictive and gen AI systems, encompassing data \\npreparation, pre-trained APIs, AutoML capabilities, training and serving hardware,  advanced \\nfine-tuning techniques and deployment tools, and a diverse selection of proprietary and \\nopen-source foundation models. It also offers evaluation methods, monitoring capabilities, \\nand governance tools, all unified within a single platform to streamline the AI development \\nlifecycle. It’s built on Google Cloud Platform, which provides a scalable, reliable, secure and \\ncompliant infrastructure for machine learning. It’s a good choice for organizations that want \\nto build and deploy machine learning models and AI applications.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 61, 'page_label': '62'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n62\\nSeptember 2024\\nThe next few years will undoubtedly see gen AI extended in directions that today are \\nunimaginable. Regardless of the direction these developments take, it will continue to \\nbe important to build on solid engineering processes that embody the basic principles \\nof MLOps. These principles support the development of scalable, robust production AI \\napplications today, and no doubt will continue to do so into the future.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 62, 'page_label': '63'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n63\\nSeptember 2024\\nEndnotes\\n1. Model Garden on Vertex AI. Available at: https://cloud.google.com/model-garden\\n2. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, \\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, \\nJeff Dean, William Fedus. 2022. Emergent Abilities of Large Language Models. Available at: https://arxiv.org/\\npdf/2206.07682.pdf\\n3. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich \\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2022. Retrieval-Augmented \\nGeneration for Knowledge-Intensive NLP Tasks. Available at: https://arxiv.org/pdf/2005.11401.pdf\\n4. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Department of \\nComputer Science, Princeton University, Google Research, Brain team, REACT: SYNERGIZING REASONING AND \\nACTING IN LANGUAGE MODELS. Available at: https://arxiv.org/pdf/2210.03629.pdf\\n5. Grounding in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/grounding/\\nground-language-models\\n6. Vertex Extensions. Connect models to APIs by using extensions. Available at:  https://cloud.google.com/\\nvertex-ai/docs/generative-ai/extensions/overview\\n7. Overview of Vertex AI Vector Search. Available at: https://cloud.google.com/vertex-ai/docs/vector-search/\\noverview\\n8. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n9. LangChain. Get your LLM application from prototype to production. Available at: https://www.langchain.\\ncom/\\n10. Introduction to the Vertex AI SDK for Python. Available at: https://cloud.google.com/vertex-ai/docs/python-\\nsdk/use-vertex-ai-python-sdk\\n11. Introduction to Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/start/introduction-unified-\\nplatform\\n12. Introduction to Vertex AI Model Registry. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nregistry/introduction'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 63, 'page_label': '64'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n64\\nSeptember 2024\\n13. Introduction to Vertex AI Pipelines. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/\\nintroduction\\n14. Dataplex. Available at: https://cloud.google.com/dataplex\\n15. BigQuery. Available at: https://cloud.google.com/bigquery?hl=en\\n16. PaLi-Gemma model card. Available at: https://ai.google.dev/gemma/docs/paligemma/model-card\\n17. Version Control. Available at: https://en.wikipedia.org/wiki/Version_control\\n18. Continuous integration. Available at: https://wikipedia.org/wiki/Continuous_integration\\n19. TFX is an end-to-end platform for deploying production ML pipelines. Available at: https://www.tensorflow.\\norg/tfx\\n20. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay \\nKrishna, Chen-Yu Lee, Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with \\nLess Training Data and Smaller Model Sizes. Available at: https://arxiv.org/pdf/2305.02301.pdf\\n21. Vertex Endpoints. Use private endpoints for online prediction. Available at: https://cloud.google.com/vertex-\\nai/docs/predictions/using-private-endpoints\\n22. Tuan Duong Nguyen, Marthinus Christoffel du Plessis, Takafumi Kanamori, Masashi Sugiyama, 2014. \\nConstrained Least-Squares Density-Difference Estimation. Available at: https://www.ms.k.u-tokyo.ac.jp/\\nsugi/2014/CLSDD.pdf\\n23. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, Alexander Smola, 2012. A Kernel \\nTwo-Sample Test. Available at: https://jmlr.csail.mit.edu/papers/v13/gretton12a.html\\n24. Oliver Cobb, Arnaud Van Looveren, 2022. Context-Aware Drift Detection. Available at: https://arxiv.org/\\npdf/2203.08644.pdf\\n25. Google Gemma Model. Available at: https://gemini.google.com/\\n26. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n27. Gemini Team, Google, 2023. Gemini: A Family of Highly Capable Multimodal Models. Available at: https://\\nstorage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n28. Anil, Dai et al., 2023. PaLM 2 Technical Report. Available at: https://arxiv.org/abs/2305.10403'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 64, 'page_label': '65'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n65\\nSeptember 2024\\n29. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed \\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David \\nJ Fleet, Mohammad Norouzi, 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language \\nUnderstanding. Available at: https://arxiv.org/abs/2205.11487\\n30. Build the future of AI with Meta Llama 3. Available at: https://llama.meta.com/llama3\\n31. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, \\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun \\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, \\nGaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff \\nDean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei. 2022. Scaling Instruction-Finetuned \\nLanguage Models. Available at: https://arxiv.org/abs/2210.11416\\n32. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018. BERT: Pre-training of Deep \\nBidirectional Transformers for Language Understanding. Available at: https://arxiv.org/abs/1810.04805\\n33. Stable Diffusion. Available at: https://github.com/CompVis/stable-diffusion\\n34. Vertex AI Function Calling. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/\\nfunction-calling\\n35. Mistral AI. Available at: https://mistral.ai/\\n36. Models available in Model Garden. Available at: https://cloud.google.com/vertex-ai/docs/start/explore-\\nmodels#available-models\\n37. Vertex AI Studio. Customize and deploy generative models. Available at: https://cloud.google.com/\\ngenerative-ai-studio\\n38. vLLM. Easy, fast, and cheap LLM serving for everyone. Available at: https://github.com/vllm-project/vllm\\n39. Overview of multimodal models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmultimodal/overview\\n40. Text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text\\n41. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n42. Code models overview. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-\\nmodels-overview'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 65, 'page_label': '66'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n66\\nSeptember 2024\\n43. Convert speech to text. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/speech/\\nspeech-to-text\\n44. Text-to-Speech AI. Available at: https://cloud.google.com/text-to-speech\\n45. Natural Language AI. Available at: https://cloud.google.com/natural-language\\n46. Translate docs, audio, and videos in real time with Google AI. Available at: https://cloud.google.com/\\ntranslate\\n47. Vision AI. Available at: https://cloud.google.com/vision\\n48. Git. Available at: https://git-scm.com/\\n49. CodeGemma model card. Available at: https://ai.google.dev/gemma/docs/codegemma/model_card\\n50. TII’s Falcon. Available at: https://falconllm.tii.ae/\\n51. Mistral AI. Available at: https://mistral.ai/\\n52. Hugging Face, 2024. Vision Transformer (ViT) Documentation. Hugging Face, [online] Available at:   \\nhttps://huggingface.co/docs/transformers/en/model_doc/vit\\n53. Mingxing Tan, Quoc V. Le, 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. \\nAvailable at: https://arxiv.org/abs/1905.11946\\n54. Anthropic Claude 3. Available at: https://www.anthropic.com/news/claude-3-haiku\\n55. Anthropic Claude 3 on Google Cloud Model Garden. Available at: https://cloud.google.com/blog/products/\\nai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai\\n56. Vertex AI API. Available at: https://cloud.google.com/vertex-ai/docs/reference/rest\\n57. Vertex AI: Python SDK. Available at: https://cloud.google.com/python/docs/reference/aiplatform/latest/\\nvertexai\\n58. Vertex AI: Node.js Client. Available at: https://cloud.google.com/nodejs/docs/reference/aiplatform/latest/\\noverview\\n59. Vertex AI for Java. Available at: https://cloud.google.com/java/docs/reference/google-cloud-aiplatform/\\nlatest/overview\\n60. Customize and deploy generative models. Available at: https://cloud.google.com/generative-ai-studio'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 66, 'page_label': '67'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n67\\nSeptember 2024\\n61. Design text prompts. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-\\nprompts\\n62. Introduction to prompt design. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/\\nintroduction-prompt-design\\n63. Supervised tuning. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-\\nmodels#supervised-tuning\\n64. RLHF model tuning. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-\\nmodels-rlhf\\n65. Vertex AI Distilation. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-\\ntext-models\\n66. Create distilled text models. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/models/\\ndistill-text-models\\n67. Pipeline Basics. Available at: https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/pipeline-\\nbasics/\\n68. Build a pipeline. Available at: https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline\\n69. Vertex AI Search extension. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/\\nextensions/vertex-ai-search\\n70. What is Vertex AI Agent Builder? Available at: https://cloud.google.com/generative-ai-app-builder/docs/\\nintroduction\\n71. Generative AI on Vertex AI, Citation Check. Available at: https://cloud.google.com/vertex-ai/generative-ai/\\ndocs/learn/overview#citation_check\\n72. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar, 2020. \\nAccelerating Large-Scale Inference with Anisotropic Vector Quantization. Available at: https://arxiv.org/\\npdf/1908.10396.pdf\\n73. Get text embeddings. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/\\nget-text-embeddings\\n74. About Vertex AI Feature Store. Available at: https://cloud.google.com/vertex-ai/docs/featurestore/latest/\\noverview'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 67, 'page_label': '68'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n68\\nSeptember 2024\\n75. Google Cloud Vertex AI. Available at: https://python.langchain.com/docs/integrations/llms/google_vertex_\\nai_palm\\n76. Generative AI - Language - LangChain. Available at: https://github.com/GoogleCloudPlatform/generative-\\nai/tree/main/language/orchestration/langchain\\n77. Introduction to Vertex AI Workbench, Workbench Instances. Available at: https://cloud.google.com/vertex-\\nai/docs/workbench/introduction\\n78. Introduction to Colab Enterprise. Available at: https://cloud.google.com/colab/docs/introduction\\n79. Introduction to Vertex AI Experiments. Available at: https://cloud.google.com/vertex-ai/docs/experiments/\\nintro-vertex-ai-experiments\\n80. Vertex AI TensorBoard Introduction to Vertex AI TensorBoard. Available at https://cloud.google.com/vertex-\\nai/docs/experiments/tensorboard-introduction\\n81. Perform metrics-based evaluation. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/\\nmodels/evaluate-models\\n82. Perform automatic side-by-side evaluation. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/models/side-by-side-eval\\n83. Rapid Evaluation Vertex AI. Available at: https://cloud.google.com/vertex-ai/generative-ai/docs/models/\\nrapid-evaluation\\n84. Citation metadata. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#citation_metadata\\n85. Responsible AI. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-\\nai#filters-palm-api\\n86. Imagen on Vertex AI | AI Image Generator. Available at: https://cloud.google.com/vertex-ai/docs/\\ngenerative-ai/image/overview\\n87. SynthID. Identifying AI-generated content with SynthID. Available at: https://deepmind.google/technologies/\\nsynthid/\\n88. Moderate text. Available at: https://cloud.google.com/natural-language/docs/moderating-text\\n89. Model bias metrics for Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/model-\\nbias-metrics'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:46:36-07:00', 'moddate': '2024-11-12T11:46:42-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf', 'total_pages': 69, 'page': 68, 'page_label': '69'}, page_content='Operationalizing Generative AI on Vertex AI using ML Ops\\n69\\nSeptember 2024\\n90. Model evaluation in Vertex AI. Available at: https://cloud.google.com/vertex-ai/docs/evaluation/\\nintroduction\\n91. Introduction to Vertex AI Model Monitoring. Available at: https://cloud.google.com/vertex-ai/docs/model-\\nmonitoring/overview\\n92. Identity and Access Management (IAM). Available at: https://cloud.google.com/iam/docs'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1'}, page_content='Agents\\nAuthors: Julia Wiesinger, Patrick Marlow  \\nand Vladimir Vuskovic'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, page_content='Agents\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nEvan Huang\\nEmily Xue\\nOlcan Sercinoglu\\nSebastian Riedel\\nSatinder Baveja\\nAntonio Gulli\\nAnant Nawalgaria\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3'}, page_content='Introduction 4\\nWhat is an agent? 5\\n The model 6\\n The tools 7\\n The orchestration layer 7\\n Agents vs. models 8\\n Cognitive architectures: How agents operate  8\\nTools: Our keys to the outside world 12\\n Extensions  13\\n  Sample Extensions  15\\n Functions  18\\n  Use cases 21\\n  Function sample code 24\\n Data stores 27\\n  Implementation and application 28\\n Tools recap 32\\nEnhancing model performance with targeted learning 33\\nAgent quick start with LangChain 35\\nProduction applications with Vertex AI agents 38\\nSummary 40\\nEndnotes 42\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4'}, page_content=\"Agents\\n4\\nSeptember 2024\\nIntroduction\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools \\n- like books, Google Search, or a calculator - to supplement their prior knowledge before \\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools \\nto access real-time information or suggest a real-world action. For example, a model can \\nleverage a database retrieval tool to access specific information, like a customer's purchase \\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a \\nuser's query, a model can make various API calls to send an email response to a colleague \\nor complete a financial transaction on your behalf. To do so, the model must not only have \\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\ndirected fashion. This combination of reasoning, logic, and access to external information \\nthat are all connected to a Generative AI model invokes the concept of an agent, or a \\nprogram that extends beyond the standalone capabilities of a Generative AI model. This \\nwhitepaper dives into all these and associated aspects in more detail.\\nThis combination of reasoning, \\nlogic, and access to external \\ninformation that are all connected \\nto a Generative AI model invokes \\nthe concept of an agent.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, page_content='Agents\\n5\\nSeptember 2024\\nWhat is an agent?\\nIn its most fundamental form, a Generative AI agent can be defined as an application that \\nattempts to achieve a goal by observing the world and acting upon it using the tools that it \\nhas at its disposal. Agents are autonomous and can act independently of human intervention, \\nespecially when provided with proper goals or objectives they are meant to achieve. Agents \\ncan also be proactive in their approach to reaching their goals. Even in the absence of \\nexplicit instruction sets from a human, an agent can reason about what it should do next to \\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this \\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of \\nbuilding at the time of publication.\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational \\ncomponents that drive the agent’s behavior, actions, and decision making. The combination \\nof these components can be described as a cognitive architecture, and there are many \\nsuch architectures that can be achieved by the mixing and matching of these components. \\nFocusing on the core functionalities, there are three essential components in an agent’s \\ncognitive architecture as shown in Figure 1.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6'}, page_content='Agents\\n6\\nSeptember 2024\\nFigure 1. General agent architecture and components\\nThe model\\nIn the scope of an agent, a model refers to the language model (LM) that will be utilized as \\nthe centralized decision maker for agent processes. The model used by an agent can be one \\nor multiple LM’s of any size (small / large) that are capable of following instruction based \\nreasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models \\ncan be general purpose, multimodal or fine-tuned based on the needs of your specific agent \\narchitecture. For best production results, you should leverage a model that best fits your \\ndesired end application and, ideally, has been trained on data signatures associated with the \\ntools that you plan to use in the cognitive architecture. It’s important to note that the model is \\ntypically not trained with the specific configuration settings (i.e. tool choices, orchestration/\\nreasoning setup) of the agent. However, it’s possible to further refine the model for the \\nagent’s tasks by providing it with examples that showcase the agent’s capabilities, including \\ninstances of the agent using specific tools or reasoning steps in various contexts.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content='Agents\\n7\\nSeptember 2024\\nThe tools\\nFoundational models, despite their impressive text and image generation, remain constrained \\nby their inability to interact with the outside world. Tools bridge this gap, empowering agents \\nto interact with external data and services while unlocking a wider range of actions beyond \\nthat of the underlying model alone. Tools can take a variety of forms and have varying \\ndepths of complexity, but typically align with common web API methods like GET, POST, \\nPATCH, and DELETE. For example, a tool could update customer information in a database \\nor fetch weather data to influence a travel recommendation that the agent is providing to \\nthe user. With tools, agents can access and process real-world information. This empowers \\nthem to support more specialized systems like retrieval augmented generation (RAG), \\nwhich significantly extends an agent’s capabilities beyond what the foundational model can \\nachieve on its own. We’ll discuss tools in more detail below, but the most important thing \\nto understand is that tools bridge the gap between the agent’s internal capabilities and the \\nexternal world, unlocking a broader range of possibilities.\\nThe orchestration layer\\nThe orchestration layer describes a cyclical process that governs how the agent takes in \\ninformation, performs some internal reasoning, and uses that reasoning to inform its next \\naction or decision. In general, this loop will continue until an agent has reached its goal or a \\nstopping point. The complexity of the orchestration layer can vary greatly depending on the \\nagent and task it’s performing. Some loops can be simple calculations with decision rules, \\nwhile others may contain chained logic, involve additional machine learning algorithms, or \\nimplement other probabilistic reasoning techniques. We’ll discuss more about the detailed \\nimplementation of the agent orchestration layers in the cognitive architecture section.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, page_content='Agents\\n8\\nSeptember 2024\\nAgents vs. models\\nTo gain a clearer understanding of the distinction between agents and models, consider the \\nfollowing chart:\\nModels Agents\\nKnowledge is limited to what is available in their \\ntraining data.\\nKnowledge is extended through the connection \\nwith external systems via tools\\nSingle inference / prediction based on the \\nuser query. Unless explicitly implemented for \\nthe model, there is no management of session \\nhistory or continuous context. (i.e. chat history)\\nManaged session history (i.e. chat history) to \\nallow for multi turn inference / prediction based \\non user queries and decisions made in the \\norchestration layer. In this context, a ‘turn’ is \\ndefined as an interaction between the interacting \\nsystem and the agent. (i.e. 1 incoming event/\\nquery and 1 agent response)\\nNo native tool implementation. Tools are natively implemented in agent \\narchitecture.\\nNo native logic layer implemented. Users can \\nform prompts as simple questions or use \\nreasoning frameworks (CoT, ReAct, etc.) to \\nform complex prompts to guide the model in \\nprediction.\\nNative cognitive architecture that uses reasoning \\nframeworks like CoT, ReAct, or other pre-built \\nagent frameworks like LangChain.\\nCognitive architectures: How agents operate \\nImagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant \\npatrons which involves some cycle of planning, execution, and adjustment.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, page_content='Agents\\n9\\nSeptember 2024\\n• They gather information, like the patron’s order and what ingredients are in the pantry \\nand refrigerator.\\n• They perform some internal reasoning about what dishes and flavor profiles they can \\ncreate based on the information they have just gathered.\\n• They take action to create the dish: chopping vegetables, blending spices, searing meat.\\nAt each stage in the process the chef makes adjustments as needed, refining their plan as \\ningredients are depleted or customer feedback is received, and uses the set of previous \\noutcomes to determine the next plan of action. This cycle of information intake, planning, \\nexecuting, and adjusting describes a unique cognitive architecture that the chef employs to \\nreach their goal.\\nJust like the chef, agents can use cognitive architectures to reach their end goals by \\niteratively processing information, making informed decisions, and refining next actions \\nbased on previous outputs. At the core of agent cognitive architectures lies the orchestration \\nlayer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly \\nevolving field of prompt engineering and associated frameworks to guide reasoning and \\nplanning, enabling the agent to interact more effectively with its environment and complete \\ntasks. Research in the area of prompt engineering frameworks and task planning for \\nlanguage models is rapidly evolving, yielding a variety of promising approaches. While not an \\nexhaustive list, these are a few of the most popular frameworks and reasoning techniques \\navailable at the time of this publication:\\n• ReAct, a prompt engineering framework that provides a thought process strategy for \\nlanguage models to Reason and take action on a user query, with or without in-context \\nexamples. ReAct prompting has shown to outperform several SOTA baselines and improve \\nhuman interoperability and trustworthiness of LLMs.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, page_content='Agents\\n10\\nSeptember 2024\\n• Chain-of-Thought (CoT), a prompt engineering framework that enables reasoning \\ncapabilities through intermediate steps. There are various sub-techniques of CoT including \\nself-consistency, active-prompt, and multimodal CoT that each have strengths and \\nweaknesses depending on the specific application.\\n• Tree-of-thoughts (ToT),, a prompt engineering framework that is well suited for \\nexploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting \\nand allows the model to explore various thought chains that serve as intermediate steps \\nfor general problem solving with language models.\\nAgents can utilize one of the above reasoning techniques, or many other techniques, to \\nchoose the next best action for the given user request. For example, let’s consider an agent \\nthat is programmed to use the ReAct framework to choose the correct actions and tools for \\nthe user query. The sequence of events might go something like this:\\n1. User sends query to the agent\\n2. Agent begins the ReAct sequence\\n3. The agent provides a prompt to the model, asking it to generate one of the next ReAct \\nsteps and its corresponding output:\\na. Question: The input question from the user query, provided with the prompt\\nb. Thought: The model’s thoughts about what it should do next\\nc. Action: The model’s decision on what action to take next\\ni. This is where tool choice can occur\\nii. For example, an action could be one of [Flights, Search, Code, None], where the first \\n3 represent a known tool that the model can choose, and the last represents “no \\ntool choice”'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content='Agents\\n11\\nSeptember 2024\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\ne. Observation: The result of the action / action input sequence\\ni. This thought / action / action input / observation could repeat N-times as needed\\nf. Final answer: The model’s final answer to provide to the original user query\\n4. The ReAct loop concludes and a final answer is provided back to the user\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide \\na grounded, concise response back to the user based on the user’s original query. While \\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge, \\nit instead used a tool (Flights) to search for real-time external information. This additional \\ninformation was provided to the model, allowing it to make a more informed decision based \\non real factual data and to summarize this information back to the user.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='Agents\\n12\\nSeptember 2024\\nIn summary, the quality of agent responses can be tied directly to the model’s ability to \\nreason and act about these various tasks, including the ability to select the right tools, and \\nhow well that tools has been defined. Like a chef crafting a dish with fresh ingredients and \\nattentive to customer feedback, agents rely on sound reasoning and reliable information to \\ndeliver optimal results. In the next section, we’ll dive into the various ways agents connect \\nwith fresh data. \\nTools: Our keys to the outside world\\nWhile language models excel at processing information, they lack the ability to directly \\nperceive and influence the real world. This limits their usefulness in situations requiring \\ninteraction with external systems or data. This means that, in a sense, a language model \\nis only as good as what it has learned from its training data. But regardless of how much \\ndata we throw at a model, they still lack the fundamental ability to interact with the outside \\nworld. So how can we empower our models to have real-time, context-aware interaction with \\nexternal systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this \\ncritical capability to the model.\\nWhile they go by many names, tools are what create a link between our foundational models \\nand the outside world. This link to external systems and data allows our agent to perform a \\nwider variety of tasks and do so with more accuracy and reliability. For instance, tools can \\nenable agents to adjust smart home settings, update calendars, fetch user information from \\na database, or send emails based on a specific set of instructions.\\nAs of the date of this publication, there are three primary tool types that Google models are \\nable to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools, \\nwe unlock a vast potential for them to not only understand the world but also act upon it, \\nopening doors to a myriad of new applications and possibilities.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, page_content='Agents\\n13\\nSeptember 2024\\nExtensions \\nThe easiest way to understand Extensions is to think of them as bridging the gap between \\nan API and an agent in a standardized way, allowing agents to seamlessly execute APIs \\nregardless of their underlying implementation. Let’s say that you’ve built an agent with a goal \\nof helping users book flights. You know that you want to use the Google Flights API to retrieve \\nflight information, but you’re not sure how you’re going to get your agent to make calls to this \\nAPI endpoint.\\nFigure 3. How do Agents interact with External APIs?\\nOne approach could be to implement custom code that would take the incoming user query, \\nparse the query for relevant information, then make the API call. For example, in a flight \\nbooking use case a user might state “I want to book a flight from Austin to Zurich.” In this \\nscenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant \\nentities from the user query before attempting to make the API call. But what happens if the \\nuser says “I want to book a flight to Zurich” and never provides a departure city? The API call \\nwould fail without the required data and more code would need to be implemented in order \\nto catch edge and corner cases like this. This approach is not scalable and could easily break \\nin any scenario that falls outside of the implemented custom code.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, page_content='Agents\\n14\\nSeptember 2024\\nA more resilient approach would be to use an Extension. An Extension bridges the gap \\nbetween an agent and an API by:\\n1. Teaching the agent how to use the API endpoint using examples.\\n2. Teaching the agent what arguments or parameters are needed to successfully call the \\nAPI endpoint.\\nFigure 4. Extensions connect Agents to External APIs\\nExtensions can be crafted independently of the agent, but should be provided as part of the \\nagent’s configuration. The agent uses the model and examples at run time to decide which \\nExtension, if any, would be suitable for solving the user’s query. This highlights a key strength \\nof Extensions, their built-in example types, that allow the agent to dynamically select the \\nmost appropriate Extension for the task. \\n \\nFigure 5. 1-to-many relationship between Agents, Extensions and APIs'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15'}, page_content='Agents\\n15\\nSeptember 2024\\nThink of this the same way that a software developer decides which API endpoints to use \\nwhile solving and solutioning for a user’s problem. If the user wants to book a flight, the \\ndeveloper might use the Google Flights API. If the user wants to know where the nearest \\ncoffee shop is relative to their location, the developer might use the Google Maps API. In \\nthis same way, the agent / model stack uses a set of known Extensions to decide which one \\nwill be the best fit for the user’s query. If you’d like to see Extensions in action, you can try \\nthem out on the Gemini application by going to Settings > Extensions and then enabling any \\nyou would like to test. For example, you could enable the Google Flights extension then ask \\nGemini “Show me flights from Austin to Zurich leaving next Friday.”\\nSample Extensions \\nTo simplify the usage of Extensions, Google provides some out of the box extensions that \\ncan be quickly imported into your project and used with minimal configurations. For example, \\nthe Code Interpreter extension in Snippet 1 allows you to generate and run Python code from \\na natural language description.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, page_content='Agents\\n16\\nSeptember 2024\\nPython\\nimport vertexai\\nimport pprint\\nPROJECT_ID = \"YOUR_PROJECT_ID\"\\nREGION = \"us-central1\"\\nvertexai.init(project=PROJECT_ID, location=REGION)\\nfrom vertexai.preview.extensions import Extension\\nextension_code_interpreter = Extension.from_hub(\"code_interpreter\")\\nCODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\"\\nresponse = extension_code_interpreter.execute(\\n  operation_id = \"generate_and_execute\",\\n  operation_params = {\"query\": CODE_QUERY}\\n  )\\nprint(\"Generated Code:\")\\npprint.pprint({response[\\'generated_code\\']})\\n# The above snippet will generate the following code.\\n```\\nGenerated Code:\\nclass TreeNode:\\n  def __init__(self, val=0, left=None, right=None):\\n    self.val = val\\n    self.left = left\\n    self.right = right\\nContinues next page...'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content='Agents\\n17\\nSeptember 2024\\nPython\\ndef invert_binary_tree(root):\\n    \"\"\"\\n    Inverts a binary tree.\\n    Args:\\n        root: The root of the binary tree.\\n    Returns:\\n        The root of the inverted binary tree.\\n    \"\"\"\\n    if not root:\\n        return None\\n    # Swap the left and right children recursively\\n    root.left, root.right = \\ninvert_binary_tree(root.right), invert_binary_tree(root.left)\\n    return root\\n# Example usage:\\n# Construct a sample binary tree\\nroot = TreeNode(4)\\nroot.left = TreeNode(2)\\nroot.right = TreeNode(7)\\nroot.left.left = TreeNode(1)\\nroot.left.right = TreeNode(3)\\nroot.right.left = TreeNode(6)\\nroot.right.right = TreeNode(9)\\n# Invert the binary tree\\ninverted_root = invert_binary_tree(root)\\n```\\nSnippet 1. Code Interpreter Extension can generate and run Python code'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, page_content='Agents\\n18\\nSeptember 2024\\nTo summarize, Extensions provide a way for agents to perceive, interact, and influence the \\noutside world in a myriad of ways. The selection and invocation of these Extensions is guided \\nby the use of Examples, all of which are defined as part of the Extension configuration.\\nFunctions \\nIn the world of software engineering, functions are defined as self-contained modules \\nof code that accomplish a specific task and can be reused as needed. When a software \\ndeveloper is writing a program, they will often create many functions to do various tasks. \\nThey will also define the logic for when to call function_a versus function_b, as well as the \\nexpected inputs and outputs.\\nFunctions work very similarly in the world of agents, but we can replace the software \\ndeveloper with a model. A model can take a set of known functions and decide when to use \\neach Function and what arguments the Function needs based on its specification. Functions \\ndiffer from Extensions in a few ways, most notably:\\n1. A model outputs a Function and its arguments, but doesn’t make a live API call.\\n2. Functions are executed on the client-side, while Extensions are executed on \\nthe agent-side.\\nUsing our Google Flights example again, a simple setup for functions might look like the \\nexample in Figure 7.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, page_content='Agents\\n19\\nSeptember 2024\\nFigure 7. How do functions interact with external APIs?\\nNote that the main difference here is that neither the Function nor the agent interact directly \\nwith the Google Flights API. So how does the API call actually happen?\\nWith functions, the logic and execution of calling the actual API endpoint is offloaded away \\nfrom the agent and back to the client-side application as seen in Figure 8 and Figure 9 below. \\nThis offers the developer more granular control over the flow of data in the application. There \\nare many reasons why a Developer might choose to use functions over Extensions, but a few \\ncommon use cases are:\\n• API calls need to be made at another layer of the application stack, outside of the direct \\nagent architecture flow (e.g. a middleware system, a front end framework, etc.)\\n• Security or Authentication restrictions that prevent the agent from calling an API directly \\n(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)\\n• Timing or order-of-operations constraints that prevent the agent from making API calls in \\nreal-time. (i.e. batch operations, human-in-the-loop review, etc.)'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, page_content='Agents\\n20\\nSeptember 2024\\n• Additional data transformation logic needs to be applied to the API Response that the \\nagent cannot perform. For example, consider an API endpoint that doesn’t provide a \\nfiltering mechanism for limiting the number of results returned. Using Functions on the \\nclient-side provides the developer additional opportunities to make these transformations. \\n• The developer wants to iterate on agent development without deploying additional \\ninfrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)\\nWhile the difference in internal architecture between the two approaches is subtle as seen in \\nFigure 8, the additional control and decoupled dependency on external infrastructure makes \\nFunction Calling an appealing option for the Developer.\\nFigure 8. Delineating client vs. agent side control for extensions and function calling'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21'}, page_content='Agents\\n21\\nSeptember 2024\\nUse cases\\nA model can be used to invoke functions in order to handle complex, client-side execution \\nflows for the end user, where the agent Developer might not want the language model to \\nmanage the API execution (as is the case with Extensions). Let’s consider the following \\nexample where an agent is being trained as a travel concierge to interact with users that want \\nto book vacation trips. The goal is to get the agent to produce a list of cities that we can use \\nin our middleware application to download images, data, etc. for the user’s trip planning. A \\nuser might say something like:\\nI’d like to take a ski trip with my family but I’m not sure where to go.\\nIn a typical prompt to the model, the output might look like the following:\\nSure, here’s a list of cities that you can consider for family ski trips:\\n• Crested Butte, Colorado, USA\\n• Whistler, BC, Canada\\n• Zermatt, Switzerland\\nWhile the above output contains the data that we need (city names), the format isn’t ideal \\nfor parsing. With Function Calling, we can teach a model to format this output in a structured \\nstyle (like JSON) that’s more convenient for another system to parse. Given the same input \\nprompt from the user, an example JSON output from a Function might look like Snippet \\n5 instead.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, page_content='Agents\\n22\\nSeptember 2024\\nUnset\\nfunction_call {\\n  name: \"display_cities\"\\n  args: {\\n    \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"],\\n    \"preferences\": \"skiing\"\\n    }\\n}\\nSnippet 5. Sample Function Call payload for displaying a list of cities and user preferences\\nThis JSON payload is generated by the model, and then sent to our Client-side server to do \\nwhatever we would like to do with it. In this specific case, we’ll call the Google Places API to \\ntake the cities provided by the model and look up Images, then provide them as formatted \\nrich content back to our User. Consider this sequence diagram in Figure 9 showing the above \\ninteraction in step by step detail.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23'}, page_content=\"Agents\\n23\\nSeptember 2024\\n \\nFigure 9. Sequence diagram showing the lifecycle of a Function Call\\nThe result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with \\nthe parameters required for the Client side UI to make the call to the Google Places API. The \\nClient side UI manages the actual API call using the parameters provided by the model in the \\nreturned Function. This is just one use case for Function Calling, but there are many other \\nscenarios to consider like:\\n• You want a language model to suggest a function that you can use in your code, but you \\ndon't want to include credentials in your code. Because function calling doesn't run the \\nfunction, you don't need to include credentials in your code with the function information.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, page_content=\"Agents\\n24\\nSeptember 2024\\n• You are running asynchronous operations that can take more than a few seconds. These \\nscenarios work well with function calling because it's an asynchronous operation.\\n• You want to run functions on a device that's different from the system producing the \\nfunction calls and their arguments.\\nOne key thing to remember about functions is that they are meant to offer the developer \\nmuch more control over not only the execution of API calls, but also the entire flow of data \\nin the application as a whole. In the example in Figure 9, the developer chose to not return \\nAPI information back to the agent as it was not pertinent for future actions the agent might \\ntake. However, based on the architecture of the application, it may make sense to return the \\nexternal API call data to the agent in order to influence future reasoning, logic, and action \\nchoices. Ultimately, it is up to the application developer to choose what is right for the \\nspecific application.\\nFunction sample code\\nTo achieve the above output from our ski vacation scenario, let’s build out each of the \\ncomponents to make this work with our gemini-1.5-flash-001 model. \\nFirst, we’ll define our display_cities function as a simple Python method.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25'}, page_content='Agents\\n25\\nSeptember 2024\\nPython\\ndef display_cities(cities: list[str], preferences: Optional[str] = None):\\n \"\"\"Provides a list of cities based on the user\\'s search query and preferences.\\n Args:\\n  preferences (str): The user\\'s preferences for the search, like skiing,\\n  beach, restaurants, bbq, etc.\\n  cities (list[str]): The list of cities being recommended to the user.\\n Returns:\\n  list[str]: The list of cities being recommended to the user.\\n \"\"\"\\n return  cities\\n \\nSnippet 6. Sample python method for a function that will display a list of cities.\\nNext, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to \\nthe model. Executing the code below would result in the output as seen at the bottom of the \\ncode snippet.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26'}, page_content='Agents\\n26\\nSeptember 2024\\nPython\\nfrom vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration\\nmodel = GenerativeModel(\"gemini-1.5-flash-001\")\\ndisplay_cities_function = FunctionDeclaration.from_func(display_cities)\\ntool = Tool(function_declarations=[display_cities_function])\\nmessage = \"I’d like to take a ski trip with my family but I’m not sure where \\nto go.\"\\nres = model.generate_content(message, tools=[tool])\\nprint(f\"Function Name: {res.candidates[0].content.parts[0].function_call.name}\")\\nprint(f\"Function Args: {res.candidates[0].content.parts[0].function_call.args}\")\\n> Function Name: display_cities\\n> Function Args: {\\'preferences\\': \\'skiing\\', \\'cities\\': [\\'Aspen\\', \\'Vail\\', \\n\\'Park City\\']}\\nSnippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place\\nIn summary, functions offer a straightforward framework that empowers application \\ndevelopers with fine-grained control over data flow and system execution, while effectively \\nleveraging the agent/model for critical input generation. Developers can selectively choose \\nwhether to keep the agent “in the loop” by returning external data, or omit it based on \\nspecific application architecture requirements.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27'}, page_content='Agents\\n27\\nSeptember 2024\\nData stores\\nImagine a language model as a vast library of books, containing its training data. But unlike \\na library that continuously acquires new volumes, this one remains static, holding only the \\nknowledge it was initially trained on. This presents a challenge, as real-world knowledge is \\nconstantly evolving. Data Stores address this limitation by providing access to more dynamic \\nand up-to-date information, and ensuring a model’s responses remain grounded in factuality \\nand relevance.\\nConsider a common scenario where a developer might need to provide a small amount of \\nadditional data to a model, perhaps in the form of spreadsheets or PDFs.\\nFigure 10. How can Agents interact with structured and unstructured data?'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='Agents\\n28\\nSeptember 2024\\nData Stores allow developers to provide additional data in its original format to an agent, \\neliminating the need for time-consuming data transformations, model retraining, or fine-\\ntuning. The Data Store converts the incoming document into a set of vector database \\nembeddings that the agent can use to extract the information it needs to supplement its next \\naction or response to the user.\\nFigure 11. Data Stores connect Agents to new real-time data sources of various types.\\nImplementation and application\\nIn the context of Generative AI agents, Data Stores are typically implemented as a vector \\ndatabase that the developer wants the agent to have access to at runtime. While we won’t \\ncover vector databases in depth here, the key point to understand is that they store data \\nin the form of vector embeddings, a type of high-dimensional vector or mathematical \\nrepresentation of the data provided. One of the most prolific examples of Data Store usage \\nwith language models in recent times has been the implementation of Retrieval Augmented'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29'}, page_content='Agents\\n29\\nSeptember 2024\\nGeneration (RAG) based applications. These applications seek to extend the breadth and \\ndepth of a model’s knowledge beyond the foundational training data by giving the model \\naccess to data in various formats like:\\n• Website content\\n• Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n• Unstructured Data in formats like HTML, PDF, TXT, etc.\\nFigure 12. 1-to-many relationship between agents and data stores, which can represent various types of \\npre-indexed data\\nThe underlying process for each user request and agent response loop is generally modeled \\nas seen in Figure 13.\\n1. A user query is sent to an embedding model to generate embeddings for the query\\n2. The query embeddings are then matched against the contents of the vector database \\nusing a matching algorithm like SCaNN\\n3. The matched content is retrieved from the vector database in text format and sent back to \\nthe agent\\n4. The agent receives both the user query and retrieved content, then formulates a response \\nor action'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30'}, page_content='Agents\\n30\\nSeptember 2024\\n5. A final response is sent to the user\\nFigure 13. The lifecycle of a user request and agent response in a RAG based application\\nThe end result is an application that allows the agent to match a user’s query to a known data \\nstore through vector search, retrieve the original content, and provide it to the orchestration \\nlayer and model for further processing. The next action might be to provide a final answer to \\nthe user, or perform an additional vector search to further refine the results.\\nA sample interaction with an agent that implements RAG with ReAct reasoning/planning can \\nbe seen in Figure 14.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, page_content='Agents\\n31\\nSeptember 2024\\nFigure 14. Sample RAG based application w/ ReAct reasoning/planning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32'}, page_content='Agents\\n32\\nSeptember 2024\\nTools recap\\nTo summarize, extensions, functions and data stores make up a few different tool types \\navailable for agents to use at runtime. Each has their own purpose and they can be used \\ntogether or independently at the discretion of the agent developer. \\nExtensions Function Calling Data Stores\\nExecution Agent-Side Execution Client-Side Execution Agent-Side Execution\\nUse Case • Developer wants \\nagent to control \\ninteractions with the \\nAPI endpoints\\n• Useful when \\nleveraging native pre-\\nbuilt Extensions (i.e. \\nVertex Search, Code \\nInterpreter, etc.)\\n• Multi-hop planning \\nand API calling \\n(i.e. the next agent \\naction depends on \\nthe outputs of the \\nprevious action / \\nAPI call)\\n• Security or \\nAuthentication \\nrestrictions prevent the \\nagent from calling an \\nAPI directly\\n• Timing constraints or \\norder-of-operations \\nconstraints that \\nprevent the agent \\nfrom making API calls \\nin real-time. (i.e. batch \\noperations, human-in-\\nthe-loop review, etc.)\\n• API that is not exposed \\nto the internet, or \\nnon-accessible by \\nGoogle systems\\nDeveloper wants to \\nimplement Retrieval \\nAugmented Generation \\n(RAG) with any of the \\nfollowing data types:\\n• Website Content from \\npre-indexed domains \\nand URLs\\n• Structured Data in \\nformats like PDF, \\nWord Docs, CSV, \\nSpreadsheets, etc.\\n• Relational / Non- \\nRelational Databases\\n• Unstructured Data in \\nformats like HTML, PDF, \\nTXT, etc.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33'}, page_content=\"Agents\\n33\\nSeptember 2024\\nEnhancing model performance with \\ntargeted learning\\nA crucial aspect of using models effectively is their ability to choose the right tools when \\ngenerating output, especially when using tools at scale in production. While general training \\nhelps models develop this skill, real-world scenarios often require knowledge beyond the \\ntraining data. Imagine this as the difference between basic cooking skills and mastering \\na specific cuisine. Both require foundational cooking knowledge, but the latter demands \\ntargeted learning for more nuanced results.\\nTo help the model gain access to this type of specific knowledge, several approaches exist:\\n• In-context learning: This method provides a generalized model with a prompt, tools, and \\nfew-shot examples at inference time which allows it to learn ‘on the fly' how and when to \\nuse those tools for a specific task. The ReAct framework is an example of this approach in \\nnatural language.\\n• Retrieval-based in-context learning: This technique dynamically populates the model \\nprompt with the most relevant information, tools, and associated examples by retrieving \\nthem from external memory. An example of this would be the ‘Example Store’ in Vertex AI \\nextensions or the data stores RAG based architecture mentioned previously.\\n• Fine-tuning based learning: This method involves training a model using a larger dataset \\nof specific examples prior to inference. This helps the model understand when and how to \\napply certain tools prior to receiving any user queries. \\nTo provide additional insights on each of the targeted learning approaches, let’s revisit our \\ncooking analogy.\"), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34'}, page_content='Agents\\n34\\nSeptember 2024\\n• Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant \\ntools) and some example dishes (few-shot examples) from a customer. Based on this \\nlimited information and the chef’s general knowledge of cooking, they will need to figure \\nout how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the \\ncustomer’s preferences. This is in-context learning.\\n• Now let’s imagine our chef in a kitchen that has a well-stocked pantry (external data \\nstores) filled with various ingredients and cookbooks (examples and tools). The chef is now \\nable to dynamically choose ingredients and cookbooks from the pantry and better align \\nto the customer’s recipe and preferences. This allows the chef to create a more informed \\nand refined dish leveraging both existing and new knowledge. This is retrieval-based \\nin-context learning.\\n• Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of \\ncuisines (pre-training on a larger dataset of specific examples). This allows the chef to \\napproach future unseen customer recipes with deeper understanding. This approach is \\nperfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-\\ntuning based learning. \\nEach of these approaches offers unique advantages and disadvantages in terms of speed, \\ncost, and latency. However, by combining these techniques in an agent framework, we can \\nleverage the various strengths and minimize their weaknesses, allowing for a more robust and \\nadaptable solution.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, page_content='Agents\\n35\\nSeptember 2024\\nAgent quick start with LangChain\\nIn order to provide a real-world executable example of an agent in action, we’ll build a quick \\nprototype with the LangChain and LangGraph libraries. These popular open source libraries \\nallow users to build customer agents by “chaining” together sequences of logic, reasoning, \\nand tool calls to answer a user’s query. We’ll use our gemini-1.5-flash-001 model and \\nsome simple tools to answer a multi-stage query from the user as seen in Snippet 8.\\nThe tools we are using are the SerpAPI (for Google Search) and the Google Places API. After \\nexecuting our program in Snippet 8, you can see the sample output in Snippet 9.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, page_content='Agents\\n36\\nSeptember 2024\\nPython\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langchain_core.tools import tool\\nfrom langchain_community.utilities import SerpAPIWrapper\\nfrom langchain_community.tools import GooglePlacesTool\\nos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\"\\nos.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\\n@tool\\ndef search(query: str):\\n \"\"\"Use the SerpAPI to run a Google Search.\"\"\"\\n search = SerpAPIWrapper()\\n return  search.run(query)\\n@tool\\ndef places(query: str):\\n \"\"\"Use the Google Places API to run a Google Places Query.\"\"\"\\n places = GooglePlacesTool()\\n return  places.run(query)\\nmodel = ChatVertexAI(model=\"gemini-1.5-flash-001\")\\ntools = [search, places]\\nquery = \"Who did the Texas Longhorns play in football last week? What is the \\naddress of the other team\\'s stadium?\"\\nagent = create_react_agent(model, tools)\\ninput = {\"messages\": [(\"human\", query)]}\\nfor s in agent.stream(input, stream_mode=\"values\"):\\n message = s[\"messages\"][-1]\\n if  isinstance(message, tuple):\\n  print (message)\\n else :\\n  message.pretty_print ()\\nSnippet 8. Sample LangChain and LangGraph based agent with tools'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content='Agents\\n37\\nSeptember 2024\\nUnset\\n=============================== Human Message ================================\\nWho did the Texas Longhorns play in football last week? What is the address \\nof the other team\\'s stadium?\\n================================= Ai Message =================================\\nTool Calls: search\\nArgs:\\n query: Texas Longhorns football schedule\\n================================ Tool Message ================================\\nName: search\\n{...Results: \"NCAA Division I Football, Georgia, Date...\"}\\n================================= Ai Message =================================\\nThe Texas Longhorns played the Georgia Bulldogs last week.\\nTool Calls: places\\nArgs:\\n query: Georgia Bulldogs stadium\\n================================ Tool Message ================================\\nName: places\\n{...Sanford Stadium Address: 100 Sanford...}\\n================================= Ai Message =================================\\nThe address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA \\n30602, USA.\\nSnippet 9. Output from our program in Snippet 8\\nWhile this is a fairly simple agent example, it demonstrates the foundational components \\nof Model, Orchestration, and tools all working together to achieve a specific goal. In the \\nfinal section, we’ll explore how these components come together in Google-scale managed \\nproducts like Vertex AI agents and Generative Playbooks.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38'}, page_content='Agents\\n38\\nSeptember 2024\\nProduction applications with Vertex \\nAI agents\\nWhile this whitepaper explored the core components of agents, building production-grade \\napplications requires integrating them with additional tools like user interfaces, evaluation \\nframeworks, and continuous improvement mechanisms. Google’s Vertex AI platform \\nsimplifies this process by offering a fully managed environment with all the fundamental \\nelements covered earlier. Using a natural language interface, developers can rapidly \\ndefine crucial elements of their agents - goals, task instructions, tools, sub-agents for task \\ndelegation, and examples - to easily construct the desired system behavior. In addition, the \\nplatform comes with a set of development tools that allow for testing, evaluation, measuring \\nagent performance, debugging, and improving the overall quality of developed agents. This \\nallows developers to focus on building and refining their agents while the complexities of \\ninfrastructure, deployment and maintenance are managed by the platform itself. \\nIn Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex \\nAI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex \\nFunction Calling and Vertex Example Store to name a few. The architecture includes many of \\nthe various components necessary for a production ready application.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, page_content='Agents\\n39\\nSeptember 2024\\nFigure 15. Sample end-to-end agent architecture built on Vertex AI platform\\nYou can try a sample of this prebuilt agent architecture from our official documentation.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40'}, page_content='Agents\\n40\\nSeptember 2024\\nSummary\\nIn this whitepaper we’ve discussed the foundational building blocks of Generative AI \\nagents, their compositions, and effective ways to implement them in the form of cognitive \\narchitectures. Some key takeaways from this whitepaper include:\\n1. Agents extend the capabilities of language models by leveraging tools to access real-\\ntime information, suggest real-world actions, and plan and execute complex tasks \\nautonomously. agents can leverage one or more language models to decide when and \\nhow to transition through states and use external tools to complete any number of \\ncomplex tasks that would be difficult or impossible for the model to complete on its own.\\n2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that \\nstructures reasoning, planning, decision-making and guides its actions. Various reasoning \\ntechniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework \\nfor the orchestration layer to take in information, perform internal reasoning, and generate \\ninformed decisions or responses. \\n3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside \\nworld for agents, allowing them to interact with external systems and access knowledge \\nbeyond their training data. Extensions provide a bridge between agents and external APIs, \\nenabling the execution of API calls and retrieval of real-time information. functions provide \\na more nuanced control for the developer through the division of labor, allowing agents \\nto generate Function parameters which can be executed client-side. Data Stores provide \\nagents with access to structured or unstructured data, enabling data-driven applications.\\nThe future of agents holds exciting advancements and we’ve only begun to scratch the \\nsurface of what is possible. As tools become more sophisticated and reasoning capabilities \\nare enhanced, agents will be empowered to solve increasingly complex problems. \\nFurthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41'}, page_content='Agents\\n41\\nSeptember 2024\\ncombining specialized agents - each excelling in a particular domain or task - we can create \\na ‘mixture of agent experts’ approach, capable of delivering exceptional results across \\nvarious industries and problem areas.\\nIt’s important to remember that building complex agent architectures demands an iterative \\napproach. Experimentation and refinement are key to finding solutions for specific business \\ncases and organizational needs. No two agents are created alike due to the generative nature \\nof the foundational models that underpin their architecture. However, by harnessing the \\nstrengths of each of these foundational components, we can create impactful applications \\nthat extend the capabilities of language models and drive real-world value.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-13T10:08:53-07:00', 'moddate': '2024-11-13T10:08:58-07:00', 'trapped': '/False', 'source': '../Data/Newwhitepaper_Agents2.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42'}, page_content=\"Agents\\n42\\nSeptember 2024\\nEndnotes\\n1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2210.03629\\n2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. \\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf.\\n3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'.  \\nAvailable at: https://arxiv.org/abs/2203.11171 .\\n4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at:  \\nhttps://arxiv.org/pdf/2302.12246.pdf .\\n5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at:  \\nhttps://arxiv.org/abs/2302.00923 .\\n6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at:  \\nhttps://arxiv.org/abs/2305.10601 .\\n7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at:  \\nhttps://arxiv.org/abs/2305.08291 .\\n8. Google. 'Google Gemini Application'. Available at: http://gemini.google.com .\\n9. Swagger. 'OpenAPI Specification'. Available at: https://swagger.io/specification/.\\n10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences from \\ntraditional supervised learning'. Available at: https://ai.stanford.edu/blog/understanding-incontext/ .\\n11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n12. LangChain. 'LangChain'. Available at: https://python.langchain.com/v0.2/docs/introduction/ .\")]\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"../Data/\")  # to load multiple files from a folder\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Split the Documents into Chunks\n",
    "\n",
    "To handle large documents efficiently, we split the documents into smaller chunks using the `RecursiveCharacterTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Chunks:  2072\n"
     ]
    }
   ],
   "source": [
    "# Chunk_size: number of characters in the chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=1000)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Total number of Chunks: \", len(chunks))  # Check how many chunks we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every chunk has a `metadata` param (dictionary) that contains the key `source` of it (pdf path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data/Newwhitepaper_Prompt Engineering_v4.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].metadata['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Generate embeddings with Gemini AI\n",
    "\n",
    "Next, to embed these chunks using Gemini AI, we access one of the models available in genAI. Embeddings are vector representations of text data, and they allow us to perform similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for m in genai.list_models():\n",
    "    if \"embedContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector's first values:  [0.0065857404842972755, -0.009072314016520977, -0.052920885384082794, 0.007686096243560314, -0.026785606518387794]\n",
      "The length of the output vector (vector's dimensionality):  768\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# example\n",
    "vector = embeddings.embed_query(\"Hello, world\")\n",
    "print(\"The vector's first values: \", vector[:5])\n",
    "print(\"The length of the output vector (vector's dimensionality): \",len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this embedding function is that it can not be used for documents. It's expecting `strings`, not a `document` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: we can only send the page content -> we miss the metadata\n",
    "vector = embeddings.embed_documents(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution, as we will see in the next step, is to create a `custom function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a Vector Store for Document Retrieval\n",
    "\n",
    "We now store the document chunks and their embeddings in a vector database, which will allow us to retrieve similar documents based on user queries.\n",
    "\n",
    "In this example, we are using Chroma as our vector database. Chroma is one of the many options available for storing and retrieving embeddings efficiently. \n",
    "\n",
    "1. Create a Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "2. Create a collection: where you'll store your embeddings, documents, and any additional metadata. Collections index your embeddings and documents, and enable efficient retrieval and filtering\n",
    "    * By default, Chroma uses the **Sentence Transformers** `all-MiniLM-L6-v2` model to create embeddings.\n",
    "    * to customize one, we just need to implement the `embedding function` protocol.\n",
    "\n",
    "3. Add documents to the collection: Chroma will store your text and handle embedding and indexing automatically. You can also customize the embedding model. You must provide unique string IDs for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"my_rag_db\"\n",
    "\n",
    "# 1. Create a Chroma client\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Collection: Custom embedding function\n",
    "# Define new class that inherits from \"EmbeddingFunction\" class all the properties and methods and can add its own\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries (Class attribute: document_mode)\n",
    "    document_mode = True\n",
    "\n",
    "    # Define a method (_class_) tha makes the class instance callable like a function\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        retry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)}\n",
    "\n",
    "        response = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=input,\n",
    "            task_type=embedding_task,\n",
    "            request_options=retry_policy,\n",
    "        )\n",
    "        # Response will be a dictionary with metadata and key \"embedding\" that we are interested in\n",
    "        return response[\"embedding\"]\n",
    "    \n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add documents to the collection\n",
    "db.add(documents=[chunks[i].page_content for i in range(len(chunks))],\n",
    "       metadatas=[chunks[j].metadata for j in range(len(chunks))],\n",
    "       ids=[str(k) for k in range(len(chunks))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Retrieve Documents Based on a Query\n",
    "\n",
    "To test the retrieval system with the custom embedding function created before, we ask a query as an example, and retrieve the most relevant document chunks (the first one is the one with the highest similarity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Newwhitepaper_Agents2.pdf\n"
     ]
    }
   ],
   "source": [
    "# Switch to query mode when generating embeddings.\n",
    "embed_fn.document_mode = False\n",
    "\n",
    "# Search the Chroma DB using the specified query.\n",
    "query = \"what is an agent and tell me where did you find the information?\"\n",
    "\n",
    "result = db.query(query_texts=[query], n_results=1)\n",
    "[[passage]] = result[\"documents\"]\n",
    "[[context]] = result['metadatas']\n",
    "\n",
    "print(context[\"source\"])\n",
    "#print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5\\nSeptember 2024\\nWhat is an agent?\\nIn its most fundamental form, a Generative AI agent can be defined as an application that \\nattempts to achieve a goal by observing the world and acting upon it using the tools that it \\nhas at its disposal. Agents are autonomous and can act independently of human intervention, \\nespecially when provided with proper goals or objectives they are meant to achieve. Agents \\ncan also be proactive in their approach to reaching their goals. Even in the absence of \\nexplicit instruction sets from a human, an agent can reason about what it should do next to \\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this \\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of \\nbuilding at the time of publication.\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational \\ncomponents that drive the agent’s behavior, actions, and decision making. The combination']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'distances', 'included'])\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Augmented Generation: build a Question-Answering (Q&A) System\n",
    "\n",
    "Now that we have found a relevant passage from the set of documents, the retrieval step, the next one is the augmented generation step. To that end, we are going to use a generative AI model from Gemini `gemini-1.5-flash`.\n",
    "\n",
    "In addition, define a proper prompt to sent to the LLM model together with the input query and the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a AI expert. Provide clear, concise answers based on the provided context. \n",
    "If the information is not found in the context, state that the answer is unavailable. \n",
    "Use a maximum of three sentences.\n",
    "\n",
    "QUESTION: {query}\n",
    "PASSAGE: {passage}\n",
    "CONTEXT: {context[\"source\"]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text mentions various prompting techniques but doesn't give specific examples.  The document, \"../Data/Newwhitepaper_Prompt Engineering_v4.pdf,\" discusses prompt engineering as an iterative process of crafting, testing, analyzing, and refining prompts.  More detail on specific techniques is unavailable in this excerpt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = model.generate_content(prompt)\n",
    "print(answer.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. The retrieval process is not in working order to increase the number of results and, thus, having more references for a more precise answer later on. As an example, in the retrieval document we are missing real prompt techniques examples. This could be fixed by changing the passage variable.\n",
    "\n",
    "2. Improve the prompt:\n",
    "    * Specify answer layout to get a clear statement and the reference of the document (citation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
