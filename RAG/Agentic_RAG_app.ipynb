{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic-RAG Application - Document Q&A\n",
    "\n",
    "In this notebook, we are going to see in a step-by-step manner how to build a document Q&A application using an `agentic RAG` pipeline. In the context of RAG applications with large amount of files, having an agent can reduce the retrieval step time and improve the generation. One idea could be that this agent can reason and decide which chunks of a specific PDF document could potentially give a better answer to the query, thus the LLM is not reading all the chunks.\n",
    "    \n",
    "    -> To implement: Summarizing the documents using LangChain, provide the LLM with this context to filter out the chunks by source and retrieve only from those filtered chunks.\n",
    "\n",
    "As in the previous notebooks, **Gemini AI models** will be used for embedding and generating answers and **ChromaDB** as the vector database. The RAG module will be constructed manually instead of using LangChain for learning purposes. \n",
    "\n",
    "**LangChain** will be also used, but to ease up integration of components in building the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "* Install the python SDK to use the `Gemini API`\n",
    "* Install langchain_community (this package contains third-party integrations -> e.g. pyPDF loaders`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-genai\n",
    "%pip install -qU langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install YPython\n",
    "# %pip install dotenv\n",
    "# %pip install langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "\n",
    "from dotenv import load_dotenv  # to load environment variables (for API key variable)\n",
    "from pathlib import Path  \n",
    "\n",
    "from IPython.display import Markdown  # to get output in Markdown style\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # langChain text splitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI  # langChain access to google GenAI embedding models\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Google API key\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/api-key \n",
    "\n",
    "* Secure your API key in a environment variable file (.env) and load it using `load_dotenv()`\n",
    "* Ignore the .env file in gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path('./env')\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A System - Step by step\n",
    "\n",
    "### 1 - Load documents\n",
    "The first step is to load PDF documents into the system. We use `PyPDFLoader` from the `langchain_community` library to achieve this.\n",
    "\n",
    "Instead of loading all the files into documents, letÂ´s do it one by one to call the LLM and get the summary. It will be used the `map/reduce technique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM generative model from gemini\n",
    "llm_generative = GoogleGenerativeAI(model=\"gemini-2.0-flash-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the LLM to get a PDF summary\n",
    "def get_pdf_summary(doc: str) -> str: \n",
    "\n",
    "    # Prompt template\n",
    "    prompt_summary = \"\"\"\n",
    "    Provide clear, concise summary of the provided document in maximum 3 lines without bullet points:\n",
    "\n",
    "    {context} \n",
    "    \"\"\"\n",
    "\n",
    "    prompt_summary = prompt_summary.format(context = doc)\n",
    "    summary = llm_generative.invoke(prompt_summary)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirectoryPath = \"../Data/\"\n",
    "fileNames = [f for f in os.listdir(DirectoryPath) if f.endswith('.pdf')]\n",
    "filePaths = [DirectoryPath + f for f in fileNames]\n",
    "\n",
    "summaries = {}\n",
    "\n",
    "for path in filePaths:\n",
    "\n",
    "    loader = PyPDFLoader(path, mode=\"single\")\n",
    "    doc = loader.load()\n",
    "\n",
    "    summary = get_pdf_summary(doc[0].page_content)\n",
    "    summaries[path] = summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Split the Documents into Chunks\n",
    "\n",
    "To handle large documents efficiently, we split the documents into smaller chunks using the `RecursiveCharacterTextSplitter` class.\n",
    "\n",
    "Every chunk has a `metadata` param (dictionary) that contains the key `source` of it (pdf path).\n",
    "\n",
    "* This will be used to filter out those chunks related to a single PDF that have higher chances of containing the answer. \n",
    "\n",
    "`Remember`: the agent will provide the LLM with the summaries to determine which PDF has highly probability to contain the answer, therefore, the RAG process will use only those related-chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"../Data/\")  # Alternatively, to load multiple files in a folder\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunk_size: number of characters in the chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=1000)\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Embeddings & Vector Store for Document Retrieval\n",
    "\n",
    "We now store the document chunks and their embeddings in a vector database, which will allow us to retrieve similar documents based on user queries and filtering by source of metadata.\n",
    "\n",
    "In this example, we are using Chroma as our vector database. Chroma is one of the many options available for storing and retrieving embeddings efficiently. \n",
    "\n",
    "1. Create a Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "2. Create a collection: where you'll store your embeddings, documents, and any additional metadata. Collections index your embeddings and documents, and enable efficient retrieval and filtering\n",
    "    * By default, Chroma uses the **Sentence Transformers** `all-MiniLM-L6-v2` model to create embeddings.\n",
    "    * to customize one, we just need to implement the `embedding function` protocol.\n",
    "\n",
    "3. Add documents to the collection: Chroma will store your text and handle embedding and indexing automatically. You can also customize the embedding model. You must provide unique string IDs for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"my_rag_db\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1. Create a Chroma client\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Collection: Custom embedding function\n",
    "# Define new class that inherits from \"EmbeddingFunction\" class all the properties and methods and can add its own\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries (Class attribute: document_mode)\n",
    "    document_mode = True\n",
    "\n",
    "    # Define a method (_class_) tha makes the class instance callable like a function\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        retry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)}\n",
    "\n",
    "        response = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=input,\n",
    "            task_type=embedding_task,\n",
    "            request_options=retry_policy,\n",
    "        )\n",
    "        # Response will be a dictionary with metadata and key \"embedding\" that we are interested in\n",
    "        return response[\"embedding\"]\n",
    "    \n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add documents to the collection\n",
    "db.add(documents=[chunks[i].page_content for i in range(len(chunks))],\n",
    "       metadatas=[chunks[j].metadata for j in range(len(chunks))],\n",
    "       ids=[str(k) for k in range(len(chunks))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Retrieve Documents Based on a Query\n",
    "\n",
    "1. Query -> Retrieval Agent: agent to take the summaries and ask the LLM wich one is more likely to have the info.\n",
    "\n",
    "2. Retrieval Agent: based on the answer, take decision to do a vector search filtering the collection by metadatata(source=path of the pdf)\n",
    "\n",
    "3. Retrieval Agent -> Generative LLM: ask to answer the query with the retrieved documents\n",
    "\n",
    "4. Generative LLM -> Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent function to give the summaries and the query to a generative LLM\n",
    "def get_potential_document(summaries: dict, query: str) -> str:\n",
    "\n",
    "    prompt_agent = \"\"\"\n",
    "    You are an expert assistant able to understand an user query and identify a document that can be relevant to answer it, \n",
    "    based on the summaries of every document.\n",
    "\n",
    "    These are the available documents and their summaries, in the format of <document_name>:<summary>. \n",
    "    {context}\n",
    "\n",
    "    This is the user query: {query}\n",
    "\n",
    "    Give me the document name that is more relevant to solve the user query:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_agent = prompt_agent.format(context = summaries, query=query)\n",
    "    pdf_name = llm_generative.invoke(prompt_agent)\n",
    "\n",
    "    return pdf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function with the llm_generative model\n",
    "query = \"Tell me what you know about agents in a few lines, no more than 5\"\n",
    "pdf_name = get_potential_document(summaries, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Search filtering by pdf_name\n",
    "embed_fn.document_mode = False  # mode for embedding query\n",
    "\n",
    "retrieved_docs = db.query(query_texts=query, where={'source':pdf_name}, n_results=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Augmented Generation: build a Question-Answering (Q&A) System\n",
    "\n",
    "Now that we have found a relevant passage from the set of documents, the retrieval step, the next one is the augmented generation step. To that end, we are going to use a generative AI model from Gemini, the one we have been using so far for generating context `llm_generative`.\n",
    "\n",
    "In addition, define a proper prompt to sent to the LLM model together with the input query and the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A Generative AI agent is an application that tries to achieve a goal by observing the world and acting upon it using its available tools. Agents are autonomous and can act independently, especially when given proper goals. They can also proactively reason about what to do next, even without explicit instructions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_final_answer= \"\"\"\n",
    "You are a AI expert. Provide clear, concise answers based on the provided context. \n",
    "If the information is not found in the context, state that the answer is unavailable. \n",
    "Use a maximum of three sentences.\n",
    "\n",
    "QUERY: {query}\n",
    "CONTEXT: {context}\n",
    "OUTPUT:\n",
    "\"\"\"\n",
    "\n",
    "prompt_final_answer = prompt_final_answer.format(query=query, context=retrieved_docs['documents'])\n",
    "answer = llm_generative.invoke(prompt_final_answer) \n",
    "Markdown(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
