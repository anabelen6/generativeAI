{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application - Document Q&A\n",
    "\n",
    "In this notebook, we are going to see in a step-by-step manner how to build a document Q&A application using a simple RAG pipeline. \n",
    "\n",
    "To that end, **Gemini AI models** will be used for embedding and generating answers, **ChromaDB** as the vector database, and **LangChain** for managing the retrieval process.\n",
    "\n",
    "Based on: https://python.plainenglish.io/building-a-rag-application-with-gemini-ai-step-by-step-guide-24636dd21f5b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "* Install the python SDK to use the `Gemini API`\n",
    "* Install langchain_community (this package contains third-party integrations -> to use `pyPDF`) \n",
    "* Install langchain-chroma integration package to access `ChromaDB` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-genai\n",
    "%pip install -qU langchain_community pypdf\n",
    "%pip install -qU \"langchain-chroma>=0.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv  # to load environment variables\n",
    "from pathlib import Path  \n",
    "\n",
    "from IPython.display import Markdown  # to get output in Markdown style\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader  # to loa PDFs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # langChain text splitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # langChain access to google GenAI embedding models\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma  # LangChain access to Crhoma DB\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Google API key\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/api-key \n",
    "\n",
    "* Secure your API key in a environment variable file (.env) and load it using `load_dotenv()`\n",
    "* Ignore the .env file in gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path('./env')\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A System - Step by step\n",
    "\n",
    "### 1 - Load documents from PDF\n",
    "The first step is to load a PDF document into the system. We use `PyPDFLoader` from the `langchain_community` library to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 0, 'page_label': '1'}, page_content='Foundational \\nLarge Language \\nModels & \\nText Generation\\nAuthors: Mohammadamin Barektain,  \\nAnant Nawalgaria, Daniel J. Mankowitz,  \\nMajd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \\nMatan Kalman, Elena Buchatskaya,                                     \\nAliaksei Severyn, and Antonio Gulli'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 1, 'page_label': '2'}, page_content='Foundational Large Language Models & Text Generation\\n2\\nSeptember 2024\\nAcknowledgements\\nReviewers and Contributors\\nAdam Sadvovsky\\nYonghui Wu\\nAndrew Dai\\nEfi Kokiopolou\\nChuck Sugnet\\nAleksey Vlasenko\\nErwin Huizenga\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nMark Iverson\\nDesigner\\nMichael Lanning'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 2, 'page_label': '3'}, page_content='Introduction 6\\nWhy language models are important 7\\nLarge language models 8\\n Transformer 9\\n  Input preparation and embedding 11\\n  Multi-head attention 12\\n   Understanding self-attention 12\\n   Multi-head attention: power in diversity 14\\n  Layer normalization and residual connections 15\\n  Feedforward layer  15\\n  Encoder and decoder 16\\n  Training the transformer 17\\n   Data preparation 17\\n   Training and loss function 18\\nThe evolution of transformers 19\\n GPT-1 19\\n BERT 21\\n GPT-2 22\\nTable of contents'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 3, 'page_label': '4'}, page_content='GPT-3/3.5/4 23\\n LaMDA 24\\n Gopher 25\\n GLaM 26\\n Chinchilla 27\\n PaLM 28\\n  PaLM 2 29\\n Gemini 29\\n Other open models 32\\n Comparison 34\\nFine-tuning large language models 37\\n Supervised fine-tuning  38\\n Reinforcement learning from human feedback 39\\n Parameter Efficient Fine-Tuning 41\\nUsing large language models 44\\n Prompt engineering  44\\n Sampling Techniques and Parameters 45\\nAccelerating inference 46\\n Trade offs 47\\n  The Quality vs Latency/Cost Tradeoff 48\\n  The Latency vs Cost Tradeoff 48\\n Output-approximating methods 49\\n  Quantization 49'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 4, 'page_label': '5'}, page_content='Distillation 50\\n Output-preserving methods 52\\n  Flash Attention 52\\n  Prefix Caching 53\\n  Speculative Decoding 55\\n Batching and Parallelization 57\\nApplications 58\\n Code and mathematics 61\\n Machine translation 62\\n Text summarization 63\\n Question-answering 63\\n Chatbots 64\\n Content generation 65\\n Natural language inference 65\\n Text classification 66\\n Text analysis 67\\n Multimodal applications 68\\nSummary 69\\nEndnotes 71'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 5, 'page_label': '6'}, page_content='Foundational Large Language Models & Text Generation\\n6\\nSeptember 2024\\nIntroduction\\nThe advent of Large Language Models (LLMs) represents a seismic shift in the world of \\nartificial intelligence. Their ability to process, generate, and understand user intent is \\nfundamentally changing the way we interact with information and technology. \\nAn LLM is an advanced artificial intelligence system that specializes in processing, \\nunderstanding, and generating human-like text. These systems are typically implemented as \\na deep neural network and are trained on massive amounts of text data. This allows them to \\nlearn the intricate patterns of language, giving them the ability to perform a variety of tasks, \\nlike machine translation, creative text generation, question answering, text summarization, \\nand many more reasoning and language oriented tasks. This whitepaper dives into the \\ntimeline of the various architectures and approaches building up to the large language \\nmodels and the architectures being used at the time of publication. It also discusses fine-\\nWe believe that this new crop of \\ntechnologies has the potential to \\nassist, complement, empower, \\nand inspire people at any time \\nacross almost any field.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 6, 'page_label': '7'}, page_content='Foundational Large Language Models & Text Generation\\n7\\nSeptember 2024\\ntuning techniques to customize an LLM to a certain domain or task, methods to make the \\ntraining more efficient, as well as methods to accelerate inference. These are then followed \\nby various applications and code examples. \\nWhy language models are important\\nLLMs achieve an impressive performance boost from the previous state of the art across \\na variety of different and complex tasks which require answering questions or complex \\nreasoning, making feasible many new applications. These include language translation, code \\ngeneration and completion, text generation, text classification, and question-answering, \\nto name a few. Although foundational LLMs trained in a variety of tasks on large amounts \\nof data perform very well out of the box and display emergent behaviors (e.g. the ability to \\nperform tasks they have not been directly trained for) they can also be adapted to solve \\nspecific tasks where performance out of the box is not at the level desired through a process \\nknown as fine-tuning. This requires significantly less data and computational resources than \\ntraining an LLM from scratch. LLMs can be further nudged and guided towards the desired \\nbehavior by the discipline of prompt engineering: the art and science of composing the \\nprompt and the parameters of an LLM to get the desired response.\\nThe big question is: how do these large language models work? The next section explores the \\ncore building blocks of LLMs, focusing on transformer architectures and their evolution from \\nthe original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \\nmost capable LLM. We also cover training and fine-tuning techniques, as well as methods to \\nimprove the speed of response generation. The whitepaper concludes with a few examples \\nof how language models are used in practice.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 7, 'page_label': '8'}, page_content='Foundational Large Language Models & Text Generation\\n8\\nSeptember 2024\\nLarge language models\\nA language model predicts the probability of a sequence of words. Commonly, when given \\na prefix of text, a language model assigns probabilities to subsequent words. For example, \\ngiven the prefix “The most famous city in the US is…”, a language model might predict high \\nprobabilities to the words “New York” and “Los Angeles” and low probabilities to the words \\n“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \\nmodern language models are often based on neural models, such as transformers.\\nBefore the invention of transformers1, recurrent neural networks (RNNSs) were the popular \\napproach for modeling sequences. In particular, “long short-term memory” (LSTM) and \\n“gated recurrent unit” (GRU) were common architectures.3 This area includes language \\nproblems such as machine translation, text classification, text summarization, and question-\\nanswering, among others. RNNs process input and output sequences sequentially. They \\ngenerate a sequence of hidden states based on the previous hidden state and the current \\ninput. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \\nduring training (though recent work in state space modeling is attempting to overcome \\nthese challenges).\\nTransformers, on the other hand, are a type of neural network that can process sequences \\nof tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \\ncan better model long-term contexts and are easier to parallelize than RNNs. This makes \\nthem significantly faster to train, and more powerful compared to RNNs for handling long-\\nterm dependencies in long sequence tasks. However, the cost of self-attention in the original \\ntransformers is quadratic in the context length which limits the size of the context, while \\nRNNs have a theoretically infinite context length. Transformers have become the most \\npopular approach for sequence modeling and transduction problems in recent years.\\nHerein, we discuss the first version of the transformer model and then move on to the more \\nrecent advanced models and algorithms.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 8, 'page_label': '9'}, page_content='Foundational Large Language Models & Text Generation\\n9\\nSeptember 2024\\nTransformer\\nThe transformer architecture was developed at Google in 2017 for use in a translation model.1 \\nIt’s a sequence-to-sequence model capable of converting sequences from one domain \\ninto sequences in another domain. For example, translating French sentences to English \\nsentences. The original transformer architecture consists of two parts: an encoder and a \\ndecoder. The encoder converts the input text (e.g., a French sentence) into a representation, \\nwhich is then passed to the decoder. The decoder uses this representation to generate the \\noutput text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \\nthe transformer encoder is linear in the size of its input. Figure 1 shows the design of the \\noriginal transformer architecture.\\nThe transformer consists of multiple layers. A layer in a neural network comprises a set of \\nparameters that perform a specific transformation on the data. In the diagram you can see \\nan example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \\nLinear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \\nThe input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \\nnetwork. Input embeddings are used to represent the input tokens to the model. Output \\nembeddings are used to represent the output tokens that the model predicts. For example, in \\na machine translation model, the input embeddings would represent the words in the source \\nlanguage, while the output embeddings would represent the words in the target language. \\nThe output layer (e.g., Softmax) is the final layer that produces the output of the network. The \\nhidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \\nwhere the magic happens!'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 9, 'page_label': '10'}, page_content='Foundational Large Language Models & Text Generation\\n10\\nSeptember 2024\\nFigure 1. Original Transformer 1 (P.C: 5 )'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 10, 'page_label': '11'}, page_content='Foundational Large Language Models & Text Generation\\n11\\nSeptember 2024\\nTo better understand the different layers in the transformer, let’s use a French-to-English \\ntranslation task as an example. Here, we explain how a French sentence is input into the \\ntransformer and a corresponding English translation is output. We will also describe each of \\nthe components inside the transformer from Figure 1.\\nInput preparation and embedding\\nTo prepare language inputs for transformers, we convert an input sequence into tokens and \\nthen into input embeddings. At a high level, an input embedding is a high-dimensional vector \\nthat represents the meaning of each token in the sentence. This embedding is then fed into \\nthe transformer for processing. Generating an input embedding involves the following steps:\\n1. Normalization (Optional): Standardizes text by removing redundant whitespace, \\naccents, etc.\\n2. Tokenization: Breaks the sentence into words or subwords and maps them to integer \\ntoken IDs from a vocabulary.\\n3. Embedding: Converts each token ID to its corresponding high-dimensional vector, \\ntypically using a lookup table. These can be learned during the training process.\\n4. Positional Encoding: Adds information about the position of each token in the sequence \\nto help the transformer understand word order.\\nThese steps help to prepare the input for the transformers so that they can better \\nunderstand the meaning of the text.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 11, 'page_label': '12'}, page_content='Foundational Large Language Models & Text Generation\\n12\\nSeptember 2024\\nMulti-head attention\\nAfter converting input tokens into embedding vectors, you feed these embeddings into \\nthe multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \\ntransformers; it enables them to focus on specific parts of the input sequence relevant to \\nthe task at hand and to capture long-range dependencies within sequences more effectively \\nthan traditional RNNs. \\nUnderstanding self-attention\\nConsider the following sentence: “The tiger jumped out of a tree to get a drink because it \\nwas thirsty.” Self-attention helps to determine relationships between different words and \\nphrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \\nso we would expect these two words to be strongly connected. Self-attention achieves this \\nthrough the following steps (Figure 2):\\n1. Creating queries, keys, and values: Each input embedding is multiplied by three learned \\nweight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \\nare like specialized representations of each word.\\n• Query: The query vector helps the model ask, “Which other words in the sequence are \\nrelevant to me?”\\n• Key: The key vector is like a label that helps the model identify how a word might be \\nrelevant to other words in the sequence.\\n• Value: The value vector holds the actual word content information.\\n2. Calculating scores: Scores are calculated to determine how much each word should \\n‘attend’ to other words. This is done by taking the dot product of the query vector of one \\nword with the key vectors of all the words in the sequence.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 12, 'page_label': '13'}, page_content='Foundational Large Language Models & Text Generation\\n13\\nSeptember 2024\\n3. Normalization: The scores are divided by the square root of the key vector dimension (dk) \\nfor stability, then passed through a softmax function to obtain attention weights. These \\nweights indicate how strongly each word is connected to the others.\\n4. Weighted values: Each value vector is multiplied by its corresponding attention weight. \\nThe results are summed up, producing a context-aware representation for each word.\\nFigure 2. The process of computing self-attention in the multi-head attention module 1 (P.C: 5 )'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 13, 'page_label': '14'}, page_content='Foundational Large Language Models & Text Generation\\n14\\nSeptember 2024\\nIn practice, these computations are performed at the same time, by stacking the query, key \\nand value vectors for all the tokens into Q, K and V matrices and multiplying them together as \\nshown in Figure 3.\\nFigure 3. The basic operation of attention, 1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \\nof queries and keys (P.C:5 )\\nMulti-head attention: power in diversity\\nMulti-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \\neach ‘head’ potentially focusing on different aspects of the input relationships. The outputs \\nfrom each head are concatenated and linearly transformed, giving the model a richer \\nrepresentation of the input sequence.\\nThe use of multi-head attention improves the model’s ability to handle complex language \\npatterns and long-range dependencies. This is crucial for tasks that require a nuanced \\nunderstanding of language structure and content, such as machine translation, text \\nsummarization, and question-answering. The mechanism enables the transformer to consider \\nmultiple interpretations and representations of the input, which enhances its performance on \\nthese tasks.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 14, 'page_label': '15'}, page_content='Foundational Large Language Models & Text Generation\\n15\\nSeptember 2024\\nLayer normalization and residual connections\\nEach layer in a transformer, consisting of a multi-head attention module and a feed-forward \\nlayer, employs layer normalization and residual connections. This corresponds to the Add \\nand Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \\ncorresponds to layer normalization. Layer normalization computes the mean and variance \\nof the activations to normalize the activations in a given layer. This is typically performed to \\nreduce covariate shift as well as improve gradient flow to yield faster convergence during \\ntraining as well as improved overall performance. \\nResidual connections propagate the inputs to the output of one or more layers. This has the \\neffect of making the optimization procedure easier to learn and also helps deal with vanishing \\nand exploding gradients. \\nThe Add and Norm layer is applied to both the multi-head attention module and the feed-\\nforward layer described in the following section.\\nFeedforward layer \\nThe output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \\nfed into the feedforward layer of each transformer block. This layer applies a position-wise \\ntransformation to the data, independently for each position in the sequence, which allows the \\nincorporation of additional non-linearity and complexity into the model’s representations. The \\nfeedforward layer typically consists of two linear transformations with a non-linear activation \\nfunction, such as ReLU or GELU, in between. This structure adds further representational \\npower to the model. After processing by the feedforward layer, the data undergoes \\nanother ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \\ntransformer models.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 15, 'page_label': '16'}, page_content='Foundational Large Language Models & Text Generation\\n16\\nSeptember 2024\\nEncoder and decoder\\nThe original transformer architecture relies on a combination of encoder and decoder \\nmodules. Each encoder and decoder consists of a series of layers, with each layer \\ncomprising key components: a multi-head self-attention mechanism, a position-wise feed-\\nforward network, normalization layers, and residual connections. \\nThe encoder’s primary function is to process the input sequence into a continuous \\nrepresentation that holds contextual information for each token. The input sequence is first \\nnormalized, tokenized, and converted into embeddings. Positional encodings are added to \\nthese embeddings to retain sequence order information. Through self-attention mechanisms, \\neach token in the sequence can dynamically attend to any other token, thus understanding \\nthe contextual relationships within the sequence. The output from the encoder is a series of \\nembedding vectors Z representing the entire input sequence. \\nThe decoder is tasked with generating an output sequence based on the context provided \\nby the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\\nof-sequence token. The decoder layers employ two types of attention mechanisms: masked \\nself-attention and encoder-decoder cross-attention. Masked self-attention ensures that \\neach position can only attend to earlier positions in the output sequence, preserving the \\nauto-regressive property. This is crucial for preventing the decoder from having access to \\nfuture tokens in the output sequence. The encoder-decoder cross-attention mechanism \\nallows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \\nembeddings generated by the encoder. This iterative process continues until the decoder \\npredicts an end-of-sequence token, thereby completing the output sequence generation.\\nMajority of recent LLMs adopted a decoder-only variant of transformer architecture. This \\napproach forgoes the traditional encoder-decoder separation, focusing instead on directly \\ngenerating the output sequence from the input. The input sequence undergoes a similar'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 16, 'page_label': '17'}, page_content='Foundational Large Language Models & Text Generation\\n17\\nSeptember 2024\\nprocess of embedding and positional encoding before being fed into the decoder. The \\ndecoder then uses masked self-attention to generate predictions for each subsequent \\ntoken based on the previously generated tokens. This streamlined approach simplifies the \\narchitecture for specific tasks where encoding and decoding can be effectively merged.\\nTraining the transformer\\nWhen talking about machine learning models, it’s important to differentiate between \\ntraining and inference. Training typically refers to modifying the parameters of the model, \\nand involves loss functions and backpropagation. Inference is when model is used only \\nfor the predicted output, without updating the model weights. The model parameters are \\nfixed during inference. Up until now we learned how transformers generate outputs during \\ninference. Next, we focus on how to train transformers to perform one or more given tasks.\\nData preparation\\nThe first step is data preparation, which involves a few important steps itself. First, clean the \\ndata by applying techniques such as filtering, deduplication, and normalization. The next \\nstep is tokenization where the dataset is converted into tokens using techniques such as \\nByte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \\nwhich is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \\n’language’ for processing and understanding text. Finally, the data is typically split into a \\ntraining dataset for training the model as well as a test dataset which is used to evaluate the \\nmodels performance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 17, 'page_label': '18'}, page_content='Foundational Large Language Models & Text Generation\\n18\\nSeptember 2024\\nTraining and loss function\\nA typical transformer training loop consists of several parts: First, batches of input \\nsequences are sampled from a training dataset. For each input sequence, there is a \\ncorresponding target sequence. In unsupervised pre-training, the target sequence is \\nderived from the input sequence itself. The batch of input sequences is then fed into the \\ntransformer. The transformer generates predicted output sequences. The difference \\nbetween the predicted and target sequences is measured using a loss function (often cross-\\nentropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \\nthe transformer’s parameters. This process is repeated until the transformer converges to a \\ncertain level of performance or until it has been trained on a pre-specified number of tokens. \\nThere are different approaches to formulating the training task for transformers depending \\non the architecture used:\\n• Decoder-only models are typically pre-trained on the language modeling task (e.g., see \\nendnote12, 13). The target sequence for the decoder is simply a shifted version of the input \\nsequence. Given a training sequence like ‘the cat sat on the mat’ various input/target \\npairs can be generated for the model. For example the input “the cat sat on” should \\npredict “the” and subsequently the input “the cat sat on the” should predict target \\nsequence “mat”.\\n• Encoder-only models (like BERT)14 are often pre-trained by corrupting the input sequence \\nin some way and having the model try to reconstruct it. One such approach is masked \\nlanguage modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \\non the mat” and the sequence target would be the original sentence.\\n• Encoder-decoder models (like the original transformer) are trained on sequence-to-\\nsequence supervised tasks such as translation (input sequence “Le chat est assis sur \\nle tapis” and target “The cat sat on the mat”), question-answering (where the input \\nsequence is a question and the target sequence is the corresponding answer), and'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 18, 'page_label': '19'}, page_content='Foundational Large Language Models & Text Generation\\n19\\nSeptember 2024\\nsummarization (where the input sequence is a full article and the target sequence is its \\ncorresponding summary). These models could also be trained in an unsupervised way by \\nconverting other tasks into sequence-to-sequence format. For example, when training \\non Wikipedia data, the input sequence might be the first part of an article, and the target \\nsequence comprises the remainder of the article.\\nAn additional factor to consider during training is the ‘context length’. This refers to the \\nnumber of previous tokens the model can ‘remember’ and use to predict the next token in \\nthe sequence. Longer context lengths allow the model to capture more complex relationships \\nand dependencies within the text, potentially leading to better performance. However, longer \\ncontexts also require more computational resources and memory, which can slow down \\ntraining and inference. Choosing an appropriate context length involves balancing these \\ntrade-offs based on the specific task and available resources.\\nThe evolution of transformers\\nThe next sections provide an overview of the various transformer architectures. These \\ninclude encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \\nGPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\\nGPT-1\\nGPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only model developed \\nby OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \\nseveral billion words) and is able to generate text, translate languages, write different kinds \\nof creative content, and answer questions in an informative way. The main innovations in \\nGPT-1 were:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 19, 'page_label': '20'}, page_content='Foundational Large Language Models & Text Generation\\n20\\nSeptember 2024\\n• Combining transformers and unsupervised pre-training: Unsupervised pre-training \\nis a process of training a language model on a large corpus of unlabeled data. Then, \\nsupervised data is used to fine-tune the model for a specific task, such as translation \\nor sentiment classification. In prior works, most language models were trained using a \\nsupervised learning objective. This means that the model was trained on a dataset of \\nlabeled data, where each example had a corresponding label. This approach has two main \\nlimitations. First, it requires a large amount of labeled data, which can be expensive and \\ntime-consuming to collect. Second, the model can only generalize to tasks that are similar \\nto the tasks that it was trained on. Semi-supervised sequence learning was one of the first \\nworks that showed that unsupervised pre-training followed by supervised training was \\nsuperior than supervised training alone.\\nUnsupervised pre-training addresses these limitations by training the model on a large \\ncorpus of unlabeled data. This data can be collected more easily and cheaply than labeled \\ndata. Additionally, the model can generalize to tasks that are different from the tasks that \\nit was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \\nwas used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \\nbooks, which provides the model with a large amount of data to learn from. Additionally, \\nthe corpus contains long stretches of contiguous text, which helps the model learn long-\\nrange dependencies. Overall, unsupervised pre-training is a powerful technique that can \\nbe used to train language models that are more accurate and generalizable than models \\nthat are trained using supervised learning alone. \\n• Task-aware input transformations: There are different kinds of tasks such as textual \\nentailment and question-answering that require a specific structure. For example, \\ntextual entailment requires a premise and a hypothesis; question-answering requires a \\ncontext document; a question and possible answers. One of the contributions of GPT-1 \\nis converting these types of tasks which require structured inputs into an input that the \\nlanguage model can parse, without requiring task-specific architectures on top of the \\npre-trained architecture. For textual entailment, the premise p and the hypothesis h are'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 20, 'page_label': '21'}, page_content='Foundational Large Language Models & Text Generation\\n21\\nSeptember 2024\\nconcatenated with a delimiter token ($) in between - [p, $, h]. For question answering, the \\ncontext document c is concatenated with the question q and a possible answer a with a \\ndelimiter token in between the question and answer - [c,q,$,a].\\nGPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \\nGPT-1 was a significant breakthrough in natural language processing (NLP), it had some \\nlimitations. For example, the model was prone to generating repetitive text, especially when \\ngiven prompts outside the scope of its training data. It also failed to reason over multiple \\nturns of dialogue and could not track long-term dependencies in text. Additionally, its \\ncohesion and fluency were limited to shorter text sequences, and longer passages would \\nlack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \\npre-training, which laid the foundation for larger and more powerful models based on the \\ntransformer architecture.\\nBERT\\nBERT14 which stands for Bidirectional Encoder Representations from Transformers, \\ndistinguishes itself from traditional encoder-decoder transformer models by being an \\nencoder-only architecture. Instead of translating or producing sequences, BERT focuses \\non understanding context deeply by training on a masked language model objective. In \\nthis setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \\nto predict the original word based on the surrounding context. Another innovative aspect \\nof BERT’s training regime is the next sentence prediction loss, where it learns to determine \\nwhether a given sentence logically follows a preceding one. By training on these objectives, \\nBERT captures intricate context dependencies from both the left and right of a word, and \\nit can discern the relationship between pairs of sentences. Such capabilities make BERT \\nespecially good at tasks that require natural language understanding, such as question-\\nanswering, sentiment analysis, and natural language inference, among others. Since this is an \\nencoder-only model, BERT cannot generate text.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 21, 'page_label': '22'}, page_content='Foundational Large Language Models & Text Generation\\n22\\nSeptember 2024\\nGPT-2\\nGPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \\nGPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \\nof its training dataset:\\n• Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \\nconsists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \\nis a rating metric used on Reddit and a value of three means that all the posts were of a \\nreasonable level of quality.\\n• Parameters: GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \\nthan the previous model. More parameters increase the model’s learning capacity. The \\nauthors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \\n(GPT-2) parameters, and found that the model with the most parameters performed better \\non every subsequent task.\\nThis scaling up resulted in a model that was able to generate more coherent and realistic text \\nthan GPT-1. Its ability to generate human-like responses made it a valuable tool for various \\nnatural language processing tasks, such as content creation and translation. Specifically, \\nGPT-2 demonstrated significant improvement in capturing long-range dependencies and \\ncommon sense reasoning. While it performed well in some tasks, it did not outperform state-\\nof-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \\nachievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \\ntransfer is the ability of a model to generalize to a new task without being trained on it, which \\nrequires the model to understand the task based on the given instruction. For example, for \\nan English to German translation task, the model might be given an English sentence followed \\nby the word “German” and a prompt (“:”). The model would then be expected to understand \\nthat this is a translation task and generate the German translation of the English sentence. \\nGPT-2 was able to perform tasks such as machine translation, text summarization, and \\nreading comprehension without any explicit supervision.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 22, 'page_label': '23'}, page_content='Foundational Large Language Models & Text Generation\\n23\\nSeptember 2024\\nThe study discovered that performance on zero-shot tasks increased in a log-linear manner \\nas the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \\nmore parameters improved the model’s ability to understand tasks and surpass the state-of-\\nthe-art on many tasks in zero-shot settings.\\nGPT-3/3.5/4\\nGPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \\nsignificant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \\nflexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \\n175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \\nThis increase in model size allowed GPT-3 to store and recall an even more vast amount of \\ninformation, understand nuanced instructions, and generate more coherent and contextually \\nrelevant text over longer passages.\\nWhile GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \\nunderstand and execute tasks with just a few examples, or sometimes even without any \\nexplicit examples—simply based on the instruction provided. This highlights GPT-3’s more \\ndynamic understanding and adaptation abilities, reducing the need for task-specific fine-\\ntuning which was more prevalent in GPT-2.\\nFinally, GPT-3’s large model scale and diverse training corpus have led to better \\ngeneralization across a broader range of tasks. This means that out-of-the-box, without \\nany further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \\ntranslation to question-answering, compared to GPT-2. It’s also worth noting that the release \\napproach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \\nthey chose to make GPT-3 available as a commercial API, reflecting both its utility and the \\norganization’s evolving stance on deployment.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 23, 'page_label': '24'}, page_content='Foundational Large Language Models & Text Generation\\n24\\nSeptember 2024\\nInstruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\\ntuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \\nmodel behaviors. Outputs from this model were then ranked and it was then further fine-\\ntuned using Reinforcement Learning from Human Feedback. This led to improved instruction \\nfollowing in the model. A 1.3B parameter InstructGPT model had better human evaluations \\nthan the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \\nreductions in toxicity.\\nGPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \\nunderstanding and generating code. It’s been optimized for dialogue. And it’s capable of \\nreceiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \\ntokens. \\nGPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \\ntext inputs and producing text outputs.19 Specifically, accepting text or images as input \\nand outputting text. This model has broader general knowledge and advanced reasoning \\ncapabilities. It can receive context windows of up to 128,000 tokens and has a maximum \\noutput of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \\nacross diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \\nwithout specialized instructions. Its performance often matches or even exceeds human \\ncapabilities and significantly outperforms earlier models like GPT-3.5.\\nLaMDA\\nGoogle’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \\ncontribution to the arena of large-scale language models, designed primarily to engage in \\nopen-ended conversations. Unlike traditional chatbots which operate in more constrained \\nand predefined domains, LaMDA is engineered to handle a wide array of topics, delivering'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 24, 'page_label': '25'}, page_content='Foundational Large Language Models & Text Generation\\n25\\nSeptember 2024\\nmore natural and flowing conversations. LaMDA was trained on dialogue-focused data to \\nencourage ongoing conversational flow, not just isolated responses, ensuring users can have \\nmore extensive and explorative dialogues.\\nWhile GPT models, especially the later iterations like GPT-3, have strived to address a \\nmultitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \\nfocus is on maintaining and enhancing conversational depth and breadth. GPT models \\nshine on their ability to produce coherent long-form content and perform various tasks \\nwith minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \\nstriving to mimic the unpredictability and richness of human conversations. \\nGopher\\nGopher22 is a 280 billion parameter language model based on the decoder-only transformer \\narchitecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \\nwrite different kinds of creative content, and answer your questions in an informative way. \\nSimilar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\\n• Dataset: The researchers curated a high-quality text dataset called MassiveText, which \\ncontains over 10 terabytes of data and 2.45B documents from web pages, books, news \\narticles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \\nImportantly, they improved the quality of the data by filtering it, such as by removing \\nduplicate text and deduplicating similar documents. This significantly improved the \\nmodel’s performance on downstream tasks.\\n• Optimization: The researchers used a warmup learning rate for 1,500 steps and then \\ndecayed it using a cosine schedule. They also had an interesting rule that as they \\nincreased the model size, they decreased the learning rate and increased the number of \\ntokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \\nbased on the global gradient norm helped stabilize the training.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 25, 'page_label': '26'}, page_content='Foundational Large Language Models & Text Generation\\n26\\nSeptember 2024\\nGopher was evaluated on a variety of tasks, including mathematics, common sense, logical \\nreasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \\nGopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \\nGopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \\ntasks such as abstract algebra.\\nThe authors also conducted a study on the effect of model size on different types of \\ntasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \\nincreasing the number of parameters had a significant impact on logical reasoning and \\nreading comprehension, but it did not improve performance as much on tasks such as \\ngeneral knowledge, where performance eventually almost plateaued.\\nFigure 4. Ablation study 22  on the effect of model size on the performance of Gopher on different types \\nof tasks\\nGLaM\\nGLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \\nlanguage model. Mixture-of-experts based models are much more computationally efficient \\ngiven their parameter count. This is achieved by only activating a subset of their parameters'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 26, 'page_label': '27'}, page_content='Foundational Large Language Models & Text Generation\\n27\\nSeptember 2024\\n(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \\nof the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \\noverall performance compared to GPT-3.\\nChinchilla\\nUntil 2022, LLMs were primarily scaled by increasing the model size and using datasets that \\nare relatively small by current standards (up to 300 billion tokens for the largest models). \\nThis approach was informed by the Kaplan et al.24 study, which examined how performance \\nof a language model, measured by cross-entropy loss, varies with changes in computational \\nbudget, model size, and dataset size. Specifically, given a 100-fold increase in computational \\nresources (C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \\n(Nopt∝ C0.73 ), while increasing dataset size by only 3.5 times (Dopt∝ C0.27 ). \\nThe Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \\napproaches to find that near equal scaling in parameters and data is optimal with increasing \\ncompute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \\nboth data size and model size. \\nFigure 5. Overlaid predictions from three different approaches from Chinchilla paper, 25  along with \\nprojections from Kaplan et al 24'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 27, 'page_label': '28'}, page_content='Foundational Large Language Models & Text Generation\\n28\\nSeptember 2024\\nTo verify the updated scaling law, DeepMind trained a 70B parameter model (called \\nChinchilla) using the same compute budget as the previously trained Gopher model. \\nChinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \\nMegatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \\n4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \\nalso smaller.\\nThe findings of Chinchilla had significant ramifications for the development of future LLMs. \\nFocus shifted into finding ways to scale dataset size (while maintaining quality) alongside \\nincreasing parameter count. Extrapolating this trend suggests that training dataset size \\nmay soon be limited by the amount of text data available. This has led to new research by \\nMuennighoff et al.27 exploring scaling laws in data-constrained regimes.\\nPaLM\\nPathways language model (PaLM)28 is a 540-billion parameter transformer-based large \\nlanguage model developed by Google AI. It was trained on a massive dataset of text and \\ncode and is capable of performing a wide range of tasks, including common sense reasoning, \\narithmetic reasoning, joke explanation, code generation, and translation.\\nAt the time of its release, PaLM was also able to achieve state-of-the-art performance on \\nmany language benchmarks, for example GLUE and SuperGLUE.29\\nOne of the key features of PaLM is its ability to scale efficiently. This is thanks to the \\nPathways system, which Google developed to distribute the training of large language \\nmodels across two TPU v4 Pods.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 28, 'page_label': '29'}, page_content='Foundational Large Language Models & Text Generation\\n29\\nSeptember 2024\\nPaLM 2\\nPaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \\narchitectural and training enhancements, PaLM 2 is even more capable than PaLM, with \\nfewer total parameters. It excels at advanced reasoning tasks, including code generation, \\nmath, classification, question answering, and translation.\\nPaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \\nnumber of commercial models Google released as part of Google Cloud Generative AI.\\nGemini\\nFigure 6. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \\ntokenized and fed into its transformer model. The transformer generates an output that can contain images \\nand text'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 29, 'page_label': '30'}, page_content='Foundational Large Language Models & Text Generation\\n30\\nSeptember 2024\\nGemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \\ntake interleaved sequences of text, image, audio, and video as input. It’s built on top of \\ntransformer decoders and has architectural improvements for scale as well as optimized \\ninference on Google’s Tensor Processing Units (TPUs). In its current 1.5 version, these models \\nare trained to support contexts of different sizes, up to 2M tokens in the Gemini 1.5 Pro \\nversion on Vertex AI and employ mechanisms such as multi-query attention for efficiency. \\nGemini models also employ a Mixture of Experts architecture to optimize efficiency and \\ncapabilities of the models. Multimodality allows the models to process text, images and video \\nin input, with more modalities in input and output expected in the future.\\nThe Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \\nsize and configuration. The pre-training data consists of web documents, books, code, and \\nimage, audio, and video data. \\nLarger models are trained for the compute-optimal number of tokens using the same \\napproach as in Chinchilla paper,25 while small models are trained on significantly more tokens \\nthan compute optimal to improve performance for a given inference budget.\\nThe Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \\nNano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\\nart results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \\nGemini Nano is designed for on-device applications. The Gemini Nano models leverage \\nadvancements such as distillation to produce state-of-the-art performance for small \\nlanguage models on tasks such as summarization and reading comprehension. As the Gemini \\nmodels are natively multi-modal, it can be seen that training across multiple modalities does \\nindeed lead to a model that is capable of achieving strong capabilities in each domain.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 30, 'page_label': '31'}, page_content='Foundational Large Language Models & Text Generation\\n31\\nSeptember 2024\\nDuring the initial part of 2024, Google introduced the latest model of the Gemini family, \\nGemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \\nmodel  also dramatically increased the size of the context window to millions of tokens \\nand is capable of recalling and reasoning over those millions of tokens, including multiple \\nlong documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \\ncapabilities across different domains:\\n• Code understanding: It can process massive codebases and answer highly specific \\ncode-related questions.\\n• Language learning: The model can learn new languages never observed at training time \\nsolely based on reference materials provided within its input\\n• Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \\nfrom the novel ‘Les Misérables’ based on a simple sketch.\\n• Video comprehension: It can analyze entire movies, answering detailed questions and \\npinpointing specific timestamps with remarkable accuracy.\\nGoogle’s Gemini 1.5 Pro model excels at retrieving information from even very long \\ndocuments. In their study,32 it demonstrated 100% recall on documents up to 530,000 \\ntokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \\naccuracy when finding information in documents up to 10 million tokens.\\nMoreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \\ninstructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \\nprevious Gemini models. The model accurately followed almost 90% of instructions and fully \\ncompleted 66% of the complex tasks.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 31, 'page_label': '32'}, page_content='Foundational Large Language Models & Text Generation\\n32\\nSeptember 2024\\nGemini Flash is a new addition to the Gemini model family and the fastest Gemini model \\nserved in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \\ncost-efficient to serve and features a breakthrough long context window of 1 million tokens. \\nAlthough it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \\nacross vast amounts of information and delivers impressive quality for its size.\\nFurthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \\nmodels built from the same research and technology used to create the Gemini models.33 The \\nfirst model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \\na massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \\nLLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \\na single GPU.\\nGemma 2,33 developed by Google AI, represents a significant advancement in the field of \\nopen large language models. Designed with a focus on efficiency, the 27-billion parameter \\nmodel boasts performance comparable to much larger models like Llama 3 70B33 on standard \\nbenchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \\ndevelopers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \\nto popular community tools, further enhances its versatility. With its strong performance, \\nefficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \\nand democratizing AI capabilities.\\nOther open models\\nThe landscape of open LLMs is rapidly evolving, with a growing number of models where \\nboth the code and pre-trained weights are publicly accessible. Below we highlight some of \\nthe known examples:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 32, 'page_label': '33'}, page_content='Foundational Large Language Models & Text Generation\\n33\\nSeptember 2024\\n• LLaMA 234: Released by Meta AI, LLaMA 2 is a family of pretrained and fine-tuned \\nLLMs ranging from 7B to 70B parameters. It shows significant improvements over its \\npredecessor, LLaMA 1, including a 40% larger pre-training dataset (2 trillion tokens), \\ndoubled context length (4096 tokens), and the use of grouped-query attention. The \\nfine-tuned version, LLaMA 2-Chat, is optimized for dialogue and shows competitive \\nperformance against closed-source models of the same size.\\n• LLaMA 3.221: Released by Meta AI, LLaMA 3.2 is the next generation of their open LLMs. \\nLlama 3.2 includes multilingual text-only models (1B, 3B) and vision LLMs (11B, 90B), with \\nquantized versions of 1B and 3B offering on average up to 56% smaller size and 2-3x \\nspeedup, ideal for on-device and edge deployments. LLaMA 3.2 utilizes grouped-query \\nattention and a 128K token vocabulary for enhanced performance and efficiency.\\n• Mixtral35: Developed by Mistral AI, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) \\nmodel. While its total parameter count is 47B, it utilizes only 13B active parameters per \\ntoken during inference, leading to faster inference and higher throughput. This model \\nexcels in mathematics, code generation, and multilingual tasks, often outperforming \\nLLaMA 2 70B in these domains. Mixtral also supports a 32k token context length, enabling \\nit to handle significantly longer sequences. Its instruction-tuned version, Mixtral 8x7B-\\nInstruct, surpasses several closed-source models on human evaluation benchmarks.\\n• Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \\n72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \\nstrong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \\nLLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \\nlanguage understanding, reasoning, and math.\\n• Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \\non a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \\nthrough rigorous cleaning and filtering processes. The 34B model achieves performance'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 33, 'page_label': '34'}, page_content='Foundational Large Language Models & Text Generation\\n34\\nSeptember 2024\\ncomparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\\ngrade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \\nvision-language model (Yi-VL), and a depth-upscaled 9B model.\\n• Grok-138: Developed by xAI, Grok-1 is a 314B parameter Mixture-of-Experts model with \\n25% of the weights active on a given token. It is the raw base model checkpoint from the \\npre-training phase and is not fine-tuned for specific tasks like dialogue. Grok-1 operates \\nwith a context length of 8k tokens.\\nThe pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \\nhave been many contributions to the field in both the academic and commercial settings. \\nWith over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \\nof the models and teams that have contributed to the development of LLMs. However, an \\nabbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \\nStanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \\nNVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai , Llama from \\nMeta mentioned above and many others. Some of notable companies developing commercial \\nfoundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \\nand many others in addition to Google and OpenAI mentioned in previous sections. It is \\nimportant when using a model to confirm that the license is appropriate for your use case as \\nmany models are provided with very specific terms of use.\\nComparison\\nIn this section, we observed how transformer-based language models have evolved. They \\nstarted as encoder-decoder architectures with hundreds of millions of parameters trained \\non hundreds of millions of tokens, and have grown to be massive decoder-only architectures \\nwith billions of parameters and trained on trillions of tokens. Table 1 shows how the \\nimportant hyperparameters for all the models discussed in this whitepaper have evolved'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 34, 'page_label': '35'}, page_content='Foundational Large Language Models & Text Generation\\n35\\nSeptember 2024\\nover time. The scaling of data and parameters has not only improved the performance of \\nLLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\\nshot generalizations to new tasks. However, even the best of these LLMs still have many \\nlimitations. For example, they are not good at engaging in human-like conversations, their \\nmath skills are limited, and they might not be aligned with human ethics (e.g., they might be \\nbiased or generate toxic responses). In the next section, we learn how a lot of these issues \\nare being addressed.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 35, 'page_label': '36'}, page_content='Foundational Large Language Models & Text Generation\\n36\\nSeptember 2024\\nModel\\nAttention\\n(2017)\\nGPT \\n(2018)\\nGPT-2\\n(2019)\\nGPT-3\\n(2020)\\nLaMDA  \\n(2021)\\nGopher\\n(2021)\\nChinchilla\\n(2022)\\nOptimizer ADAM ADAM ADAM ADAM ADAM ADAM ADAM-W\\n# Parameters 213M 117M 1.5B 175B 137B 280B 70B\\nVocab size ~37K ~40K ~50K ~50K ~32K ~32K ~32K\\nEmbedding \\ndimension 1024 768 1600 12288 8192 16384 8192\\nKey dimension 64 64 64 128 128 128 128\\n# heads (H) 16 12 25 96 128 128 64\\n# encoder \\nlayers 6 N/A N/A N/A N/A N/A N/A\\n# decoder \\nlayers 6 12 48 96 64 80 80\\nFeed forward \\ndimension 4 * 1024 4 * 768 4 * 1600 4 * 12288 8 * 8192 4 * 16384 4 * 8192\\nContext Token \\nSize N/A 512 1024 2048 N/A 2048 2048\\nPre-Training \\ntokens ~160MA ~1.25BA ~10B ~300B ~168B ~300B ~1.4T\\nTable 1. Important hyperparameters for transformers-based large language models\\nA. This number is an estimate based on the reported size of the dataset.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 36, 'page_label': '37'}, page_content='Foundational Large Language Models & Text Generation\\n37\\nSeptember 2024\\nFine-tuning large language models\\nLarge language models typically undergo multiple training stages. The first stage, often \\nreferred to as pre-training, is the foundational stage where an LLM is trained on large, \\ndiverse, and unlabelled text datasets where it’s tasked to predict the next token given the \\nprevious context. The goal of this stage is to leverage a large, general distribution of data \\nand to create a model that is good at sampling from this general distribution. After language \\nmodel pretraining, the resulting LLM usually demonstrates a reasonable level of language \\nunderstanding and language generation skills across a variety of different tasks which \\nare typically tested through zero-shot or few-shot prompting (augmenting the instruction \\nwith a few examples / demonstrations). Pretraining is the most expensive in terms of time \\n(from weeks to months depending on the size of the model) and the amount of required \\ncomputational resources, (GPU/TPU hours).\\nAfter training, the model can be further specialized via fine-tuning, typically called \\ninstruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \\nset of task-specific demonstration datasets where its performance is also measured across \\na set of domain-specific tasks. The following are some examples of behaviors that can be \\nimproved using fine-tuning:\\n• Instruction-tuning/instruction following: The LLM is provided as input an instruction to \\nfollow which might include summarizing a piece of text, writing a piece of code, or writing \\na poem in a certain style.17\\n• Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \\non conversational data in the form of questions and responses. This is often called \\nmulti-turn dialogue.39'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 37, 'page_label': '38'}, page_content='Foundational Large Language Models & Text Generation\\n38\\nSeptember 2024\\n• Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \\ntoxic outputs. It involves a multi-pronged approach encompassing careful data selection, \\nhuman-in-the-loop validation, and incorporating safety guardrails. Techniques like \\nreinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \\nand ethical responses.\\nFine-tuning is considerably less costly and more data efficient compared to pre-training. \\nNumerous techniques exist to optimize the costs further which are discussed later in \\nthis whitepaper.\\nSupervised fine-tuning \\nAs mentioned in the previous section, SFT is the process of improving an LLM’s performance \\non a specific task or set of tasks by further training it on domain-specific, labeled data. The \\ndataset is typically significantly smaller than the pre-training datasets, and is usually human-\\ncurated and of high quality. \\nIn this setting, each data point consists of an input (prompt) and a demonstration (target \\nresponse). For example, questions (prompt) and answers (target response), translations from \\none language (prompt) to another language (target response), a document to summarize \\n(prompt), and the corresponding summary (target response). \\nIt’s important to note that, while fine-tuning can be used to improve the performance on \\nparticular tasks as mentioned above, it can also serve the purpose of helping the LLM \\nimprove its behavior to be safer, less toxic, more conversational, and better at following \\ninstructions.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 38, 'page_label': '39'}, page_content='Foundational Large Language Models & Text Generation\\n39\\nSeptember 2024\\nReinforcement learning from human feedback\\nTypically, after performing SFT, a second stage of fine-tuning occurs which is called \\nreinforcement learning from human feedback (RLHF). This is a very powerful fine-tuning \\ntechnique that enables an LLM to better align with human-preferred responses (i.e. making \\nits responses more helpful, truthful, safer, etc.). \\nFigure 7. An example RLHF procedure \\nIn contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \\ndemonstration data), RLHF makes it possible to also leverage negative outputs thus \\npenalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \\nnegative output makes it less likely to generate unhelpful or unsafe responses. \\nTo leverage RLHF, a reward model (RM) typically needs to be trained with a procedure similar \\nto that in Figure 7. An RM is usually initialized with a pretrained transformer model, often also \\none that is SFT. Then it is tuned on human preference data which is either single sided (with a \\nprompt, response and a score) or composed of a prompt and a pair of responses along with'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 39, 'page_label': '40'}, page_content='Foundational Large Language Models & Text Generation\\n40\\nSeptember 2024\\na preference label indicating which of the two responses was preferred. For example, given \\ntwo summaries, A and B, of the same article, a human rater selects a preferred summary \\n(relying on the detailed guidance). We refer to the provided preference labels as human \\nfeedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \\nrank order when more than 2 candidates are evaluated, or a more detailed assessment of the \\nsummary quality. The preference signal can also incorporate many dimensions that capture \\nvarious aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \\ntruthfulness. \\nFigure 7 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \\npreference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \\n(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \\ngenerate responses that are better aligned with human preferences. \\nTo better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \\nfeedback to generate preference labels. It’s also possible to remove the need for training \\nRLHF by leveraging approaches such as direct preference optimization (DPO).45 Both RLHF \\nand RLAIF can be used on Google Cloud.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 40, 'page_label': '41'}, page_content='Foundational Large Language Models & Text Generation\\n41\\nSeptember 2024\\nParameter Efficient Fine-Tuning\\nBoth SFT and RLHF are still very costly in terms of compute time and accelerators required, \\nespecially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \\nthere are some really useful and effective techniques that can make fine-tuning significantly \\ncheaper and faster compared to pre-training and full fine-tuning. One such family of \\nmethods is parameter efficient fine-tuning (PEFT) techniques. \\nAt a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \\norder of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \\nThe perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \\nThis has the benefit of training a significantly smaller set of weights, compared to traditional \\nfine-tuning of the entire model. \\nSome common PEFT techniques include the adapter, low-rank adaptation, and \\nsoft prompting:\\n• Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\\ntrained model. Only the adapter parameters are trained, resulting in significantly fewer \\nparameters than traditional SFT. \\n• Low-Rank Adaptation (LoRA)47 tackles efficiency differently. It uses two smaller matrices \\nto approximate the original weight matrix update instead of fine-tuning the whole LLM. \\nThis technique freezes the original weights and trains these update matrices, significantly \\nreducing resource requirements with minimum additional inference latency. Additionally, \\nLoRA has improved variants such as QLoRA,48 which uses quantized weights for even \\ngreater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \\nmeaning you can train a LoRA module that specializes in one task and easily replace it with \\nanother LoRA module trained on a different task. It also makes it easier to transfer the \\nmodel since assuming the receiver has the original matrix, only the update matrices need \\nto be provided.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 41, 'page_label': '42'}, page_content='Foundational Large Language Models & Text Generation\\n42\\nSeptember 2024\\n• Soft prompting49 is a technique for conditioning frozen large language models with \\nlearnable vectors instead of hand-crafted text prompts. These vectors, called soft \\nprompts, are optimized on the training data and can be as few as five tokens, making them \\nparameter-efficient and enabling mixed-task inference. \\nFor most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \\nprompting, but the order is reversed when it comes to cost. All three approaches are more \\nmemory efficient than traditional fine-tuning and achieve comparable performance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 42, 'page_label': '43'}, page_content='Foundational Large Language Models & Text Generation\\n43\\nSeptember 2024\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel\\nfrom vertexai.preview.tuning import sft\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = ‘<project_id>’\\nREGION = ‘<region>’\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n# define training & eval dataset.\\nTRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\\npeft_train_sample.jsonl’\\n# set base model and specify a name for the tuned model\\nBASE_MODEL = ‘gemini-1.5-pro-002’\\nTUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\\n# start the fine-tuning job\\nsft_tuning_job = sft.train(\\n   source_model=BASE_MODEL,\\n   train_dataset=TRAINING_DATASET,\\n   # # Optional:\\n   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\\n)\\n# Get the tuning job info.\\nsft_tuning_job.to_dict()\\n# tuned model endpoint name\\ntuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\\n# use the tuned model\\ntuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\\nprint(tuned_genai_model.generate_content(contents=’What is a LLM?’))\\nSnippet 1. SFT fine tuning on Google cloud'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 43, 'page_label': '44'}, page_content='Foundational Large Language Models & Text Generation\\n44\\nSeptember 2024\\nUsing large language models\\nPrompt engineering and sampling techniques have a strong influence on the performance of \\nLLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \\nthat you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \\ndetermine the way in which output tokens are chosen and influence the correctness, \\ncreativity and diversity of the resulting output. We next discuss different variants of prompt \\nengineering and sampling techniques as well as touch on some important parameters that \\ncan have a significant impact on LLM performance.\\nPrompt engineering \\nLLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \\nengineering is a critical component in guiding an LLM to yield desired outputs. This might \\ninclude grounding the model to yield factual responses or unleashing the creativity of the \\nmodel to tell a story or write a song. Examples of prompt engineering include providing \\nclear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \\nimportant information, providing additional background details etc. \\nYou will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \\ncontext of prompt engineering. We define these terms below: \\n• Few-shot prompting: This is when you provide the LLM with a task description, as well \\nas a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \\nresponse. For example, you might provide the model with the name of a few countries \\nand their capital cities, then ask it to generate the capital for a new country that isn’t in \\nthe examples.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 44, 'page_label': '45'}, page_content='Foundational Large Language Models & Text Generation\\n45\\nSeptember 2024\\n• Zero-shot prompting: This is when you provide the LLM directly with a prompt with \\ninstructions. You usually give the LLM a task description and the LLM relies heavily on its \\nexisting knowledge to output the correct response. This requires no additional data or \\nexamples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\\n• Chain-of-thought prompting: This technique aims to improve performance on complex \\nreasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \\nthat demonstrates how to solve similar problems using step-by-step reasoning. The \\nLLM then generates its own chain of thought for the new problem, breaking it down into \\nsmaller steps and explaining its reasoning. Finally, it provides an answer based on its \\nreasoning process.\\nPrompt engineering is an active area of research.\\nSampling Techniques and Parameters\\nA variety of sampling techniques can be employed to determine how the model chooses \\nthe next token in a sequence. They are essential for controlling the quality, creativity, and \\ndiversity of the LLM’s output. The following is a breakdown of different sampling techniques \\nand their important parameters:\\n• Greedy search50: Selects the token with the highest probability at each step. This is the \\nsimplest option but it can lead to repetitive and predictable outputs.\\n• Random sampling:50 Selects the next token according to the probability distribution, where \\neach token is sampled proportionally to its predicted probability. This can produce more \\nsurprising and creative text, but also a higher chance of nonsensical output.\\n• Temperature sampling:50 Adjusts the probability distribution by a temperature parameter. \\nHigher temperatures promote diversity, lower temperatures favor high-probability tokens.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 45, 'page_label': '46'}, page_content='Foundational Large Language Models & Text Generation\\n46\\nSeptember 2024\\n• Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \\ncontrols the degree of randomness.\\n• Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \\ncumulative probability adds up to P. This allows the model to adapt the number of potential \\ncandidates depending on its confidence, favoring more diversity when uncertain and \\nfocusing on a smaller set of highly probable words when confident.\\n• Best-of-N sampling: Generates N separate responses and selects the one deemed best \\naccording to a predetermined metric (e.g., a reward model or a logical consistency check). \\nThis is particularly useful for short snippets or situations where logic and reasoning \\nare key.\\nBy combining prompt engineering with sampling techniques and correctly calibrated \\nhyperparameters, you can greatly influence the LLM’s response, making it more relevant, \\ncreative, and consistent for your specific needs.\\nUntil now, we have seen the various types of LLM architectures, their underlying technology, \\nas well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \\nnow look at some key research about how the decoding process in LLMs can be sped up \\nconsiderably to generate faster responses. \\nAccelerating inference\\nThe scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \\nto hold today. Language models have been consistently increasing in size and this has been \\na direct contributor to the vast improvement in these models’ quality and accuracy over the \\nlast few years. As increasing the number of parameters has improved the quality of LLMs it'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 46, 'page_label': '47'}, page_content='Foundational Large Language Models & Text Generation\\n47\\nSeptember 2024\\nhas also increased the computational resources needed to run them. Numerous approaches \\nhave been used to try and improve the efficiency of LLMs for different tasks as developers \\nare incentivized to reduce cost and latency for model users. Balancing the expense of \\nserving a model in terms of time, money, energy is known as the cost-performance tradeoff \\nand often needs adjusting for particular use cases.\\nTwo of the main resources used by LLMs are memory and computation. Techniques for \\nimproving the efficiency or speed of inference focus primarily on these resources. The \\nspeed of the connection between memory and compute is also critical, but usually hardware \\nconstrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \\nAdditional parameters increase both the size of memory required to hold the model and \\ncomputations needed to produce the model results.\\nWith LLMs being increasingly adopted for large-scale and low-latency use cases, finding \\nways to optimize their inference performance has become a priority and an active research \\ntopic with significant advancements. We will explore a number of methods and a few \\ntradeoffs for accelerating inference.\\nTrade offs\\nMany of the high yielding inference optimisation methods mandate trading off a number of \\nfactors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \\ndifferent inference use cases and requirements. A number of the optimization methods we \\nwill discuss later fall somewhere on the spectrum of these tradeoffs.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 47, 'page_label': '48'}, page_content='Foundational Large Language Models & Text Generation\\n48\\nSeptember 2024\\nTrading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \\nwe’re completely sacrificing that factor, it just means that we’re accepting what might be \\na marginal degradation in quality, latency or cost for the benefit of substantially improving \\nanother factor.\\nThe Quality vs Latency/Cost Tradeoff\\nIt is possible to improve the speed and cost of inference significantly through accepting \\nwhat might be marginal to negligible drops in the model’s accuracy. One  example of this \\nis using a smaller model to perform the task. Another example is quantisation where we \\ndecrease the precision of the model’s parameters thereby leading to faster and less memory \\nintensive calculations.\\nOne important distinction when approaching this trade-off is between the theoretical \\npossibility of a quality loss versus the practical capability of the model to perform the desired \\ntask. This is use case specific and exploring it will often lead to significant speedups without \\nsacrificing quality in a meaningful or noticeable way. For example, if the task we want the \\nmodel to perform is simple, then a smaller model or a quantised one will likely be able to \\nperform this task well. Reduction in parametric capacity or precision does not automatically \\nmean that the model is less capable at that specific task.\\nThe Latency vs Cost Tradeoff\\nAnother name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \\nto the system’s ability at handling multiple requests efficiently. Better throughput on the same \\nhardware means that our LLM inference cost is reduced, and vice versa.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 48, 'page_label': '49'}, page_content='Foundational Large Language Models & Text Generation\\n49\\nSeptember 2024\\nMuch like traditional software systems, there are often multiple opportunities to tradeoff \\nlatency against the cost of LLM inference. This is an important tradeoff since LLM inference \\ntends to be the slowest and most expensive component in the entire stack; balancing latency \\nand cost intentionally is key to making sure we tailor LLM performance to the product or use \\ncase it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \\nwhere cost can be a more important factor than the latency of any particular request. On the \\nother hand, an LLM chatbot product will place much higher importance on request latency.\\nNow that we’ve covered some of the important tradeoffs to consider when optimizing \\ninference, let’s examine some of the most effective inference acceleration techniques. As \\ndiscussed in the tradeoffs section, some optimization techniques can have an impact on the \\nmodel’s output. Therefore we will split the methods into two types: output-approximating \\nand output-preserving.\\nOutput-approximating methods\\nQuantization\\nLLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \\nDuring inference, matrix operations are then applied to these model weights to produce \\nnumerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \\nprecision in which weights and activations are stored, transferred and operated upon. The \\ndefault representation of weights and activations is usually 32 bits floating numbers, with \\nquantization we can drop the precision to 8 or even 4 bit integers.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 49, 'page_label': '50'}, page_content='Foundational Large Language Models & Text Generation\\n50\\nSeptember 2024\\nQuantization has multiple performance benefits, it reduces the memory footprint of \\nthe model, allowing to fit larger models on the same hardware, it also reduces the \\ncommunication overhead of weights and activations within one chip and across chips in \\na distributed inference setup- therefore speeding up inference as communication is a \\nmajor contributor to latency. In addition, decreasing the precision of weights/activations \\ncan enable faster arithmetic operations on these models as some accelerator hardware \\n(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \\nprecision representations.\\nQuantization’s impact on quality can be very mild to non-existent depending on the use \\ncase and model.  Further, in cases where quantisation might introduce a quality regression, \\nthat regression can be small compared to the performance gain, therefore allowing for an \\neffective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \\nspeed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.\\nQuantization can be either applied as an inference-only operation, or it can be incorporated \\ninto the training (referred to as Quantisation Aware Training QAT). QAT is generally \\nconsidered to be a more resilient approach as the model is able to recover some of the \\nquantisation-related quality losses during training. To make sure we get the best cost/quality \\ntradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \\nvs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \\nor group-wise58).\\nDistillation\\nUsing a smaller model to perform a task is one of the most efficient inference optimization \\ntechniques, however, smaller models can demonstrate significant regressions on quality \\ncompared to their larger counterparts.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 50, 'page_label': '51'}, page_content='Foundational Large Language Models & Text Generation\\n51\\nSeptember 2024\\nDistillation is a set of training techniques that targets improving the quality of a smaller model \\n(the student) using a larger model (the teacher). This method can be effective because larger \\nmodels outperform smaller ones even if both are trained on the same data, mainly due to \\nparametric capacity and training dynamics. The gap in performance continues as the training \\ndataset grows as illustrated by Figure 8.\\nIt is worth noticing that even at low volumes of training data, large models can already \\ndemonstrate better performance than the correspondingly trained smaller models, this fact \\nleads us to the first variant of distillation which is referred to as data distillation or model \\ncompression.56 We use a large model which was trained on the data we have to generate \\nmore synthetic data to train the smaller student model, the increase in data volume will help \\nmove the the student further along the quality line compared to only training on the original \\ndata. Synthetic data needs to be approached carefully as it needs to be of high quality and \\ncan lead to negative effects otherwise.\\nFigure 8. An illustration of the performance of models of various sizes as a function of the training \\ndataset’s size'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 51, 'page_label': '52'}, page_content='Foundational Large Language Models & Text Generation\\n52\\nSeptember 2024\\nOther distillation techniques attempt to bring the student model closer to the teacher \\non a more granular level than just synthetic data generation. One prominent technique is \\nknowledge distillation57, in this approach we attempt to align the output token distribution \\nof the student model to that of the teacher’s, this can be much more sample efficient than \\ndata distillation. On-policy distillation59 is another technique that leverages feedback from \\nthe teacher model on each sequence generated by the student in a reinforcement learning \\nsetup. \\nOutput-preserving methods\\nThese methods are guaranteed to be quality neutral, they cause no changes to the model \\noutput which often makes them obvious first steps to optimize inference before facing the \\nmore nuanced tradeoffs of the approximating methods\\nFlash Attention\\nScaled Dot-product Attention, which is the predominant attention mechanism in the \\ntransformer architecture, is a quadratic operation on the input length. Optimizing the self-\\nattention calculation can bring significant latency and cost wins.\\nFlash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \\nthe attention algorithm IO Aware, particularly trying to minimize the amount of data we move \\nbetween the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \\nTPUs and GPUs. When calculating attention, the order of operations is changed and multiple \\nlayers are fused so we can utilize the faster memory tiers as efficiently as possible.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 52, 'page_label': '53'}, page_content='Foundational Large Language Models & Text Generation\\n53\\nSeptember 2024\\nFlash Attention is an exact algorithm, it maintains the numerical output of the attention \\ncomputation and can yield significant latency benefits due to reducing the IO overhead, Tri \\nDao et al.62 showed 2-4X latency improvements in the attention computation.\\nPrefix Caching\\nOne of the most compute intensive, and thus slowest, operations in LLM inference is \\ncalculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \\nLLM, this operation is often referred to as prefill. The final output of prefill is what is termed \\nKV Cache which includes the attention key and value scores for each layer of the transformer \\nfor the entire input. This cache is vital during the decoding phase which produces the output \\ntokens, the KV cache allows us to avoid recalculating attention scores for the input on each \\nautoregressive decode step.\\nPrefix Caching refers to the process of caching the KV Cache itself between subsequent \\ninference requests in order to reduce the latency and cost of the prefill operation. The way \\nthe self-attention mechanism works makes reusing KV caches possible because tokens will \\nonly pay attention to tokens that came before them in the sequence. If there’s new input \\nbeing appended to input that the model has seen before, then we can potentially avoid \\nrecalculating the prefill for the older input.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 53, 'page_label': '54'}, page_content='Foundational Large Language Models & Text Generation\\n54\\nSeptember 2024\\nFigure 9. An illustration of Prefix Caching in a chat scenario\\nFigure 9 illustrates how prefix caching works in a multi-turn scenario with a document upload. \\nOn the first user turn, the prefill operation has to process the entire document therefor taking \\n500ms, the resulting KV cache is then stored so that on the second user turn, we can retrieve \\nthe cache directly from storage and avoid recomputing it for the long doc, therefore saving \\nsubstantial amounts of compute and latency.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 54, 'page_label': '55'}, page_content='Foundational Large Language Models & Text Generation\\n55\\nSeptember 2024\\nPrefix caches can be stored either in memory or on disk and fetched on-demand. One \\nimportant consideration is making sure that the input structure/schema remains prefix-\\ncaching friendly, we should avoid changing the prefix in subsequent requests as that will \\ninvalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \\nthe very beginning of each request will invalidate the cache completely as every subsequent \\nrequest will have a new prefix.\\nMany LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \\nwhere users will have a multi-turn conversation that can span 10s of 1000s of tokens and \\nwe can avoid recalculating the KV cache for the previous parts of the conversation. Large \\ndocument/code uploads is another use case where the artifact the user uploads will remain \\nunchanged from one request to the next. All that’s changing are the questions the user is \\nasking, so caching the KV cache for the document (especially for larger artifacts) can result \\nin significant latency and cost savings.\\nPrefix caching is available as a service called Context Caching on Google AI studio52 and  \\nVertex AI on Google Cloud53.\\nSpeculative Decoding\\nThe first phase of LLM inference, known as prefill, is compute bound due large matrix \\noperations on many tokens occurring in parallel. The second phase, known as decode, is \\ngenerally memory bound as tokens are auto-regressively decoded one at a time.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 55, 'page_label': '56'}, page_content='Foundational Large Language Models & Text Generation\\n56\\nSeptember 2024\\nIt is not easy to naively use additional parallel compute capacity to speed up decode \\ngiven the  need to wait for the current token to be produced before we can calculate what \\nthe next token should be (as per the self-attention mechanism), the decode process is \\ninherently serial.\\nSpeculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \\na way to utilize the spare compute capacity to make each decode step faster. The main idea \\nis to use a much smaller secondary model (often referred to as the drafter) to run ahead of \\nthe main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \\nas the drafter is much faster and smaller than the main model. We then use the main model to \\nverify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \\nfirst two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \\nhypothesis with the maximum number of tokens. For example:\\nFigure 10. An illustration of speculative decoding over 3 tokens\\nNote that the 3 main model steps run in parallel. And because we are not compute bound in \\ndecode, we can use the spare capacity to get much better decode latencies. In the example \\nabove, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \\nspeculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 56, 'page_label': '57'}, page_content='Foundational Large Language Models & Text Generation\\n57\\nSeptember 2024\\ndecoding, there’s only one main model step on the critical path due to parallelization, so we \\nneed 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely \\nquality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \\nin the first place, so the only thing speculative decoding does is run ahead and present \\nhypotheses that the main model can accept or reject in parallel.\\nOne important condition for speculative decoding to work effectively is that the drafter model \\nhas good levels of alignment with the main model, otherwise we won’t be able to accept any \\nof the tokens. So investing in the training quality of the drafter model is worthwhile to get \\nbetter latencies.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nBatching and Parallelization\\nMost of the optimization techniques we’ve discussed so far are specific to Machine Learning \\nand Transformer architecture in particular. However, much like any software system, there \\nare opportunities to improve throughput and latency through a combination of 1) batching \\nless compute-intensive operations (i.e. we can run multiple requests on the same hardware \\nsimultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\\nintensive parts of the computations (i.e. we can divide the computation and split it amongst \\nmore hardware instances to get more compute capacity and therefore better latencies\\nBatching in LLMs is most useful on the decode side - as we explained in the Speculative \\nDecoding section, decode is not compute-bound and therefore there’s an opportunity \\nto batch more requests. We need to be careful that we batch computations in a way that'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 57, 'page_label': '58'}, page_content='Foundational Large Language Models & Text Generation\\n58\\nSeptember 2024\\nenables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \\nand GPUs). We also need to make sure we remain within the memory limits, as decode is a \\nmemory intensive operations, batching more requests will put more pressure on the free \\nmemory available. Batching has become an important component in most high-throughput \\nLLM inference setups.\\nParallelization is also a widely used technique given the variety of opportunities in \\ntransformers for horizontal scaling across more hardware instances. There are multiple \\nparallelism techniques across the model input (Sequence parallelism) the model layers \\n(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \\nconsiderations for parallelism is the cost of communication and synchronization between \\nthe different shards that we distribute to other machines. Communication is a significant \\noverhead and can erode the benefits of adding more computational capacity if we’re not \\ncareful about which parallelization strategy to use. On the other hand, selecting the right \\nstrategy to balance the need for additional compute and the communication cost can yield \\nsignificant latency wins.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nApplications\\nLarge language models are revolutionizing the way we interact with and process information. \\nWith their unprecedented ability to understand context and generate content, they’re \\ntransforming numerous applications in the worlds of text, code, images, audio and video. \\nHere we collected a few examples of application areas, but the reader should keep in mind \\nthat this is not a comprehensive list and that many new ideas are emerging continuously'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 58, 'page_label': '59'}, page_content='Foundational Large Language Models & Text Generation\\n59\\nSeptember 2024\\nabout how to best utilize the capabilities of these new tools. For more information about \\noptimally building and deploying functioning applications based on the following mentioned \\nuse cases, refer to the subsequent whitepapers. \\nIt is also very simple to generate text-based responses for your use case using either \\nthe Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \\ncode examples from these SDKs to generate responses to text prompts using the Gemini \\nmodel. Note that the multimodal aspects of Gemini are covered in their respective \\ndedicated whitepapers.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 59, 'page_label': '60'}, page_content='Foundational Large Language Models & Text Generation\\n60\\nSeptember 2024\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.language_models import TextGenerationModel\\nfrom vertexai.preview.generative_models import GenerationConfig,GenerativeModel\\n# Set values as per your requirements\\nPROJECT_ID = ‘<project_id>’ # set to your project_id\\nvertexai.init(project=PROJECT_ID, location=’us-central1’)\\nPROMPT= ‘What is a LLM?’ # set your prompt here\\nmodel = GenerativeModel(‘gemini-1.5-pro-002’)\\n# call the Gemini API\\nresponse = model.generate_content(\\n   PROMPT)\\nprint(response.text)\\n# google AI Studio SDK\\nimport google.generativeai as genai\\nimport os\\n# update with your API key\\ngenai.configure(api_key=os.environ[“GOOGLE_API_KEY”])\\n# choose the model\\nmodel = genai.GenerativeModel(‘gemini-pro’)\\nresponse = model.generate_content(‘What is a LLM?’) # set your prompt here\\nprint(response.text)\\nSnippet 3. Using Vertex AI and Google AI studio SDKs for unimodal text gene'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 60, 'page_label': '61'}, page_content='Foundational Large Language Models & Text Generation\\n61\\nSeptember 2024\\nCode and mathematics\\nGenerative models can comprehend and generate code and algorithms to supercharge \\ndevelopers by assisting them across many application areas. Some of the popular use cases \\nfor code include:\\n• Code generation: LLMs can be prompted in natural language to generate code in a \\nspecific programming language to perform certain operations. The output can be used as \\na draft.\\n• Code completion: LLMS can proactively suggest useful code as the user types it. This \\ncan save developers time and improve code quality.\\n• Code refactoring and debugging: LLMs can help reduce technical debt by refactoring \\nand debugging code to improve quality, efficiency and correctness.\\n• Code translation: LLMs can significantly help developer time and effort by helping to \\nconvert code from one programming language to another. For example, an LLM might \\nconvert Python code to Java.\\n• Test case generation: LLMs can be prompted to generate unit tests for a provided \\ncodebase which saves considerable time and reduces errors.\\n• Code documentation and understanding: LLMs can be used in a conversational manner \\nto engage in a natural language chat to help you understand a codebase. They can also \\ngenerate appropriate comments, understand copyright status, and create release notes.\\nRecently, a number of exciting advancements have been made in the space of competitive \\ncoding and mathematics. AlphaCode 2,64 combines Gemini’s reasoning capabilities with \\nsearch and the use of tools to solve competitive coding problems. It receives as input a \\ndescription of a problem to solve, and outputs a code solution that solves the problem. It'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 61, 'page_label': '62'}, page_content='Foundational Large Language Models & Text Generation\\n62\\nSeptember 2024\\nnow ranks among the top 15% competitive coders on the popular Codeforces competitive \\ncoding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \\na pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \\nproblem in mathematics, and also discovered more efficient bin-packing algorithms which \\nare used in many applications such as making data centers more efficient. Another recent \\napproach called AlphaGeometry tackles the problem of finding proofs for complex geometric \\ntheorems. It comprises a neuro-symbolic system made up of a neural language model and \\na symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \\ngeometry problems, where the average human gold medalist scores on average 25.9. 67\\nMachine translation\\nLLMs are capable of generating fluid, high-quality and contextually accurate translations. \\nThis is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \\ncontext. The following are some possible real world use cases:\\n• Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \\ntranslations that feel natural. Unlike previous algorithms that might translate word-\\nfor-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \\ncross-language communication.\\n• E-commerce: On global platforms like AliExpress, product descriptions are automatically \\ntranslated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \\ndetails are appropriately translated, leading to fewer misunderstandings.\\n• Travel apps: In apps like Google Translate, travelers get real-time spoken translations. \\nWith LLMs, the translated conversations are smoother, making interactions in foreign \\ncountries more effortless.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 62, 'page_label': '63'}, page_content='Foundational Large Language Models & Text Generation\\n63\\nSeptember 2024\\nText summarization\\nText summarization is a core capability of many of the LLMs mentioned in this whitepaper. \\nThere are a number of natural potential use cases which include:\\n• News aggregators: LLMs could craft summaries that capture not only the main \\nevents but also the sentiment and tone of the article, providing readers with a more \\nholistic understanding.\\n• Research databases: LLMs could help researchers generate abstracts that encapsulate \\nthe core findings and implications of scientific papers.\\n• Chat management: In platforms like Google Chat, LLM-based systems could generate \\nthread summaries that capture the urgency and tone, aiding users in prioritizing \\ntheir responses.\\nQuestion-answering\\nThe older generation of QA systems often worked by keyword matching, frequently missing \\nout on the contextual depth of user queries. LLMs, however, dive deep into context. They can \\ninfer user intent, traverse vast information banks, and provide answers that are contextually \\nrich and precise. Some of the examples where this could be used include:\\n• Virtual assistants: LLMs can offer detailed explanations of a weather forecast \\nconsidering the user’s location, time of year, and recent weather trends.\\n• Customer support: In business platforms, LLM-based bots could provide answers that \\ntake into account the user’s purchase history, past queries, and potential issues, offering \\npersonalized assistance.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 63, 'page_label': '64'}, page_content='Foundational Large Language Models & Text Generation\\n64\\nSeptember 2024\\n• Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \\nuser queries by understanding the depth and context of academic questions, offering \\nanswers that suit everyone from a high school student to a postgraduate researcher.\\nThe quality of the generated answers, as well as the corresponding citations and sources \\ncan be significantly improved by using advanced search systems (such as those based on \\nRetrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \\ninformation, as well as post-hoc grounding after the response has been generated. Clear \\ninstructions, roles of what should and should not be used to answer the question, and \\nadvanced prompt engineering approaches (such as chain of thought and search/RAG \\narchitectures), combined with a lower temperature value amongst other things can also \\nhelp greatly.\\nChatbots\\nEarlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \\ntransform this space by offering dynamic, human-like interactions. They can analyze \\nsentiment, context, and even humor, making digital conversations feel more authentic. Some \\nexamples of where this can be used include:\\n• Customer service: A chatbot on retail platforms like Zara could not only answer product-\\nrelated queries but also offer fashion advice based on current trends.\\n• Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \\nreacting to live events in the stream and moderating chats with contextual understanding.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 64, 'page_label': '65'}, page_content='Foundational Large Language Models & Text Generation\\n65\\nSeptember 2024\\nContent generation\\nText generation isn’t new, but what LLMs bring to the table is the unprecedented ability \\nto generate human-like text that’s contextually relevant and rich in detail. Earlier models \\nwould often lose context or coherence over longer passages. LLMs, with their vast \\nknowledge and nuanced understanding, can craft text spanning various styles, tones, and \\ncomplexities, mixing factuality with creativity (depending on the context) effectively bridging \\nthe gap between machine-generated and human-written content. The following are some \\nreal-world examples:\\n• Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \\nInstead of generic content, the LLMs could generate creative, targeted, and \\naudience-specific messages.\\n• Scriptwriting: LLMs could potentially assist with producing scripts for movies or TV \\nshows. Writers could input themes or plot points, and the model can suggest dialogues or \\nscene descriptions, enhancing the creative process.\\nText generation is a wide task encompassing a variety of use cases that might range from \\nthose where correctness of the generated output is more or less important than its creativity/\\ndiversity of the language. The sampling methods and parameters like temperature should be \\ntuned accordingly. For more information, see the prompt engineering and architecting for \\nLLM applications whitepapers.\\nNatural language inference\\nNatural language inference (NLI) is the task of determining whether a given textual \\nhypothesis can be logically inferred from a textual premise.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 65, 'page_label': '66'}, page_content='Foundational Large Language Models & Text Generation\\n66\\nSeptember 2024\\nTraditional models struggled with nuanced relationships or those that require a deeper \\nunderstanding of context. LLMs, with their intricate grasp of semantics and context, excel \\nat tasks like these, bringing accuracy levels close to human performance. The following are \\nsome real-world examples:\\n• Sentiment analysis: Businesses could utilize LLMs to infer customer sentiment from \\nproduct reviews. Instead of just basic positive or negative tags, they could extract \\nnuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\\n• Legal document review: Law firms could employ LLMs to infer implications \\nand intentions in contracts, ensuring there are no contradictions or potentially \\nproblematic clauses.\\n• Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \\ndoctors in inferring potential diagnoses or health risks, ensuring early intervention.\\nThe whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \\napplications give further insight into these use cases.\\nText classification\\nText classification involves categorizing text into predefined groups. While traditional \\nalgorithms were efficient, they often struggled with ambiguous or overlapping categories. \\nLLMs, given their deep understanding of context, can classify text with higher precision, even \\nwhen faced with subtle distinctions. Some examples of this include:\\n• Spam detection: Email services could utilize LLMs to classify emails as spam or \\nlegitimate. Instead of just keyword-based detection, the models understand the context \\nand intent, potentially reducing false positives.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 66, 'page_label': '67'}, page_content='Foundational Large Language Models & Text Generation\\n67\\nSeptember 2024\\n• News categorization: News platforms could employ LLMs to categorize articles into \\ntopics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \\nbetween categories.\\n• Customer feedback sorting: Businesses could analyze customer feedback through \\nLLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \\nensuring targeted responses.\\n• Evaluating LLMs as autorater: LLMs could be used to rate, compare and rank the \\ngenerated outputs of other LLMs as well.\\nText analysis\\nLLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \\ninsights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \\ndelve deep, offering rich and actionable insights. Some potential real-world examples are:\\n• Market research: Companies could leverage LLMs to analyze consumer conversations on \\nsocial media, extracting trends, preferences, and emerging needs.\\n• Literary analysis: Academics could employ LLMs to understand themes, motifs, and \\ncharacter developments in literary works, offering fresh perspectives on classic and \\ncontemporary literature.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 67, 'page_label': '68'}, page_content='Foundational Large Language Models & Text Generation\\n68\\nSeptember 2024\\nMultimodal applications\\nMultimodal LLMs, capable of processing and generating text, images, audio, and video, have \\nopened up a new frontier in AI, offering a range of exciting and innovative applications across \\nvarious sectors. The following are some examples: \\nCreative content generation:\\n• Storytelling: An AI system could watch an image or video and spin a captivating narrative, \\nintegrating details from the visual with its knowledge base.\\n• Advertising and marketing: Generating targeted and emotionally resonant advertisements \\nbased on product photos or videos.\\nEducation and accessibility:\\n• Personalized learning: Tailoring educational materials to individual learning styles by \\ncombining text with interactive visual and audio elements.\\n• Assistive technology: Multimodal LLMs could power tools that describe images, videos, \\nand audio for visually or hearing-impaired individuals.\\nBusiness and industry:\\n• Document understanding and summarization: Automatically extracting key information \\nfrom complex documents, combining text and visuals like invoices and contracts.\\n• Customer service: Multimodal chatbots can understand and respond to customer queries \\ncombining text and images, offering a richer and more personalized experience. Science \\nand research:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 68, 'page_label': '69'}, page_content='Foundational Large Language Models & Text Generation\\n69\\nSeptember 2024\\n• Medical diagnosis: Analyzing medical scans and reports together, identifying potential \\nissues and providing insights for doctors.\\n• Bioinformatics and drug discovery: Integrating knowledge from diverse data sources like \\nmedical images, protein structures, and research papers to accelerate research.\\nThese examples are just the tip of the iceberg. As research progresses, the applications \\nof multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \\nprofound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \\nUnimodal LLMs ( i.e., text based LLMs).\\nLLMs, thanks to their ability to understand and process language, are reshaping how we \\ninteract with, generate, and analyze text across diverse sectors. As they continue to evolve, \\ntheir applications will only grow, boosting the ability for machines and humans to have rich \\nnatural language interactions.\\nSummary\\nIn this whitepaper we have discussed the basics of transformers, upon which all modern-day \\nLLMs are based. We detailed the evolution of the various LLM model architectures and their \\ncomponents. We’ve also seen the various methodologies you can use to train and fine-tune \\nmodels efficiently and effectively. We briefly discussed prompt engineering and sampling \\ntechniques that greatly influence the output of an LLM, and also touched on possible \\napplications of this technology. There are a number of key takeaways to keep in mind:'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 69, 'page_label': '70'}, page_content='Foundational Large Language Models & Text Generation\\n70\\nSeptember 2024\\n• The transformer architecture is the basis for all modern-day LLMs. Across the various \\narchitectures mentioned in this whitepaper we see that it’s important not only to add more \\nparameters to the model, but the composition of the dataset is equally important. \\n• The order and strategies used for fine-tuning is important and may include multiple steps \\nsuch as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \\nin capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \\ndistribution from the pretraining distribution to a more desired one through the power of \\nthe reward function, that can reward desirable behaviors and penalize undesirable ones.\\n• Making inference from neural models efficient is an important problem and an active \\nfield of research. Many methods exist to reduce serving costs and latency with minimal \\nimpact to model performance, and some exact acceleration methods guarantee identical \\nmodel outputs.\\n• Large language models can be used for a variety of tasks including summarization, \\ntranslation, question answering, chat, code generation, and many more. You can \\ncreate your own tasks using the Vertex and Makersuite text generation services which \\nleverage Google’s latest language models. After the model has been trained and tuned, \\nit is important to experiment with engineering prompts. You should use the technique \\nmost appropriate for the task-at-hand because LLMs can be sensitive to prompts k. \\nFurthermore, it is also possible to enhance task specific performance or creativity and \\ndiversity by tweaking the parameters corresponding to sampling techniques such as \\nTop-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \\nand creativity required for the task at hand.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 70, 'page_label': '71'}, page_content='Foundational Large Language Models & Text Generation\\n71\\nSeptember 2024\\nEndnotes\\n1. 1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin,\\xa0I., 2017, Attention is \\nall you need. Advances in Neural Information Processing Systems , 30.\\n2. Wikipedia, 2024, Word n-gram language model. Available at:  \\nhttps://en.wikipedia.org/wiki/Word_n-gram_language_model .\\n3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \\nNeural Information Processing Systems,  27.\\n4. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\narXiv preprint arXiv:2111.00396.\\n5. Jalammar, J. (n.d.). The illustrated transformer. Available at:  \\nhttps://jalammar.github.io/illustrated-transformer/ .\\n6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \\narXiv preprint arXiv:1607.06450.\\n7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition.\\n8. HuggingFace., 2024, Byte Pair Encoding. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt .\\n9. Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n10. HuggingFace, 2024, Unigram tokenization. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/7?fw=pt .\\n11. Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org .\\n12. Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\\n13. Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \\nProcessing Systems, 33, 1877-1901.\\n14. Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 71, 'page_label': '72'}, page_content='Foundational Large Language Models & Text Generation\\n72\\nSeptember 2024\\n15. Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\\n16. Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \\nProcessing Systems.\\n17. Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \\nin Neural Information Processing Systems,  35, 27730-27744.-27744.\\n18. OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5 .\\n19. OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774 .\\n20. Thoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \\narXiv\\xa0preprint arXiv:2201.08239.\\n21. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \\nat: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/ .\\n22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \\nmodels: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf.\\n23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \\nmodels with mixture-of-experts. In International Conference on Machine Learning (pp. 2790-2800). PMLR.\\n24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \\nfor neural language models. arXiv preprint arXiv:2001.08361.\\n25. Hoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \\npreprint arXiv:2203.15556.\\n26. Shoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \\nmodel parallelism. arXiv preprint arXiv:1909.08053.\\n27. Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n28. Chowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \\nLearning Research, 24(240), 1-113.\\n29. Wang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \\nsystems. Advances in Neural Information Processing Systems , 32.\\n30. Anil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 72, 'page_label': '73'}, page_content='Foundational Large Language Models & Text Generation\\n73\\nSeptember 2024\\n31. DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf .\\n32. DeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf .\\n33. Google Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \\nAvailable at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/ .\\n34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n35. Jiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088.\\n36. Qwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/ .\\n37. Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652.\\n38. Grok-1, 2024, Available at: https://github.com/xai-org/grok-1.\\n39. Duan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \\narXiv\\xa0preprint arXiv:2310.13650.\\n40. Google Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf .\\n41. Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n42. Wikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale .\\n43. Sutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\\n44. Bai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n45. Rafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \\nmodel. arXiv preprint arXiv:2305.18290.\\n46. Houlsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \\nMachine Learning (pp. 2790-2799). PMLR.\\n47. Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \\narXiv\\xa0preprint arXiv:2106.09685.\\n48. Dettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 73, 'page_label': '74'}, page_content='Foundational Large Language Models & Text Generation\\n74\\nSeptember 2024\\n49. Lester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \\npreprint arXiv:2104.08691.\\n50. HuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate .\\n51. Google AI Studio Context caching. Available \\nat: https://ai.google.dev/gemini-api/docs/caching?lang=python .\\n52. Vertex AI Context caching overview. Available \\nat: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview .\\n53. Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\nAvailable at: https://arxiv.org/abs/2111.00396 .\\n54. Hubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \\nactivations. Available at: https://arxiv.org/abs/1609.07061.\\n55. Benoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \\ninference. Available at: https://arxiv.org/abs/1712.05877 .\\n56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \\nMining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf .\\n57. Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \\nAvailable at: https://arxiv.org/abs/1503.02531 .\\n58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \\nQuantisation for LLM. Available at: https://arxiv.org/abs/2310.04836 .\\n59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\\nPolicy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \\nat: https://arxiv.org/abs/2306.13649.\\n60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \\nnetworks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538 .\\n61. Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \\nhttps://arxiv.org/abs/2207.07061.\\n62. Tri Dao et al. “FlashAttention. Available at:  \\nhttps://arxiv.org/abs/2205.14135.'), Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.0 (Macintosh)', 'creationdate': '2024-11-12T11:43:11-07:00', 'moddate': '2024-11-12T11:43:17-07:00', 'trapped': '/False', 'source': './Data/Newwhitepaper_Foundational Large Language models & text generation.pdf', 'total_pages': 75, 'page': 74, 'page_label': '75'}, page_content='Foundational Large Language Models & Text Generation\\n75\\nSeptember 2024\\n63. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \\nspeculative decoding. Available at: https://arxiv.org/abs/2211.17192 .\\n64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \\ngeneration with AlphaCode. Science, 378(1092-1097). DOI: 10.1126/science.abq1158.\\n65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \\ndiscoveries from program search with large language models. Nature. DOI: 10.1038/s41586-023-06924-6.\\n66. Wikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set .\\n67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. \\nNature, 625, 476–482. DOI: 10.1038/s41586-023-06747-5.')]\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./Data/Newwhitepaper_Foundational Large Language models & text generation.pdf\")  # Load your PDF file\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Split the Document into Chunks\n",
    "\n",
    "To handle large documents efficiently, we split the PDF into smaller chunks using the `RecursiveCharacterTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Chunks:  616\n"
     ]
    }
   ],
   "source": [
    "# Chunk_size: number of characters in the chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Total number of Chunks: \", len(docs))  # Check how many chunks we have\n",
    "# for chunk in docs:\n",
    "#     print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Generate embeddings with Gemini AI\n",
    "\n",
    "Next, to embed these chunks using Gemini AI, we access one of the models available in genAI. Embeddings are vector representations of text data, and they allow us to perform similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a Vector Store for Document Retrieval\n",
    "\n",
    "We now store the document chunks and their embeddings in a vector database, which will allow us to retrieve similar documents based on user queries.\n",
    "\n",
    "In this example, we are using Chroma as our vector database. Chroma is one of the many options available for storing and retrieving embeddings efficiently. \n",
    "\n",
    "`from_documents` create a vector store from a list of documents (docs) using the embedding function specified (defaults to none)\n",
    "\n",
    "`as_retriever` returns a vector store retriever from the vector store, k: amounts of documents to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoredb = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "retriever = vectorstoredb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Retrieve Documents Based on a Query\n",
    "\n",
    "To test the retrieval system, we ask \"Wjat is a large language model?\" as an example, and retrieve the most relevant document chunks (the fist one is the one with the highest similarity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Foundational Large Language Models & Text Generation\n",
      "8\n",
      "September 2024\n",
      "Large language models\n",
      "A language model predicts the probability of a sequence of words. Commonly, when given \n",
      "a prefix of text, a language model assigns probabilities to subsequent words. For example, \n",
      "given the prefix “The most famous city in the US is…”, a language model might predict high \n",
      "probabilities to the words “New York” and “Los Angeles” and low probabilities to the words \n",
      "“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \n",
      "modern language models are often based on neural models, such as transformers.\n",
      "Before the invention of transformers1, recurrent neural networks (RNNSs) were the popular \n",
      "approach for modeling sequences. In particular, “long short-term memory” (LSTM) and \n",
      "“gated recurrent unit” (GRU) were common architectures.3 This area includes language \n",
      "problems such as machine translation, text classification, text summarization, and question-\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is a large language model\")\n",
    "print(len(retrieved_docs))\n",
    "print(retrieved_docs[0].page_content)  # Print the first retrieved document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Build a Question-Answering (Q&A) System\n",
    "\n",
    "Now we move to the core of the RAG model by building a question-answering chain using the `ChatGoogleGenerativeAI` model from Gemini AI.\n",
    "\n",
    "This step initializes the Gemini AI model for generating responses based on retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Create the RAG Chain\n",
    "\n",
    "We combine document retrieval with question answering using a custom prompt. The system will retrieve relevant documents and generate concise answers.\n",
    "\n",
    "This prompt structure ensures the model generates answers concisely and within a specific context.\n",
    "\n",
    "NOTES:\n",
    "* create_stuff_documents_chain: create a chain for passing a list of documents to a model\n",
    "    * the prompt must contain a context (see \"{context}\" in the system prompt)\n",
    "* create_retrieval_chain: retriever parameter contains the documents (the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt \n",
    "system_prompt = (\n",
    "   \"You are a AI expert. Provide clear, concise answers based on the provided context. \"\n",
    "    \"If the information is not found in the context, state that the answer is unavailable. \"\n",
    "    \"Use a maximum of three sentences.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Set up the prompt for the QA chain -> langchain_core > prompts > ChatPromptTemplate to see the template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\")  # input = message passed through invoke method later on\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)  \n",
    "rag_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Examples of prompt engineering include providing clear instructions, examples, keywords, formatting, and background details.  This information is found in the provided text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response = rag_chain.invoke({\"input\": \"What is a LLM\"})\n",
    "response = rag_chain.invoke({\"input\": \"Give me examples of prompt engineering and let me know in which document was found\"})\n",
    "Markdown(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Some ideas to improve this exercise:\n",
    "\n",
    "1. Chat history: The LLM has no history and it´s lacking interaction\n",
    "\n",
    "2. References/citation: as the metadata is not good, chunks don't save its procedence/reference\n",
    "\n",
    "2. Create an Agent with the next flow: \n",
    "    1. Add metadata\n",
    "    2. Prompt -> Agent: to give a summary of each pdf to the LLM to decide which one to split and make vector search\n",
    "    3. LLM -> RAG: one the right pdf is splitted into chunks, RAG chain to get the proper chunks\n",
    "    4. RAG -> LLM -> answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
